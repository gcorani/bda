#
## Comparing two proportions

# Hypothesis test for two proportions

* We denote by $\pi_1$ e $\pi_2$ the proportion of successes in two populations.

\bigskip 

* The term success and failure refer to the outcome being 1 or 0.

\bigskip

* We want to check whether $\pi_1$ e $\pi_2$ are significantly different.

\bigskip 

* $\pi_1$ e $\pi_2$  cannot be observed. We observe instead the *sample* proportions, $p_1 = \frac{X_1}{n_1}$ and  $p_2 = \frac{X_2}{n_2}$.


\bigskip 

* If both samples contains at least 5 successes and failures, then  $p_1$ and  $p_2$ are approximately normally distributed. We use this approximation in order to define the sampling distribution of the statistic.


# Comparing two proportions

* We have two samples of size $n_1$ e $n_2$, containing $X_1$ and $X_2$    successes.

\bigskip

* The two-tailed test is:

\begin{align*}
H_0 \; & : \pi_1 = \pi_2 \\
H_1 \; & : \pi_1 \neq \pi_2
\end{align*}

# The test statistic


\begin{align*}
Z & = \frac{(p_1 - p_2)}{\sqrt{\bar{p}(1-\bar{p}) \left( \frac{1}{n_1} + \frac{1}{n_2} \right)}} \\
& \text{with :}\\
\\
\bar{p} & = \frac{X_1 + X_2}{n_1 + n_2}\\
\end{align*}


* Under $H_0$,  $Z \sim N(0,1)$.

# The test statistic

* If we extract many times two samples of size $n_1$ and $n_2$ from two populations with
  $\pi_1 = \pi_2$ and compute the statistic, it will be different every time, following 
  approximately a  $N(0,1)$ distribution.

\bigskip

* It the test is one-tailed the statistics remains the same, but the rejection region changes.

# Rejection regions

|  $H_1$                    |             Rejection region            |       p-value       |   |
|:-------------------------:|:-----------------------------------------:|:-------------------:|---|
|     $\pi_1 \neq \pi_2$    | $z < z_{\alpha/2}$ e $z > z_{1-\alpha/2}$ | $2 (1-\Phi(|z|))$ |   |
|      $\pi_1 > \pi_2$      |             $z > z_{1-\alpha}$            |     $1-\Phi(z)$     |   |
|      $\pi_1 < \pi_2$      |              $z < z_{\alpha}$             |      $\Phi(z)$      |   |


# CI of $\pi_1 - \pi_2$

* If we extract many times two samples of size $n_1$ and $n_2$ from two populations with
  $\pi_1 = \pi_2$ and compute the CI, it will contain the actual value of  $\pi_1 - \pi_2$ 
  in ($1 - \alpha$) of the experiments.

\bigskip

* The CI is:

\[
p_1 - p_2 \pm z_{1-\alpha/2} \sqrt{ \frac{p_1 (1-p_1)}{n_1} + \frac{p_2 (1-p_2)}{n_2}}
\]

\bigskip 

* The CI and the two-tailed test approximate differently the standard error and they might  sometimes draw inconsistent conclusions.


# Example: assess the effect of a process innovation

* From a traditional production process we have 262 boards, of which 154 without any defect.

\bigskip

* From a innovative production process we have 227 boards, of which 163 without any defect.

\bigskip

* Is the new process significantly more accurate than the previous one?


# Comparing the two proportions

* Both samples contain more than 5 successes and 5 failures; the normal approximation is sound.

\bigskip

* The  test is:

\begin{align*}
H_0 \; : \pi_{\text{new}} \leq \pi_{\text{old}}\\
H_1 \; : \pi_{\text{new}} > \pi_{\text{old}}\\
\end{align*}

* and we use  $\alpha=0.01$.

* We take as success the board being without any defect.

# Comparing the two proportions

* The rejection region contains positive values  of $p_{\text{new}} - p_{\text{old}}$ and thus also of the  statistic.

\bigskip

* Rejection region: $Z_0 > \Phi^{-1}({.99})$ = `r round(qnorm(.99),2)`

# Comparing the two proportions

```{r, echo=FALSE, out.width = '85%', fig.align = 'center', warning=FALSE}

n1 <- 227
n2 <- 262
p1 <- 163/n1
p2 <- 154/n2
p_bar  <- (163 + 154) / (n1 + n2)
p_err <- sqrt ( (p_bar * (1 - p_bar)) * (1/n1 + 1/n2) )
p_stat <- (p1 - p2) / p_err
p_stat <- round(p_stat,2)
p_val  <-  1 - pnorm(p_stat)
```


\bigskip 

\begin{align*}
p_{\text{new}} & = 163/227 = 0.72 \\
p_{\text{old}} & = 154/262 = 0.59 \\
\bar{p} & = (163 + 154)/(227 + 262) = 0.65\\
Z & = \frac{p_{\text{new}} - p_{\text{old}}} 
{\sqrt{ \bar{p} \cdot (1-\bar{p}) \cdot 1/n}} = 3.01 > 2.33\\
\end{align*}


\bigskip
* The statistic is in rejection region.

#
## Beauty and sex ratio

>  Chap. 9.4 of "Regression and other stories", A. Gelman, J. Hill, A. Vehtari

* Book published from Cambridge University Press (2020).

\bigskip

* The book is also freely available online.

\bigskip
* Keep in mind these example, we will re-analyze the data in a later lecture using a Bayesian approach.


# Beauty and sex ratio

* Some years ago a researcher analyzed data from a survey of 3000 Americans and observed a correlation between attractiveness of parents and the sex of their children.

\bigskip

* He considers  3000 couple of parents, among which  300 classified as highly attractive.

\bigskip

* The proportion of girls among the children of "standard" parents is 48% ($X=1296, n=2700$). 

\bigskip

* The proportion of girls among the children of "highly attractive" parents is 56% ($X=168, n=300$). 

# Is the difference significant?

The test is:
\begin{align*}
H_0 \; : \pi_{\text{attr}} \leq \pi_{\text{std}}\\
H_1 \; : \pi_{\text{attr}} > \pi_{\text{std}}\\
\end{align*}

# Is the difference significant?

\begin{align*}
p_{\text{attr}} & = 0.56 \\
p_{\text{std}} & = 0.48 \\
\bar{p} & = \frac{168 + 1296}{300 + 2700} = 0.488\\
\text{sd err} &=
\sqrt{ \bar{p} \cdot (1-\bar{p}) \cdot ( \frac{1}{n_1} + \frac{1}{n_2})} = 0.03\\
Z & = \frac{p_{\text{new}} - p_{\text{old}}} 
{\text{sd err}} = 2.63 > 2.33,\\
\end{align*}

where 2.33 is the 99-th quantile ($\alpha$=0.01).

\bigskip

* The difference in the proportion of girls between the two groups is statistically *significant*.