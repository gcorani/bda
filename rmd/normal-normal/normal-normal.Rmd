---
title: "The normal-normal model"
author: "Giorgio Corani"
date: 'Bayesian Data Analysis and Probabilistic Programming'
output:
  beamer_presentation:
    latex_engine: xelatex
    fig_width: 7
    fig_height: 3.5
    highlight: tango
    theme: metropolis
    includes:
      in_header: header.tex
  slidy_presentation:
    highlight: tango
  ioslides_presentation:
    highlight: tango
fontsize: 13pt
---

```{r setup, include=FALSE}
library(reticulate)
```

```{python, echo=FALSE}
import numpy as np
import pandas as pd
import arviz as az
import scipy.stats as stats
from matplotlib import pyplot as plt
from scipy.stats import norm
from scipy.stats import halfnorm
import seaborn as sns
sns.set_theme()
plt.rcParams.update({
    "text.usetex": True,
    "font.family": "sans-serif",
    "font.sans-serif": ["Helvetica"]})
az.style.use('arviz-darkgrid')
```

# Credits

* Chap. 5 of *Bayes Rules! An Introduction to Applied Bayesian Modeling*
  * https://www.bayesrulesbook.com/chapter-5.html



# The Normal model

* Let $Y$  be a continuous random variable which can take  values in (−$\infty$,$\infty$)  

\bigskip

* The variability of Y might be well represented by a Normal model 
 $$Y ∼ N(\mu,\sigma^2)$$


# The Normal model

* The Normal pdf is 

$$ f(y) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\bigg[{-\frac{(y-\mu)^2}{2\sigma^2}}\bigg] $$

\bigskip

* With:

\begin{align*}
E(Y) &= Mode(Y)=μ \\
Var(Y) &= \sigma^2 \\
SD(Y) &= \sigma
\end{align*}

# Standard deviation $\sigma$

* $\sigma$ provides a sense of scale for $Y$. 

\bigskip

* Roughly 95% of $Y$ values are within 2 standard deviations of $\mu$:
$$\mu \pm 2 \sigma$$

\bigskip 

* Roughly 99% of $Y$ values are within 3 standard deviations of $\mu$:
$$\mu \pm 3 \sigma$$

# The normal model

* The Normal model is bell-shaped and symmetric around $\mu$. 

\bigskip

* As σ gets larger, the pdf becomes more spread out.

\bigskip
* Though a Normal variable is defined in (−∞, ∞), the   plausibility of values that are more than 3 standard deviations σ from the mean $\mu$ is negligible.

```{r figurename, echo=FALSE, out.width = '75%'}
knitr::include_graphics("normal-tuning.png")
```

# Example


* The  volume of the hippocampus (a part of the brain)  is  researched in studies about the effect of concussions.

\bigskip

* In the general population, both halves of the hippocampus have a volume between 3.0 and 3.5 cm$^3$.

\bigskip

* Thus, the  hippocampal volume is thought to vary, within the population,  between 6 and 7 cm$^3$.

\bigskip

* The average volume $\mu$ is thought to be between 6.4  and 6.6 cm$^3$.


# Normal prior

* Assuming  symmetry, we  formalize our  prior information about $\mu$ as:

\begin{align*}
\mu & \sim N(\mu', \sigma_{\mu})\\
\mu & \sim N(6.5, 0.05)\\
\end{align*}

*  $\sigma_{\mu}$ represents our prior uncertainty on the value of $\mu$.

\bigskip

*  $\mu'$ is our prior guess on the value of $\mu$.

\bigskip

* According to this prior, there is about 95% probability of $\mu$ lying in (6.4, 6.6). 

\bigskip

* There is generally no single right prior, but multiple reasonable priors.


# Normal likelihood

* We now define a model for the distribution of 
 the observations.

\bigskip


* We make a *second* assumption of normality.

\bigskip

* The observed volumes $y_1,y_2,…,y_n$, are independent and normally distributed around  $\mu$, with standard deviation $\sigma$.


\bigskip

* $\sigma$ expresses the spread of the hippocampal volumes within the population.

\bigskip

* As we expect $y$ to vary between roughly 6 and 7, we  set $\sigma$=0.25 (we interpret the interval as $\mu \pm 2\sigma$, hence it has length of 4$\sigma$).


# Independence 

* We assume the observations $y_1, ..., y_n$ to be *independent* samples from $N(\mu, \sigma)$.

# Likelihood

Assuming independence, the joint pdf of the $n$ measures ($y_1,y_2,…,y_n$), is the product of the unique Normal pdfs $f(y_i \mid \mu)$:

$$
f(\vec{y} | \mu) = \prod_{i=1}^{n}f(y_i|\mu) = \prod_{i=1}^{n}\frac{1}{\sqrt{2\pi\sigma^2}} \exp\bigg[{-\frac{(y_i-\mu)^2}{2\sigma^2}}\bigg]  .
$$
\bigskip

*  $\vec{y}$ is the vector containing the measures $y_1,....y_n$.

\bigskip

* Theoretically, the normal model lets  each  hippocampal volume  range from $-\infty$ to $\infty$. However it will assign negligible weight to values which are beyond $\mu \pm 3 \sigma$.

# The Normal-Normal  model

\begin{align*}
\mu & \sim N(\mu', \sigma_{\mu}) \\
\vec{y} & \sim N(\mu, \sigma) \\
\end{align*}

* For the moment we assume $\sigma$ to be known and fixed. Later we will express our prior uncertainty also about it. 



# Conjugacy 

* Denote the sample mean as  $\bar{y} = \frac{1}{n} \sum_i y_i$.

\bigskip

* The posterior density of $\mu$ is 
 normal with updated parameters:

$$
\mu|\vec{y} \; \sim \;  N\bigg(\mu'\frac{\sigma^2}{n\sigma_{\mu}^2+\sigma^2} + \bar{y}\frac{n\sigma_{\mu}^2}{n\sigma_{\mu}^2+\sigma^2}, \; \frac{\sigma_{\mu}^2\sigma^2}{n\sigma_{\mu}^2+\sigma^2}\bigg)  .
$$

# Posterior mean

$$
\mu|\vec{y} \; \sim \;  N\bigg(\mu'\frac{\sigma^2}{n\sigma_{\mu}^2+\sigma^2} + \bar{y}\frac{n\sigma_{\mu}^2}{n\sigma_{\mu}^2+\sigma^2}, \; \frac{\sigma_{\mu}^2\sigma^2}{n\sigma_{\mu}^2+\sigma^2}\bigg)  .
$$

* The posterior mean is a weighted average of the prior mean $\mu'$ and the sample mean $\bar{y}$.


\bigskip

* As $n$ increases, the posterior mean converges to $\bar{y}$.

\bigskip

* As $n$ increases, the posterior variance decreases.

# Your turn

* Assume that the sample of $n$ measures has mean $\bar{y}=6.7$.

\bigskip

* Which is the posterior mean?  

#  What if $\sigma$ is unknown?

*  A more sophisticated approach is to treat $\sigma$ as a parameter, by assigning a prior to it and make inference about it, rather than keeping it fixed.

\bigskip

* In this case there is no closed-form expression of the posterior.

# Prior distribution of $\sigma$

*  $\sigma$ is strictly positive; a suitable prior is the *half-normal* distribution.

\bigskip

* The half-normal is  a Gaussian  restricted to positive values.

\bigskip

* You sample from a half-normal by:
  * sampling from a normal distribution 
  * applying the absolute value to the  sampled values.


# The half-normal distribution

* The HN pdf is asymmetric and right-skewed pdf.

\bigskip


* It has long tails which are much larger than the median.

\bigskip

* The prior should cover a wide range of plausible values for $\sigma$, leaving out however values that make no sense.




```{r, echo=FALSE, out.width = '75%', fig.cap= "from wikipedia"}
knitr::include_graphics("half_normal.png")
```


# The half-normal distribution

* The half-normal pdf is characterized by a scale parameter.

```{python, echo=FALSE, fig.height=2, fig.align="center"}
plt.figure(figsize=(10, 3))


x = np.linspace(-2,5, 100)

#at each x we plot the pdf of the halfnorm with different scales
plt.plot(x, halfnorm.pdf(x, scale=1), 'r-', lw=3, alpha=0.6, label='scale 1')
plt.plot(x, halfnorm.pdf(x, scale=3), 'b-', lw=3, alpha=0.6, label='scale 3');
plt.plot(x, halfnorm.pdf(x, scale=6), 'g-', lw=3, alpha=0.6, label='scale 6');
plt.legend()

```




# Tuning the half-normal distribution

* You can tune the HN prior distribution by matching its median with a plausible value of $\sigma$

\bigskip

* For instance  we think a plausible value for the standard deviation of the noise is 7.5.

\bigskip

  * with 95% probability the measures are lie in an interval of +- 15 around the mean.

\bigskip

* Of course, we are uncertain about this statament.

\bigskip

* Perhaps, with 95% probability the measures lie in an interval of +- 30, 
in which case the standard deviation of the noise is around 10.

# Tuning the half-normal distribution


\bigskip


# Tuning the half-normal distribution

* The halfnormal distribution  has been obtained by trying different scale parameters.

\bigskip    

* Notice the long tails of the distribution, which allows to model to correct if our prior median guess (7.5) is underestimated.  

```{python, echo=TRUE, fig.height=2, fig.align="center"}
pd.DataFrame(halfnorm.rvs(size=1000, scale=11)).describe()
```
# The probabilistic model

\begin{align*}
\mu &\sim N(\mu_{\mu}, \sigma_{\mu}) && \text{prior beliefs about } \mu\\ 
\sigma &\sim \text{Half-Normal}(\sigma_{\sigma}) && \text{prior beliefs about } \sigma \\
y & \sim \mathcal{N}(\mu, \sigma) && \text{the observation are  affected by a noise with standard deviation } \sigma\\
\end{align*}

* We cannot treat this model analytically, as the prior are no longer conjugates.

\bigskip

* We will implement it later via probabilistic programming.

# Conceptual exercise

* Try to define the priors of a probabilistic model which  represents the distribution of height of adult males in Switzerland 

# Population of Swiss adult males: $p(\mu)$


* The mean height of the population could  be 175, though this is uncertain. Keeping our prior broad,  the mean height of the population lies with 99% probability between 160 and 190 cm.
  
  \bigskip
  
  * $\mu \sim \mathcal{N}(175, 5)$ 
  
# Population of Swiss adult males: $p(\sigma)$

* We shall now assign a prior to $\sigma$. We assume that within the whole population the height  varies with 99% probability between 100 and 250. 

\bigskip

  * Hence the corresponding value of the standard deviation is (250-100)/6 = 25.
    
    \bigskip
    
  * Notice the broad but sensible range.
  
  \bigskip
    
  * A half-normal distribution with scale 35 has roughly this median:
  
      * $\sigma \sim \text{Half-Normal}(35)$

# Population of Swiss adult males: $p(\sigma)$

```{python, echo=TRUE, fig.height=2, fig.align="center"}
pd.DataFrame(halfnorm.rvs(size=1000, scale=35)).describe()
```

# Likelihood

* The likelihood  $y  \sim \mathcal{N}(\mu, \sigma)$ requires no parameter specification. 

\bigskip

* We are assuming that the measures are normally distributed around the mean.

\bigskip

* Moreover we assume that the measures are i.i.d.