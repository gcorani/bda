---
title: "Reporting the results of a Bayesian analysis"
author: "Giorgio Corani  - (IDSIA, SUPSI)"
date: ''
output:
  beamer_presentation:
    latex_engine: xelatex
    fig_width: 7
    fig_height: 3.5
    highlight: tango
    theme: metropolis
    includes:
      in_header: header.tex
  slidy_presentation:
    highlight: tango
  ioslides_presentation:
    highlight: tango
fontsize: 13pt
---


```{r setup, include=FALSE}
library(reticulate)
use_condaenv("r-reticulate")
```

```{python, echo=FALSE}
import numpy as np
import pandas as pd
import arviz as az
import scipy.stats as stats
from matplotlib import pyplot as plt
import seaborn as sns
sns.set_theme()
plt.rcParams.update({
    "text.usetex": True,
    "font.family": "sans-serif",
    "font.sans-serif": ["Helvetica"]})
az.style.use('arviz-darkgrid')
```

# Analysis of the posterior

* Assume you want to make inference about parameter $\theta$.

\bigskip

* The result of the Bayesian analysis is the whole posterior  of $\theta$,  **not** a single value.

\bigskip

* The dispersion of the posterior distribution (posterior variance) quantifies our  uncertainty.

\bigskip

* Uncertainty decreases when we have more data.

# The impact of the prior 

* If many data are available, the posterior is the same  regardless the prior. 

\bigskip

* But how many data are needed for the likelihood to *overwhelm* the prior? That is not know in advance. 



# The impact of the prior 

* With few  data, the posterior might be different using different priors; it makes sense to repeat the analysis with different priors (analysis of *sensitivity* to the prior).

\bigskip

* The priors impact our  results. This makes  sense, since the prior  encodes  previous knowledge and different experts have different opinions. 



# Discussion
* Priors and likelihood are based on assumptions which should be justified.

\bigskip

* There is no single right prior, but many reasonable priors.

\bigskip

* *Weakly informative*  priors are recommended, which for instance limit the parameter values only to the positive range, or to a certain order or magnitude, etc. 

\bigskip

* A Beta(1,1) prior is flat but  limits the   values of $\theta$ between 0 and 1.

\bigskip

* Sensitivity analysis: evaluate if the results significantly change with the priors, or instead the data are strong enough to overwhelm the prior.

# Summarizing the posterior

*  Different measures can be used to summarize the posterior:

\bigskip

  * the mean (or the mode, or the median) 
  
\bigskip

  * the  probability of $\theta$ belonging to a certain interval
  
\bigskip
  * the 95% (or 90%, 99%, etc) *credibility* interval (HDI: highest density interval) 
  
# Example: inference on the bias $\theta$ of the coin


* Consider a uniform prior, Beta(1,1)
* Data: $y$=8 heads in  $n$=18 tosses.
```{python, echo=FALSE, fig.height=1, fig.align='center'}
plt.figure(figsize=(10, 3))
x = np.linspace(0, 1, 200)
heads=10
tails= 8
for ind, (a, b) in enumerate([(1, 1)]):
    y = stats.beta.pdf(x, a+heads, b+tails)
    plt.subplot(1, 3, ind+1)
    plt.plot(x, y, linewidth=2)
    plt.title('Beta(%s+%s , %s+%s)' % (a,heads,b,tails))
```

* The posterior mean is $E_{\text{post}}=\frac{1+8}{1+18}=0.474$

# Probability of the coin being almost fair

```{python, echo=FALSE, fig.height=1, fig.align='center'}
plt.figure(figsize=(10, 3))
x = np.linspace(0, 1, 200)
heads=10
tails= 8
for ind, (a, b) in enumerate([(1, 1)]):
    y = stats.beta.pdf(x, a+heads, b+tails)
    plt.subplot(1, 3, ind+1)
    plt.plot(x, y, linewidth=2)
    plt.title('Beta(%s+%s , %s+%s)' % (a,heads,b,tails))
```

$$ p_{\text{post}} = 0.49 \leq \theta \leq 0.51 = \int_{0.49}^{0.51} p_{\text{post}}(\theta) d\theta $$
```{python, echo=TRUE, fig.align='center'}
from scipy.stats import beta
beta.cdf(0.51, 9, 19) - beta.cdf(0.49, 9, 11)
```




# Highest Density Interval (HDI)   

* The  HDI is the shortest interval that contains a given portion of  probability , usually 95% (or 90% or 50% are common). 

\bigskip 
* It shows the most credible  points of the distribution. 

\bigskip

* Any point within the HDI has  higher density than any point outside the interval. 


# HDI   of unimodal distributions

* (1-$\alpha$) is the coverage of the HDI. 

\bigskip

* If the distribution is unimodal, the HDI lies between the quantile $\alpha/2$ and $1-\alpha/2$.

\bigskip

* For instance, the 95% HDI lies between quantiles 0.025 and 0.975

# HDI   of unimodal distributions

```{r, out.width='45%', fig.align='center', echo=FALSE}
knitr::include_graphics('HPD-unimodal.png')
```

* Any $x$ value inside the HDI has higher density than any point outside the interval. 


# HDI of bimodal distribution
```{r, out.width='45%', fig.align='center', echo=FALSE}
knitr::include_graphics('HPD_bimodal.png')
```

* The HDI can become split into two sub-intervals, one for each mode of the distribution. 

* The  characteristics are as before: 
  * The shaded area has total area of 0.95.
  * Any $x$ within such limits has higher probability density than any $x$ outside the limits.
  
* Numerical procedures might be needed in order to compute such interval.