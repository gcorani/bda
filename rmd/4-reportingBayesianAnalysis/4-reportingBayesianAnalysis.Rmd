---
title: "Reporting the results of a Bayesian analysis"
author: "Giorgio Corani"
date: ''
output:
  beamer_presentation:
    latex_engine: xelatex
    fig_width: 7
    fig_height: 3.5
    highlight: tango
    theme: metropolis
    includes:
      in_header: header.tex
  slidy_presentation:
    highlight: tango
  ioslides_presentation:
    highlight: tango
fontsize: 13pt
---


```{r setup, include=FALSE}
library(reticulate)
use_condaenv("r-reticulate")
```

```{python, echo=FALSE}
import numpy as np
import pandas as pd
import arviz as az
import scipy.stats as stats
from matplotlib import pyplot as plt
import seaborn as sns
sns.set_theme()
plt.rcParams.update({
    "text.usetex": True,
    "font.family": "sans-serif",
    "font.sans-serif": ["Helvetica"]})
az.style.use('arviz-darkgrid')
```

# Analysis of the results - 1

* The result of the Bayesian analysis is the posterior  distribution of $\theta$,  **not** a single value.

* The dispersion of the posterior distribution (posterior variance) is a measure of  uncertainty.

* The uncertainty decreases when the number of experiments is greater.

# The impact of the prior 
* Given a _sufficient_ amount of data, the posterior will be practically the same  with any prior.

* Given a limited amount of data, the posterior obtained by different priors can be significantly different; it makes sense to repeat the analysis with different prior if you are unsure.

* The priors impact the results of our calculations. This makes  sense, since  we  use the prior to encode our previous knowledge and different experts have different opinions. 

* How much data is needed for the prior effect to be negligible varies with the problem.


# Discussion
* Priors and likelihood are based on assumptions and are  part of the model.

* *Slightly informative*  priors are recommended.
In many problems we know at least something about the possible values that our parameters can take, for example that they can only be positive, or that they are restricted to sum to 1 or the approximate range, etc. 

* A Beta(1,1) prior is flat but  limits the   values of $\theta$ between 0 and 1.

* If we had more than one reasonable prior, we could perform a sensitivity analysis, that is, evaluate if the results significantly change with the priors, or instead the data are strong enough to overwhelm the prior.

# Summarizing the posterior
*  Different measures can be used to summarize the a priori:
  * the mean (or the mode, or the median) of the posterior distribution
  * the  probability of $\theta$ belonging to a certain interval
  * the HPD, also called the *credibility* interval. 
  
# Example: inference on the bias $\theta$ of the coin


* Let us consider a uniform prior, Beta(1,1)
* Let us assume the data are $y$=8 heads out of  $n$=18 tosses.
```{python, echo=FALSE, fig.height=1, fig.align='center'}
plt.figure(figsize=(10, 3))
x = np.linspace(0, 1, 200)
heads=10
tails= 8
for ind, (a, b) in enumerate([(1, 1)]):
    y = stats.beta.pdf(x, a+heads, b+tails)
    plt.subplot(1, 3, ind+1)
    plt.plot(x, y, linewidth=2)
    plt.title('Beta(%s+%s , %s+%s)' % (a,heads,b,tails))
```

* The posterior mean is $E_{\text{post}}=\frac{1+8}{1+18}=0.474$

# Probability of the coin being almost fair

```{python, echo=FALSE, fig.height=1, fig.align='center'}
plt.figure(figsize=(10, 3))
x = np.linspace(0, 1, 200)
heads=10
tails= 8
for ind, (a, b) in enumerate([(1, 1)]):
    y = stats.beta.pdf(x, a+heads, b+tails)
    plt.subplot(1, 3, ind+1)
    plt.plot(x, y, linewidth=2)
    plt.title('Beta(%s+%s , %s+%s)' % (a,heads,b,tails))
```

$$ p_{\text{post}} = 0.49 \leq \theta \leq 0.51 = \int_{0.49}^{0.51} p_{\text{post}}(\theta) d\theta $$
```{python, echo=TRUE, fig.align='center'}
from scipy.stats import beta
beta.cdf(0.51, 9, 19) - beta.cdf(0.49, 9, 11)
```




# Highest Density Interval (HDI)   

* The HDI indicates which points of a distribution are most credible. 

* The  HDI is the shortest interval that contains a chosen portion of the probability density, usually 95% (although other values such as 90% or 50% are common). 

* Any point within this interval has a higher density than any point outside the interval. For a unimodal distribution, the HDI 95 is  the interval between the 2.5th and 97.5th percentiles.

* The HDI is also referred to as HPD (high posterior density) in some books. 


# HDI   of unimodal distributions

```{r, out.width='45%', fig.align='center', echo=FALSE}
knitr::include_graphics('HPD-unimodal.png')
```

* The height of the horizontal arrow marks the minimal density exceeded by all $x$ values inside the 95% HDI


# HDI of bimodal distribution
```{r, out.width='45%', fig.align='center', echo=FALSE}
knitr::include_graphics('HPD_bimodal.png')
```

* The HDI is split into two sub-intervals, one for each mode of the distribution. 

* The  characteristics are as before: 
  * The shaded area has total area of 0.95.
  * Any $x$ within such limits has higher probability density than any $x$ outside the limits.