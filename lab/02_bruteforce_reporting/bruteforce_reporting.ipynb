{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 20 # parameter a of the beta prior\n",
    "b = 10 # parameter b of the beta prior\n",
    "theta = 0.25 # true unknown parameter, P(HEAD)\n",
    "n = 100 # number of coin tosses. Try 5000!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "#y = np.random.binomial(n, theta) # random number of heads from the distribution from a binomial distribution\n",
    "y = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical lecture 1: Beta-Binomial, Gridding, Logarithmic tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider a coin that lands heads with probability $\\theta=0.25$ (unknown to us). \n",
    "\n",
    "Our prior knowledge of $\\theta$ is encoded in a Beta distribution (where \"success\" corresponds to the coin landing heads), with parameters $a=20$ and $b=10$:\n",
    "\n",
    "$$ \\theta \\sim \\rm{Beta}(a,b).$$\n",
    "\n",
    "We toss the coin $n=100$ times and observe a number $y=24$ of heads. How does our belief of $\\theta$ change with the measurement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you need the Beta probability distribution $\\rm Beta$ in Python, use `scipy.stats.beta`. If you need the beta function $B$ in Python, it is `scipy.special.beta`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0: Probabilistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Derive and comment the full probabilistic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full probabilistic model is:\n",
    "\n",
    "\\begin{align}\n",
    "\\theta \\sim \\rm{Beta}(a,b)\\\\\n",
    "y \\mid \\theta \\sim \\rm{Bin}(n, \\theta).\n",
    "\\end{align}\n",
    "\n",
    "More explicitly, the prior probability density function (pdf) of $\\theta$ is:\n",
    "\\begin{align}\n",
    "f_{\\rm prior}(\\theta) = \\frac{1}{B(a, b)} \\theta^{a-1} (1-\\theta)^{b-1},\\qquad \\theta \\in (0, 1),\n",
    "\\end{align}\n",
    "and the probability mass function of $y$, conditioned on $\\theta$, is:\n",
    "$$\n",
    "P(y \\mid \\theta) = {{n}\\choose{y}} \\theta^{y} (1-\\theta)^{n-y}.\n",
    "$$\n",
    "\n",
    "\n",
    "It is the classic Beta-binomial model. The random variable $y$ represents the number of success (heads) events over $n$ trials. The success probability $\\theta$ is constant in all flips, which are (conditionally) independent events. For a given value of $\\theta$, the probability of observing $y$ heads in $n$ trials is a binomial. The prior of $\\theta$ is a Beta with *fixed* coefficients $a$ and $b$.\n",
    "\n",
    "The Beta prior distribution is *conjugate* with the binomial likelihood, and thus the posterior of $\\theta$ has a close-form Beta structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Prior distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the probability density function (pdf) of the prior $f_{\\rm prior}(\\theta)$. Explain our prior belief on $\\theta$ in words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior distribution is:\n",
    "\n",
    "$$ f_{\\rm prior}(\\theta) = \\frac{1}{B(a,b)} \\theta^{a-1} (1-\\theta)^{b-1}, \\qquad \\theta \\in (0, 1),$$\n",
    "\n",
    "where $B(a,b)$ is the [beta function](https://en.wikipedia.org/wiki/Beta_function) returning the right normalization constant such that $\\int_{0}^1 f_{\\rm prior}(\\theta) \\; d \\theta = 1.$ <br/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_pdf(theta, a=20, b=10):\n",
    "    return 1/scipy.special.beta(a,b) * theta**(a-1) * (1-theta)**(b-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dtheta = 1e-3 # discretization step for theta\n",
    "theta_vec = np.arange(0, 1, dtheta) # discretized theta range\n",
    "plt.plot(theta_vec, prior_pdf(theta_vec))\n",
    "plt.xlabel(r\"$\\theta$\");\n",
    "plt.title(r\"Prior distribution of $\\theta$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.70370647, 0.67551564])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_var = stats.beta(a, b) # prior random variable object\n",
    "prior_var.rvs(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(np.allclose(prior_var.pdf(0.5) , prior_pdf(0.5))) # our implementation is close to the true value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute the mean, the mode, and the standard deviation of the prior distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6666666666666666, 0.6785714285714286, np.float64(0.08466675133346033))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See formulas in the lecture notes \"The beta-binomial model\"\n",
    "prior_mean = a/(a+b)\n",
    "prior_mode = (a-1)/(a + b - 2)\n",
    "prior_sd = np.sqrt(a*b/((a+b)**2*(a+b+1)))\n",
    "prior_mean, prior_mode, prior_sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the prior distribution together with its mean and mode, and comment the result. Is the prior representative of the true value of $\\theta$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## 2: Posterior distribution - exact derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Obtain the posterior distribution in closed form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Beta-binomial model:\n",
    "\\begin{align}\n",
    "\\theta &\\sim \\mathrm{Beta}(a,b)\\\\\n",
    "y \\mid\\theta &\\sim \\mathrm{Bin}(n, \\theta)\n",
    "\\end{align}\n",
    "is **conjugate**, meaning that the posterior distribution of $\\theta$ given $y$ belongs to the same family as the prior.\n",
    "\n",
    "It is well known that:\n",
    "$$\\theta \\mid y \\sim \\mathrm{Beta}(a+y, b + n - y)$$\n",
    "Which means:\n",
    "$$f_{\\rm post}(\\theta) = f(\\theta \\mid y) = \\frac{1}{B(a+y,b+n-y)} \\theta^{a+y-1} (1-\\theta)^{b+n-y-1}, \\qquad \\theta \\in (0, 1).$$\n",
    "\n",
    "\n",
    "The result can be derived from Bayes' rule:\n",
    "\n",
    "\\begin{equation*}\n",
    "f(\\theta \\mid y) = \\frac{P(y \\mid \\theta) f_{\\rm prior}(\\theta)}{P(y)} \\propto \\theta^{a+y-1} (1-\\theta)^{b+n-y-1}.\n",
    "\\end{equation*}\n",
    "Only a $\\mathrm{Beta}$ random variable with parameters $a+y$ and $b+n-y$ has a pdf proportional to $\\theta^{a+y-1} (1-\\theta)^{b+n-y-1}$. The posterior is then a $\\mathrm{Beta}(a+y, b + n - y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_post = a + y\n",
    "b_post = b + n - y\n",
    "post_var = stats.beta(a=a_post, b=b_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the posterior distribution together with the prior and the true value of $\\theta$. Comment on the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute the likelihood function $\\mathcal{L}(\\theta) = P(y \\mid \\theta)$ on a grid, for fixed $y$ equal to the measured number of heads and values of $\\theta$ in the numerical range $[0, \\,1]$. You may disregard the multiplicative factor that does not depend on $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lik(theta):\n",
    "    return (theta ** y) * (1 - theta)**(n-y)\n",
    "\n",
    "lik_vec = lik(theta_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the likelihood function. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Point estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Obtain the maximum likelihood (ML) point estimate $\\theta^{\\rm ml}$.\n",
    "\n",
    "Hint: $\\theta^{\\rm ml}$ is the value of $\\theta$ corresponding to the maximum of the likelihood function $\\mathcal{L}(\\theta)$:\n",
    "\n",
    "$$\\theta^{\\rm ml} = \\arg \\max_{\\theta} \\mathcal{L}(\\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Obtain the maximum a posteriori (MAP) point estimate $\\theta^{\\rm MAP}$.\n",
    "\n",
    "Hint: $\\theta^{\\rm MAP}$ is the value of $\\theta$ corresponding to the maximum of the posterior density $f_{\\rm post }(\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Obtain the posterior mean point estimate $E[\\theta \\mid y]$<br/>\n",
    "Hint: You can use close-form formulas for the mean of the Beta random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Posterior distribution - gridding, aka brute-force approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "* Obtain a numerical approximation of the posterior $f_{\\rm post}(\\theta)$, by normalizing the product $\\mathcal{L}(\\theta) \\cdot f_{\\rm prior}(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "By definition, the posterior distribution $f_{\\rm post}(\\theta)$ is:\n",
    "$$f_{\\rm post}(\\theta) = f(\\theta \\mid  y) = \\frac{\\overbrace{P(y \\mid \\theta)}^{=\\mathcal{L}(\\theta)} \\cdot f_{\\rm prior}(\\theta)}{P(y)},$$\n",
    "where $P(y) = \\int{P(y \\mid\\theta)\\; d\\theta}$.\n",
    "\n",
    "Thus, $f_{\\rm post}(\\theta)$ corresponds to the product $\\mathcal{L}(\\theta) \\cdot f_{\\rm prior}(\\theta)$, up to a multiplicative constant to be determined.\n",
    "\n",
    "We have:\n",
    "$$f_{\\rm post}(\\theta) = \\frac{1}{Z} \\cdot \\mathcal{L}(\\theta) f_{\\rm prior}(\\theta),$$\n",
    "\n",
    "where the normalization constant $Z$ must be chosen to satisfy:\n",
    "\n",
    "$$\\int_\\theta f_{\\rm post}(\\theta) \\; d\\theta = 1,$$\n",
    "thus\n",
    "\n",
    "$$Z = \\int_\\theta \\mathcal{L}(\\theta) f_{\\rm prior}(\\theta) \\; d\\theta = 1.$$\n",
    "\n",
    "Any numerical integration method can be used to approximate the integral above. Easiest choice: Riemann sum on a uniform grid, with step size $\\Delta \\theta$\n",
    "$$ Z \\approx \\Delta \\theta \\sum_{i} \\mathcal{L}(\\theta_i) f_{\\rm prior}(\\theta_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "p_theta_post = lik_vec * prior_var.pdf(theta_vec)\n",
    "Z = (np.sum(p_theta_post) * dtheta)\n",
    "p_theta_post = p_theta_post/Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**NOTE**: The Beta-Binomial case is a very special and lucky (conjugate) combination where we can compute $f_{\\rm post}(\\theta)$ analytically. The gridding approach above is also applicable to non-conjugate prior/likelihood combinations. However, it is only feasible in 1-3 dimensions  (curse of dimensionality!). For non-conjugate prior/likelihood combinations in higher dimensions, more advanced methods are needed. See following lectures!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Verify that the numerical approximation of the posterior is accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Obtain a scaled version of the likelihood function $\\mathcal{L}(\\theta)$, such that $\\int_{0}^{1} \\mathcal{L}(\\theta)\\; d\\theta = 1$. What does this function represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the posterior, the prior, and the scaled likelihood on the same axes. Comment the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Try to execute the code above with a larger value of n (ex. n = 5000). What happens? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter the logarithm: log-likelihood and log-posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To avoid numerical difficulties, we may instead compute the *logarithm* of the likelihood and the posterior density. Starting from the definition:\n",
    "\n",
    "$$f_{\\rm post}(\\theta) = f(\\theta \\mid y) = \\frac{{P(y \\mid \\theta)} \\cdot f_{\\rm prior}(\\theta)}{P(y)},$$\n",
    "\n",
    "we obtain the following formula for the log-posterior $g(\\theta) = \\log f_{\\rm post}(\\theta)$:\n",
    "$$g(\\theta) = \\log f_{\\rm post}(\\theta) = \\overbrace{\\log P(y \\mid \\theta)}^{=\\ell(\\theta)} + \\log f_{\\rm prior}(\\theta) + \\log P(y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let us compute the *log-likelihood* $\\ell(\\theta)$:\n",
    "\n",
    "$$\\ell(\\theta) = \\log \\mathcal{L}(\\theta) = \\log P(y \\mid \\theta) = \n",
    "\\log {{n}\\choose{y}} \\theta^{y} (1-\\theta)^{(n-y)}=\\log{{n}\\choose{y}} + y \\log \\theta + (n-y) \\log (1-\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluate and plot the log-likelihood $\\ell(\\theta)$ up to an additive constant over a grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluate and plot the log-prior $\\log f_{\\rm prior}(\\theta)$ up to an additive additive constant over a grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluate and plot the log-posterior $g(\\theta)$ up to an additive constant over a grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Obtain the normalized posterior $f(\\theta \\mid y)$ over a grid starting from the unnormalized log-posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative implementation using the logsumexp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous implementation, we have first added the constant ``-np.max(log_post)`` to the log-posterior for numerical reasons. We then exponentiated and finally normalized the result so that it integrates to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a \"magic\" normalization constant that help us achieve the same result even more easily, and it is based on the ``logsumexp`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(4.835708180330829)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randn(100)\n",
    "scipy.special.logsumexp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0000000000000004)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.exp(x - scipy.special.logsumexp(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_post_vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m LSE = scipy.special.logsumexp(log_post_vec)\n\u001b[32m      2\u001b[39m post_norm_ = \u001b[32m1\u001b[39m/dtheta * np.exp(log_post_vec - LSE)\n",
      "\u001b[31mNameError\u001b[39m: name 'log_post_vec' is not defined"
     ]
    }
   ],
   "source": [
    "LSE = scipy.special.logsumexp(log_post_vec)\n",
    "post_norm_ = 1/dtheta * np.exp(log_post_vec - LSE) # same as post_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, $\\mathrm{logsumexp} : \\mathbb{R}^{n} \\mapsto \\mathbb{R}$ is a *vector-to-scalar* function. For vector $\\mathrm{x} = [x_0\\; x_1\\; \\dots \\; x_{n-1}]^\\top \\in \\mathbb{R}^{n}$, it is defined by:\n",
    "\n",
    "$$\\mathrm{logsumexp}(\\mathrm{x}) = \\log \\sum_{i=0}^{n-1} e^{x_i}.$$\n",
    "\n",
    "Literally, it computes the log of the sum of the exponentiated elements of $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have found the \"magic constant\" that provides a good scaling and the right normalization constant in one shot! How does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us denote as $\\tilde g(\\theta)$ the log-posterior up to an additive constant $K$:\n",
    "\n",
    "$$ \\tilde g(\\theta) = \\log f_{\\rm post}(\\theta) + K \\Rightarrow f_{\\rm post}(\\theta) = \\frac{1}{Z} e^{\\tilde g(\\theta)}.$$\n",
    "\n",
    "The constant $Z$ is such that $Z e^{\\tilde g(\\theta)}$ integrates to 1. Then,\n",
    "\n",
    "$$ f_{\\rm post}(\\theta) = \\frac{e^{\\tilde g(\\theta)}}{ \\int{e^{\\tilde g(\\theta_i)}}\\; d\\theta}\n",
    "\\approx  \\frac{e^{\\tilde g(\\theta)}}{\\Delta \\theta \\sum_i{e^{\\tilde g(\\theta_i)}}} = \n",
    "\\frac{e^{\\tilde g(\\theta)}}{\\Delta \\theta e^{\\log \\sum_i{e^{\\tilde g(\\theta_i)}}}} = \\frac{1}{\\Delta \\theta} \\frac{e^{\\tilde g(\\theta)}}{e^{\\rm logsumexp(G)}} = \\frac{1}{\\Delta \\theta}  e^{\\tilde g(\\theta) - \\mathrm{logsumexp}(G)},$$\n",
    "where $\\mathrm{G} = [\\tilde g(\\theta_0), \\tilde g(\\theta_1), \\dots, \\tilde g(\\theta_{n-1})]$ is the vector of the samples of $\\tilde g$ at the grid points used for numerical integration of $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The logsumexp internals\n",
    "\n",
    "* Internally, the logsumexp function avoid exponentiation with very small/very large numbers exploiting the identity:\n",
    "\n",
    "$$ {\\rm logsumexp}(x) = \\log \\sum_i e^{x_i} = c + \\log \\sum_i e^{(x_i - c)}.$$\n",
    "\n",
    "* A robust implementation of logsumexp sets $c = \\max(x_0, \\dots, x_{n-1})$. This ensures that the largest exponentiated term is always $e^0=1$!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x):\n",
    "    c = x.max()\n",
    "    return c + np.log(np.sum(np.exp(x - c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Whenever we have a summation over the original probability/likelihood variables, we can alternatively perform a logsumexp operation over the corresponding the log-probability/log-likelihood variables. \n",
    "\n",
    "* Imagine we have a vector $x$ of log-probabilities and want to compute the sum of the probabilities. We have:\n",
    "$$\\sum_{i} e^{x_i}\n",
    "% = \\log e^{\\overbrace{\\log \\sum_{i} e^{x_i}}^{= {\\rm logsumexp}(x)}} \n",
    "= e^{{\\rm logsumexp}(x)}.\n",
    "%\\log \\left(\\sum_{i} e^{x_i}\\right) \n",
    "% = \\log e^{\\overbrace{\\log \\sum_{i} e^{x_i}}^{= {\\rm logsumexp}(x)}} \n",
    "%= {{\\rm logsumexp}(x)}.\n",
    "$$\n",
    "\n",
    "* If the probabilities $e^{x_i}$ are small (i.e., $x_i$ are negative values with a large modulus), direct evaluation of the left-hand side may fail and the logsumexp implementation at the right-hand side is to be preferred. \n",
    "    \n",
    "* If we are interested in a result in the log-domain, the exponentiation at the right-hand side (which could also be problematic) is avoided.\n",
    "\n",
    "* In general, it is suggested to perform as many operations as possible on log-variables and go back to probabilities/likelihoods only when it is strictly necessary (and numerically safe!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
