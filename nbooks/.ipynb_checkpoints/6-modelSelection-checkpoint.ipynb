{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Haujd00xUk8Q"
   },
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import arviz as az\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "az.style.use('arviz-darkgrid')\n",
    "np.random.seed(44)\n",
    "sns.set_theme()\n",
    "plt.rcParams['font.size'] = 15\n",
    "plt.rcParams['legend.fontsize'] = 'medium'\n",
    "sns.set(rc={'figure.figsize':(7,3)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian model selection\n",
    "\n",
    "**Bayesian Data Analysis and Probabilistic Programming**\n",
    "\n",
    "Giorgio Corani - giorgio.corani@supsi.ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "> Richard McElreath, \"Statistical Rethinking\", Chap. 7  (https://github.com/rmcelreath/stat_rethinking_2024/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8wt9dlVfUk8C"
   },
   "source": [
    "# Model selection \n",
    "\n",
    "Model selection estimates the out-of-sample performance of models and allows choosing among competing models such as:\n",
    "\n",
    "* models with different priors (e.g., compare hierarchical and unpooled model)\n",
    "* models with different likelihoods (e.g., compare regression model with student and gaussian likelihood)\n",
    "* regression models with different set of covariates (feature selection). For instance, compare:\n",
    "    *  $Y = \\alpha + \\beta_1 X_1$\n",
    "    * $Y = \\alpha + \\beta X_1 + \\beta_2 X_2$\n",
    "\n",
    "* As an example, we consider the student and the Gaussian regression model fitted on a data set containing an outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression data with outliers  (example from lecture on linear regression)\n",
    "* The following cells of codes fit the linear regression model with normal and student likelihood. The code is the same of the lecture on linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "TGlPozDfUk8b"
   },
   "outputs": [],
   "source": [
    "# regression data with outlier\n",
    "x  = np.array([10.,  8., 13.,  9., 11., 14.,  6.,  4., 12.,  7.,  5.])\n",
    "y  = np.array([ 7.46,  6.77, 12.74,  7.11,  7.81,  8.84,  6.08,  5.39,  8.15, 6.42,  5.73])\n",
    "\n",
    "#we  center X\n",
    "x_c = (x - x.mean())\n",
    "plt.plot(x_c, y, '*');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_x = x_c.std()\n",
    "s_y = y.std()\n",
    "y_bar = y.mean()\n",
    "\n",
    "#normal model\n",
    "with pm.Model() as normal_model:\n",
    "    #priors on intercept, slope and standard deviation of noise for standardized data\n",
    "    alpha   = pm.Normal ('alpha', mu=y_bar, sigma=2 * s_y)\n",
    "    beta    = pm.Normal ('beta',  mu=0,  sigma= 2.5 * s_y / s_x)\n",
    "    sigma   = pm.HalfNormal ('sigma', sigma= 1.5 * s_y)\n",
    "\n",
    "    y_pred   = pm.Normal ('y_pred', mu= alpha + beta * x_c,  sigma=sigma, observed=y)\n",
    "\n",
    "    #the kwargs is needed to store the log_likelihood associated to each sample. Will be used later for a statistical\n",
    "    #comaprison between the two models\n",
    "    normal_trace   = pm.sample(idata_kwargs={\"log_likelihood\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model with student likelihood\n",
    "with pm.Model() as st_model:\n",
    "    #priors on intercept, slope and standard deviation of noise for standardized data\n",
    "    alpha_st   = pm.Normal ('alpha_st', mu=y_bar, sigma=2 * s_y)\n",
    "    beta_st    = pm.Normal ('beta_st',  mu=0,  sigma= 2.5 * s_y / s_x)\n",
    "    sigma_st   = pm.HalfNormal ('sigma_st', sigma= 1.5 * s_y)\n",
    "\n",
    "    y_pred   = pm.StudentT ('y_pred', mu= alpha_st + beta_st * x_c,  sigma=sigma_st, nu=4, observed=y)\n",
    "\n",
    "    #the kwargs is needed to store the log_likelihood associated to each sample. Will be used later for a statistical\n",
    "    #comaprison between the two models\n",
    "    st_trace   = pm.sample(idata_kwargs={\"log_likelihood\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LuE8sJ-IUk83"
   },
   "source": [
    "# The Student model fits the correct line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the two models\n",
    "#posterior mean of the parameters, robust model\n",
    "post_st    = az.extract_dataset(st_trace)\n",
    "a_st_hat   = post_st.alpha_st.values.mean()\n",
    "b_st_hat   = post_st.beta_st.values.mean()\n",
    "\n",
    "#posterior mean of the parameters, gaussian model\n",
    "post_gauss  = az.extract_dataset(normal_trace)\n",
    "a_hat       = post_gauss.alpha.values.mean()\n",
    "b_hat       = post_gauss.beta.values.mean()\n",
    "\n",
    "plt.plot(x_c, a_st_hat + b_st_hat * x_c,  c='k', lw=2, label='robust')\n",
    "plt.plot(x_c,  a_hat + b_hat * x_c,  c='b', lw=2, label='gauss')\n",
    "plt.plot(x_c, y, '*', markersize=10, label='obs')\n",
    "plt.xlabel('$x$', fontsize=16)\n",
    "plt.ylabel('$y$', rotation=0, fontsize=16)\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Student model  provides less uncertain estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(st_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(normal_trace)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The need for statistical model selection\n",
    "\n",
    "* The model with Student likelihood looks qualitatively better but we need a data-driven criterion to decide.\n",
    "\n",
    "* In machine learning we use k-fold cross-validation for model selection.\n",
    "\n",
    "* `loo` (leave-one-out) is k-fold cross validation in which each fold contains only a unique observation.\n",
    " \n",
    "* The trouble with leave-one-out cross-validation is that, if we have 1000 observations, we need refitting 1000 times the model.\n",
    "\n",
    "* We will see how WAIC approximates the cross-validation score without actually re-running the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log-score for categorical variables\n",
    "\n",
    "* Consider predicting probabilistically the variable {rain, no rain}.\n",
    "* In table we have the probabilistic prediction by two forecasters, who report their estimated probability of rain.\n",
    "\n",
    "|  | Day 1 | Day 2 | Day 3 | Day 4 |\n",
    "|---|:---:|:---:|:---:|:---:|\n",
    "| Observations | rain | no rain | rain | rain |\n",
    "|  |  |  |  |  |\n",
    "| p(rain) |  |  |  |  |\n",
    "| Forecaster A | 0.6 | 0.4 | 0.6 | 0.7 |\n",
    "| Forecaster B | 0.5 | 0.4 | 0.5 | 0.6 |\n",
    "\n",
    "\n",
    "* Log-score: log-probability of the actual event, summed over all days. Higher is better.\n",
    "  \n",
    "* The log score of forecaster A is:\n",
    "$$ \\log(.6) + \\log(1-.4) + \\log(.6) + \\log (.7) =  -2.29 $$\n",
    "\n",
    "* The log score of forecaster B is:\n",
    "$$ \\log(.5) + \\log(1-.4) + \\log(.5) + \\log (.6) =  -2.81 $$\n",
    "\n",
    "* Forecasters A produces better forecast and his log-score is higher.\n",
    "\n",
    "* Log-score are negative; hence the higher scores are the 'less negative' ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log-score for real-valued variables: log-predictive density (lpd)\n",
    "\n",
    "Consider the prediction for a real-valued variable.\n",
    "\n",
    "|  | Day 1 | Day 2 | Day 3 | Day 4 |\n",
    "|---|:---:|:---:|:---:|:---:|\n",
    "| Observations | 1000 | 1200 | 800 | 780 |\n",
    "|  |  |  |  |  |\n",
    "| Prediction |  |  |  |  |\n",
    "| Forecaster A | $N(800, 100^2)$ | $N(1500, 150^2)$ | $N(750, 50^2)$ | $N(760, 20^2)$ |\n",
    "\n",
    "\n",
    "\n",
    "* The log score of forecaster A is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{log } N(1000 | \\mu= 800, \\sigma=100) +  \\text{log } N(1200 | 1500, 150) + \\text{log } N(800 | 750, 50) + \\text{log } N(780 | 760, 20)\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "* Real-valued variables are predicted with a density. The log-score is referred to as *log-predictive density* (lpd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the log score of the above cell\n",
    "from scipy.stats import norm\n",
    "\n",
    "lpd = (norm.logpdf(x=1000, loc=800, scale=100) + norm.logpdf(x=1200, loc=1500, scale=150) +  norm.logpdf(x=800, loc=750, scale=50) +  \n",
    "    norm.logpdf(x=800, loc=750, scale=50))\n",
    "\n",
    "print(lpd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap\n",
    "\n",
    " $$ \\displaystyle \\text{log score} \\underbrace{= \\text{lpd}}_{\\text{real values variables}} = \\sum_i^n \\log p(y_i)$$\n",
    "\n",
    "* $p(y_i)$ is the predictive density evaluated in correspondence of the actual value $y_i$\n",
    "\n",
    "* $n$ is the training set size\n",
    " \n",
    "* Depending on whether the variable is categorical or numerical, $p$ is a probability or a probability density.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian log-score\n",
    "\n",
    "* Given a model with parameters $\\theta$, whose posterior distribution is given in form of samples $\\theta_s$ ($s$ = 1,2... 4000)\n",
    "* Average the log-score over  $\\theta_s$\n",
    "* It is sometimes referred to as 'lppd' (log-pointwise-predictive-density):\n",
    "\n",
    "  $$ \\text{lppd} = \\sum_i^n \\log \\underbrace{\\frac{1}{S} \\sum_s p(y_i | \\theta_s)}_{\\text{lpd averaged over parameter samples}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Predicting predictive accuracy\n",
    "\n",
    " * The lppd is evaluated on the training data.\n",
    " * The lppd on test data will be generally worse (more negative)\n",
    " * How to estimate the lppd on test data (out-of-sample)?\n",
    "    * cross-validation (k-fold, leave-one-out); requires refitting the model many times and computing lppd on the data which are left out.\n",
    "    * as an alternative, information criteria provide a theoretical estimate of the  out-of-sample lppd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # WAIC\n",
    "\n",
    "* WAIC estimates the out-of-sample lppd; its approximation converges to the leave-one-out estimate  in a large sample.\n",
    "\n",
    "* WAIC is composed by two pieces, computed from posterior samples of the parameters.\n",
    "\n",
    "* WAIC  = lppd - penalty, where the penalty is proportional to the variance in the posterior predictions.\n",
    "\n",
    "$$ \\text{WAIC} = \\text{lppd} - \\underbrace{\\sum_i \\text{var}_{\\theta} \\log (p(y_i | \\theta) )}_{\\text{penalty}}$$\n",
    "\n",
    "> The penalty term means, “compute the variance in log-probabilities for each observation i, and then sum up these variances to get the total penalty.” Each observation has its own penalty. And since these scores measure overfitting risk, you can also assess overfitting risk at the level of each observation. **Stat Rethinking, Sec. 7.2**\n",
    "\n",
    "*  WAIC decomposes over the individual observations. Indeed some observations are much harder to predict than others and may also have different uncertainty.\n",
    "\n",
    "*  The above expression of WAIC is in the 'log scale'. Thus, higher (less negative) is better.\n",
    "\n",
    "*  Other versions of WAIC include a multiplicative constant (-1 or -2) which change the sign of WAIC and have to be interpreted in the opposite way.\n",
    "\n",
    " * In `arviz`, specificy `log_scale` when computing WAIC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom implementation of the WAIC for the gaussian and the student model\n",
    "\n",
    "* We implement:\n",
    "\n",
    "    * a function which computes lppd\n",
    "    * a function which computes the penalty.\n",
    "\n",
    "\n",
    "\n",
    "* We are comparing models with different likelihoods; the density function used in WAIC is normal or student depending on which model we are evaluating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x7NZN7JeUk87",
    "outputId": "2ba70c7c-82e2-4160-992a-0e9295285e88"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy.stats import t\n",
    "\n",
    "# computes lppd given a data set of observations (x and y), the posterior samples of parameters alpha, beta and sigma and the type\n",
    "# of predictive density (normal or st).\n",
    "\n",
    "def get_lppd (x, y, alpha, beta, sigma, type='normal'):\n",
    "\n",
    "    lppd = 0\n",
    "    tmp = np.zeros(len(alpha))\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        # vector of 4000 means\n",
    "        mu = alpha + beta * x[i]\n",
    "\n",
    "        # the pdf is computed *without* the log. We apply the log later on the average predictive probability.\n",
    "        \n",
    "        if type == 'normal':\n",
    "            # set of 4000 predictive densities one for each parameter sample\n",
    "            tmp = norm.pdf(y[i], loc=mu, scale=sigma)\n",
    "        elif type == 'st':\n",
    "            tmp = t.pdf(y[i], df=4, loc=mu, scale=sigma)\n",
    "            \n",
    "        avg_pd = np.mean(tmp)\n",
    "        lppd = lppd + np.log(avg_pd)\n",
    "    return( lppd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each instance, variance of the log-score across the different samples.\n",
    "# as in lppd case, we use a density which matches the likelihood of the model\n",
    "def get_penalty (x, y, alpha, beta, sigma, type='normal'):\n",
    "    penalty = 0\n",
    "    tmp = np.zeros(len(alpha))\n",
    "    for i in range(len(y)):\n",
    "        # vector of 4000 means\n",
    "        mu = alpha + beta * x[i]\n",
    "\n",
    "        if type == 'normal':\n",
    "            # set of 4000 log-predictive densities for the current sample\n",
    "            tmp = norm.logpdf(y[i], loc=mu, scale=sigma)\n",
    "        elif type == 'st':\n",
    "            tmp = t.logpdf(y[i], df=4, loc=mu, scale=sigma)\n",
    "            \n",
    "        var_log_pd = np.var(tmp)\n",
    "        penalty = penalty + var_log_pd\n",
    "    return(penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior samples of the two models\n",
    "\n",
    "# student model\n",
    "post_st    = az.extract_dataset(st_trace)\n",
    "a_st       = post_st.alpha_st.values\n",
    "b_st       = post_st.beta_st.values\n",
    "s_st       = post_st.sigma_st.values\n",
    "\n",
    "# gaussian model\n",
    "post_gauss     = az.extract_dataset(normal_trace)\n",
    "a      = post_gauss.alpha.values\n",
    "b      = post_gauss.beta.values\n",
    "s      = post_gauss.sigma.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# densities are not probabilities, they can be larger than 1.\n",
    "# hence the student model has lppd of 17, much higher than that of the normal model.\n",
    "\n",
    "lppd_normal = get_lppd (x_c, y, a, b, s, type='normal')# -17.30\n",
    "lppd_st     = get_lppd (x_c, y, a_st, b_st, s_st, type='st')#17.6\n",
    "\n",
    "penalty_normal =  get_penalty (x_c, y, a, b, s, type='normal')\n",
    "penalty_st = get_penalty (x_c, y, a_st, b_st, s_st, type='st')\n",
    "\n",
    "waic_normal = (lppd_normal - penalty_normal)\n",
    "waic_st =  (lppd_st - penalty_st)\n",
    "\n",
    "print (\"waic_normal: \" +  str(waic_normal))\n",
    "print (\"waic_st: \" +  str(waic_st))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WHEg3KRWUk9A"
   },
   "source": [
    "# WAIC in Pymc\n",
    "PyMC3 includes two functions for model comparison:\n",
    "* `compare` computes the  WAIC of a set of given models (remember to specify the log scale).\n",
    "* `plot_compare` graphically compares the  WAIC of competing models, showing also their uncertainties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "skwg6Sr2Uk9A",
    "outputId": "6cb9b914-af2b-4857-9382-3aed58b82147"
   },
   "outputs": [],
   "source": [
    "from arviz import compare\n",
    "\n",
    "df_comp_WAIC = az.compare({'normal':normal_trace, 'student':st_trace}, ic='waic', scale='log')\n",
    "df_comp_WAIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x33jOJ5zUk9D"
   },
   "source": [
    "\n",
    "1. The first column is the **rank** of the models; zero is best.\n",
    "\n",
    "2. The second column contains the **values of WAIC**.  **elpd_waic** stands for expected lpd, hence out-of-sample lpd (here, lpd and lppd are used equivalently).\n",
    "\n",
    "3. The third column is the penalty term. In our case, the normal model has much larger of the log-score across the parameter samples.\n",
    "When comparing instead models of different complexity (.e.g, regression models with different number of covariates), more complex models are more flexible; it follows have larger variance of the lpd (i.e., higher risk of overfitting) and higher penalty.\n",
    "\n",
    "5. The fourth column is the **difference between the WAIC of the top-ranked model and the WAIC of each other model**.  It is 0 for the first model. \n",
    "\n",
    "6. The fifth column contains **model weights** (not covered in this course). Sometimes when comparing models, we do not want to select the \"best\" model, instead we want to perform predictions by averaging along all the models (or at least several models). Ideally we would like to perform a weighted average, giving more weight to the model that seems to explain/predict the data better. There are many approaches to perform this task, one of them is to use Akaike weights based on the values of WAIC for each model. These weights can be loosely interpreted as the probability of each model (among the compared models) given the data. One caveat of this approach is that the weights are based on point estimates of WAIC (i.e. the uncertainty is ignored).\n",
    "\n",
    "7. The sixth column records the **standard error for the WAIC computations**. The standard error can be useful to assess the uncertainty of the WAIC estimates. Nevertheless, caution need to be taken because the estimation of the standard error assumes normality and hence could be problematic when the sample size is low.\n",
    "\n",
    "\n",
    "8. The second-last column is a flag for **warnings**. A value of `True` indicates that the computation of WAIC may not be reliable.\n",
    "\n",
    "9. The last column indicates the **scale** used for the information criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_compare(df_comp_WAIC);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The 'psis-loo' score\n",
    "  \n",
    "* An alternative to WAIC is the 'psis-loo' score, which is numerically more realible than WAIC. The derivation of the 'psis-loo' is more complicated and we cannot cover it.\n",
    "\n",
    "* As the name suggest, 'psis-loo' is an estimate of the leave-one-out lpd, without actually performing leave-one-out.\n",
    "\n",
    "* In arviz it is referred to as 'loo'.\n",
    "\n",
    "* Below we show how to use it  and we see that the results are numerically close to those of WAIC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arviz import compare\n",
    "\n",
    "df_comp_loo = az.compare({'normal':normal_trace, 'student':st_trace}, ic='loo', scale='log')\n",
    "df_comp_loo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn\n",
    "\n",
    "* Generate a data set of size $n$=30 from  $Y = 1 + 0.5 X_1 + \\epsilon$ with:\n",
    "    *  $X_1 \\sim N(0,1)$\n",
    "    *  $\\epsilon \\sim N(0,0.5)$\n",
    "* Generate two further fictitious covariates $X_2 \\sim N(0,1)$.\n",
    "* Perform model selection between two regression model with different covariates:\n",
    "    * the first model has covariate $X_1$ \n",
    "    * the second  model has covariate $X_1, X_2$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the data\n",
    "from scipy.stats import norm\n",
    "\n",
    "#generate the data\n",
    "n = 30\n",
    "\n",
    "# if we do not specify loc and scale, 0 and 1 are assumed\n",
    "# The X are already centered. If they were not centered, we should center before fitting the model.\n",
    "x1 = norm.rvs(size=n)\n",
    "x2 = norm.rvs(size=n)\n",
    "x3 = norm.rvs(size=n)\n",
    "y  = 1 + 0.5 * x1 + norm.rvs(size=n, scale=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model with a single covariate\n",
    "s_x = x1.std()\n",
    "s_y = y.std()\n",
    "y_bar = y.mean()\n",
    "\n",
    "with pm.Model() as x1_model:\n",
    "    #priors on intercept, slope and standard deviation of noise for standardized data\n",
    "    alpha   = pm.Normal ('alpha', mu=y_bar, sigma=2 * s_y)\n",
    "    beta    = pm.Normal ('beta',  mu=0,  sigma= 2.5 * s_y / s_x)\n",
    "    sigma   = pm.HalfNormal ('sigma', sigma= 1.5 * s_y)\n",
    "\n",
    "    y_pred   = pm.Normal ('y_pred', mu= alpha + beta * x1,  sigma=sigma, observed=y)\n",
    "\n",
    "    #the kwargs is needed to store the log_score associated to each sample. Will be used later by WAIC\n",
    "    x1_trace   = pm.sample(idata_kwargs={\"log_likelihood\": True})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model with both covariates\n",
    "s_x2 = x2.std()\n",
    " \n",
    "with pm.Model() as x1_2_model:\n",
    "    #priors on intercept, slope and standard deviation of noise for standardized data\n",
    "    alpha   = pm.Normal ('alpha', mu=y_bar, sigma=2 * s_y)\n",
    "    beta    = pm.Normal ('beta',  mu=0,  sigma= 2.5 * s_y / s_x)\n",
    "    beta2    = pm.Normal ('beta2', mu=0,  sigma= 2.5 * s_y / s_x2)\n",
    "    sigma   = pm.HalfNormal ('sigma', sigma= 1.5 * s_y)\n",
    "\n",
    "    # a more elegant implementation is possible, using pm.dot(beta, X) instead of beta * x1 + beta2 * x2\n",
    "    y_pred   = pm.Normal ('y_pred', mu= alpha + beta * x1 + beta2 * x2,  sigma=sigma, observed=y)\n",
    "\n",
    "    #the kwargs is needed to store the log_score associated to each sample. Will be used later by WAIC\n",
    "    x1_2_trace   = pm.sample(idata_kwargs={\"log_likelihood\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model selection\n",
    "df_comp_WAIC = az.compare({'x1':x1_trace, 'x1_2':x1_2_trace}, ic='waic', scale='log')    \n",
    "df_comp_WAIC "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Section4_9-Model-Comparison.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
