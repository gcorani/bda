{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gcorani/bda/blob/main/5_LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8cKQNrfB4GA"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import pymc as pm\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import arviz as az\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "az.style.use('arviz-darkgrid')\n",
        "np.random.seed(44)\n",
        "sns.set_theme()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTuYJPgaB4GD"
      },
      "outputs": [],
      "source": [
        "plt.rcParams['font.size'] = 15\n",
        "plt.rcParams['legend.fontsize'] = 'medium'\n",
        "plt.rcParams.update({\n",
        "    \"figure.figsize\": [7, 3],\n",
        "    'figure.facecolor': '#fffff8',\n",
        "    'axes.facecolor': '#fffff8',\n",
        "    'figure.constrained_layout.use': True,\n",
        "    'font.size': 14.0,\n",
        "    'hist.bins': 'auto',\n",
        "    'lines.linewidth': 3.0,\n",
        "    'lines.markeredgewidth': 2.0,\n",
        "    'lines.markerfacecolor': 'none',\n",
        "    'lines.markersize': 8.0,\n",
        "})\n",
        "sns.set(rc={'figure.figsize':(7,3)})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbDjrKPpB4GE"
      },
      "source": [
        "# Linear regression\n",
        "\n",
        "\n",
        "Giorgio Corani <br/>\n",
        "\n",
        "\n",
        "*Bayesian Data Analysis and Probabilistic Programming*\n",
        "<br/>\n",
        "<br/>\n",
        "``giorgio.corani@supsi.ch``\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNryWThiB4GF"
      },
      "source": [
        "# Based on..\n",
        "\n",
        "\n",
        "*   Chapter 3 of O. Martin, *Bayesian Analysis with Python, Second Edition*.\n",
        "\n",
        "\n",
        "* Chapter 8 of *the Bayes rule book* https://www.bayesrulesbook.com/chapter-10.html\n",
        "\n",
        "\n",
        "*  Notebook by G. Corani"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBRAIMsZB4GF"
      },
      "source": [
        "# Linear regression\n",
        "\n",
        "* We want to predict the value of $Y$ given the observation of $X$.\n",
        "\n",
        "*  $Y$ is the *dependent* (or  *response*) variable\n",
        "*  $X$  is the *independent*  variable   (or *explanatory variable*   or *covariate*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfmYuzY8B4GF"
      },
      "source": [
        "# Simple linear regression: a single explanatory variable\n",
        "\n",
        "$$Y = \\alpha +  \\beta X  + \\epsilon$$\n",
        "\n",
        "\n",
        "* $\\alpha$  (*intercept*):  predicted value of $Y$ for $X$ = 0; can be   seen as  a constant which calibrates the shift along the y-axis.\n",
        "\n",
        "\n",
        "* $\\beta$ (*slope*):  predicted change in $Y$  for a unit change in  $X$.\n",
        "\n",
        "\n",
        "* $\\epsilon \\sim N(0, \\sigma^2)$ is a random noise which the  scatters the  observations  around the line."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSz4F69KB4GF"
      },
      "source": [
        "# Simple linear regression\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        " <img src='https://drive.google.com/uc?export=view&id=1D2vnXUbyNiFawkmW_b6QMVq0ix7dL7xW' width='600'>\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "*  The noise  $\\epsilon$ implies a deviation from the  linear model. It captures anything that may affect $Y$ other than $X$.\n",
        "\n",
        "* $\\epsilon \\sim N(0, \\sigma^2)$ implies an equal probability of the observations lying above or below the line."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ9dB3RIB4GF"
      },
      "source": [
        "# The effect of $\\sigma$\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        " <img src='https://drive.google.com/uc?export=view&id=1MD68eXgvU6DEKRX95nmXKtdRtTNZ9tFC' width='700'>\n",
        "</div>\n",
        "\n",
        "\n",
        "* $\\sigma$ is the std dev of  $\\epsilon$.\n",
        "\n",
        "* Large $\\sigma$ (left): large variability of the observations around the linear model, weak relationship.\n",
        "\n",
        "* Small $\\sigma$ (right): the observations deviates  little from the  model; strong relationship.\n",
        "\n",
        "* Traditional regression fitted via least squares yields a line which passes through the point $\\bar{x}, \\bar{y}$, which represents the center of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGzr_926B4GG"
      },
      "source": [
        "# The effect of $\\sigma$\n",
        "\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        " <img src='https://drive.google.com/uc?export=view&id=1MD68eXgvU6DEKRX95nmXKtdRtTNZ9tFC' width='700'>\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "* Under the normal assumption:\n",
        "     *  about 68% of the observations  lie  in an interval of $\\pm 1 \\sigma$ around the regression line.\n",
        "    *  how many  observations will lie in an interval of $\\pm 2 \\sigma$ around the regression line?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSVsYbatB4GG"
      },
      "source": [
        "# Multiple linear regression\n",
        "\n",
        "* Linear regression with $k$ explanatory variables:\n",
        "\n",
        "\\begin{align}\n",
        " Y = \\alpha+ \\sum_i β_i X_i +ε,\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "* The coefficients $\\beta_1,…,\\beta_k$ measure the *marginal effects* of each  explanatory variable, i.e. the effect of increasing of one unit the explanatory variable while keeping fixed all the remanining  ones.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p84tzQlWB4GG"
      },
      "source": [
        "# Probabilistic regression\n",
        "\n",
        "* The prediction account for the epistemic uncertainty on ($\\alpha, \\beta, \\sigma$)\n",
        "\n",
        "* Possibility of *robust regression* to deal with outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8leAFMyzB4GG"
      },
      "source": [
        "# Probabilistic  regression\n",
        "\n",
        "\n",
        "\\begin{align*}\n",
        "Y &\\sim N (\\mu=\\alpha + X \\beta,  \\sigma) \\\\\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "*  We must specify a prior distribution for each of  parameter: $\\alpha$, $\\beta$, $\\sigma$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Mt9qfXzB4GH"
      },
      "source": [
        "# Probabilistic  regression\n",
        "\n",
        "* We  independently specify the prior of each parameter.\n",
        "\n",
        "\n",
        "* Ideally, we define the priors based on background information\n",
        "\n",
        "\n",
        "* If this is not possible, it is recommended setting  *weakly informative*  which define the order of magnitude of the parameters, after having scaled  the  data.\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guZiqKJTB4GH"
      },
      "source": [
        "# Using background information\n",
        "\n",
        "Define  a regression model for a bike sharing company, based on:\n",
        "\n",
        "\n",
        "*    For every one degree increase in temperature, rides  increases by about 100 rides; the average increase is between  20 and 180.\n",
        "\n",
        "\n",
        "*    On an average temperature day (65 - 70 °F, that is 18-21 °C), there are around 5000 riders, though this varies between 3000 and 7000.\n",
        "\n",
        "\n",
        "*    At any given temperature, daily ridership varies with a moderate standard deviation of 1250 rides.\n",
        "\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISuhUzOyB4GI"
      },
      "source": [
        "# Prior for the slope $\\beta$\n",
        "\n",
        "\n",
        "> For every one degree increase in temperature, ridership typically increases by 100 rides; the average increase is between  20 and 180.\n",
        "\n",
        "$$ \\beta  \\sim N(100, 40^2)  $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1XQ89glB4GI"
      },
      "source": [
        "# Prior for the intercept $\\alpha$\n",
        "\n",
        "* We have no  information about the intercept, i.e., the average number of rides when the temperature is 0.\n",
        "\n",
        "\n",
        "* We know however the expected number of rides given an average temperature.\n",
        "\n",
        "\n",
        ">  On an average temperature day, there are around 5000 riders, though this  could vary between 3000 and 7000.\n",
        "   \n",
        "\n",
        "\n",
        "*  To use this information, we *center* the temperature $X$:\n",
        "\n",
        "\\begin{align*}\n",
        "X_c = X - \\bar{x}\n",
        "\\end{align*}\n",
        "\n",
        "where $\\bar{x}$ is the average temperature in the sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xchkEUzMB4GI"
      },
      "source": [
        "# Regression with centered data\n",
        "   \n",
        "\\begin{align*}\n",
        "Y & = \\alpha + \\beta X  \\\\\n",
        "Y & = \\alpha + \\beta \\underbrace{(X - \\bar{x})}_{X_c} + \\beta \\bar{x}   \\\\\n",
        "Y & = \\alpha + \\beta X_c + \\beta \\bar{x}   \\\\\n",
        "Y & =  \\underbrace{\\alpha +  \\beta \\bar{x}}_{\\alpha_c} + \\beta X_c   \\\\\n",
        "Y & = \\alpha_c + \\beta X_c    \\\\\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "* The intercept with centered data $\\alpha_c$ is the expected value of $Y$ when $X_c = 0$ i.e., when $X = \\bar{x}$.\n",
        "\n",
        "* $\\alpha_c$ is the average value of $Y$ when $X$ is at its mean.\n",
        "\n",
        "* The meaning of $\\beta$ does not change.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqYqI6KWB4GI"
      },
      "source": [
        "# Centering yields also better sampling\n",
        "\n",
        "* Centering the explanatory variables helps to numerically sample the posterior distribution of the parameters.\n",
        "\n",
        "\n",
        "\n",
        "* Sampling the posterior of the regression model on the raw data might be  slow and inefficient (low ESS, equivalent sample size)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37UhovBZB4GI"
      },
      "source": [
        "# Prior for $\\alpha_c$\n",
        "\n",
        "\n",
        ">  On an average temperature day (65 - 70 degrees), there are around 5000 riders, though this  could vary between 3000 and 7000.\n",
        "   \n",
        "\n",
        "* $\\alpha_c \\sim N(5000, 1000^2)$.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqHQ9A9uB4GJ"
      },
      "outputs": [],
      "source": [
        "# Prior for sigma\n",
        "\n",
        "# > At any given temperature, daily ridership will tend to vary with a moderate standard deviation of 1250 rides.\n",
        "\n",
        "# We can  set:\n",
        "# sigma ~  HalfNormal (1850), as the median of this distribution is around 1250.\n",
        "# The scale of the halfnormal has been found via trial and error.\n",
        "\n",
        "from scipy.stats import halfnorm\n",
        "np.round(halfnorm.median(scale=1850),2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjJNDdhBB4GJ"
      },
      "source": [
        "# The resulting model: bike rides as a function of temperature\n",
        "\n",
        "\n",
        "\\begin{align*}\n",
        "\\alpha_c & \\sim  N (5000, 1000^2) \\\\\n",
        "\\beta & \\sim N(100, 40^2) \\\\\n",
        "\\sigma & \\sim \\text{HalfNormal} (1850)\\\\\n",
        "Y & \\sim N( \\alpha_c + \\beta X_c, \\sigma)\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "* To  fit the model it is necessary to use the centered covariate $X_c = X - \\bar{x}$, where  $\\bar{x}$ is the sample mean."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK2GHVxdB4GJ"
      },
      "source": [
        "# Linear regression in PyMC3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxlB3JZ7B4GK"
      },
      "outputs": [],
      "source": [
        "#load and check the data\n",
        "#500 rows of data\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/gcorani/bda/refs/heads/main/nbooks/data/bikes.csv'\n",
        "bike_data = pd.read_csv(url)\n",
        "rides = bike_data[\"rides\"]\n",
        "temperature = bike_data[\"temp_actual\"]\n",
        "\n",
        "bike_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaiBx7xKB4GK"
      },
      "outputs": [],
      "source": [
        "#centered covariate\n",
        "temperature_c = temperature - temperature.mean()\n",
        "\n",
        "with pm.Model() as reg_model:\n",
        "    alpha_c  = pm.Normal ('alpha_c', mu=5000,  sigma= 1000)\n",
        "    beta     = pm.Normal ('beta',   mu=100,     sigma= 40)\n",
        "    sigma    = pm.HalfNormal ('sigma', sigma=1850 )\n",
        "    y_pred = pm.Normal('y_pred', mu=alpha_c + beta * temperature_c, sigma=sigma, observed=rides)\n",
        "\n",
        "    trace = pm.sample()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhuUHpOHB4GK"
      },
      "outputs": [],
      "source": [
        "#posterior values are reasonably close to our prior guess. Prior information was reliable.\n",
        "az.summary(trace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugMWN9ilB4GK"
      },
      "outputs": [],
      "source": [
        "# Without centered covariates, sampling would be worse\n",
        "az.plot_trace(trace);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dXk6ETNB4GK"
      },
      "source": [
        "# Your turn: define a model of probabilistic regression\n",
        "\n",
        "\n",
        "*    Ridership tends to decrease as humidity increases: for every one percentage point increase in humidity level, ridership tends to decrease by 10 rides, though the decrease could vary between 0 and 20.\n",
        "\n",
        "\n",
        "*    On an average humidity day, there are typically around 5000 riders, the actual number varying between 1000 and 9000.\n",
        "\n",
        "\n",
        " *  Ridership is only weakly related to humidity. At any given humidity, ridership will tend to vary with a large standard deviation of 2000 rides."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3jwGLsKB4GK"
      },
      "source": [
        "# Priors and likelihood"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RpZIN8WB4GK"
      },
      "source": [
        "# Priors\n",
        "\n",
        "* We assume independence of the parameters.\n",
        "\n",
        "\n",
        "* In any case, the  joint prior  of the parameters is thus the product of their marginal pdfs:\n",
        "\n",
        "\\begin{align}\n",
        "f(\\alpha,\\beta,\\sigma)=f(\\alpha)f(\\beta)f(\\sigma)\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXN_58JzB4GL"
      },
      "source": [
        "# Likelihood  \n",
        "\n",
        "\n",
        "* It is the probability of observing our data $\\mathbf{y}$ given a specific value of the parameters $\\alpha,\\beta,\\sigma$\n",
        "\n",
        "\n",
        "* Assuming   independence of the observations, we multiply the normal density of each observation:\n",
        "\n",
        "$$ f(\\mathbf{y}|\\alpha,\\beta,\\sigma)=∏_{i=1}^n N( y_i|  \\alpha + \\beta x_i, \\sigma) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H32xVoYIB4GL"
      },
      "source": [
        "# Your turn\n",
        "\n",
        "\n",
        "* Consider the observations:\n",
        "\n",
        "|$X$|$Y$|\n",
        "|:---:|:---:|       \n",
        "| 1 | 11 |\n",
        "| 5 | 50 |\n",
        "\n",
        "\n",
        "* Which is the likelihood function of  $\\alpha$=2, $\\beta=10$, $\\sigma$=1?\n",
        "\n",
        "\\begin{align*}\n",
        "\\text{normpdf  (x=11, loc = 12, scale = 1) *  normpdf\n",
        "(x=50, loc = 52, scale = 1)} \\\\\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGrzm6NAB4GL"
      },
      "source": [
        "# Posterior\n",
        "\n",
        "The three-dimensional posterior distribution is computed as:\n",
        "\n",
        "\\begin{align}\n",
        "\\underbrace{\n",
        "f ( (\\alpha,\\beta,\\sigma)| \\mathbf{y})  \n",
        "}_{\\text{posterior}}\n",
        "\\propto \\text{prior} \\cdot \\text{likelihood} & =\n",
        "f(\\alpha)f(\\beta)f(\\sigma) \\left[ ∏_i^n f(y_i|\\alpha,\\beta,\\sigma) \\right]\n",
        "\\end{align}\n",
        "\n",
        "* That is, we shall evaluate  prior and  likelihood for each possible value of the parameters and eventually normalize the result. We did  this using Pymc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFcHGu-7B4GL"
      },
      "source": [
        "# Visualizing the model\n",
        "\n",
        "* After having sampled the posterior, we have many posterior samples of type  $<\\alpha_s,\\beta_s, \\sigma_s>$ with $s= 1,2, ....4000$.\n",
        "\n",
        "* For visualizing the model we ignore $\\sigma$.\n",
        "\n",
        "* Each sample  $<\\alpha_s,\\beta_s>$ yields a plausible line.\n",
        "\n",
        "*  We can visualize the average line using the posterior mean of $\\alpha_c$ and $\\beta$:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{\\alpha}_c & =  \\frac{1}{S} \\sum_s \\alpha_{c_s} \\\\\n",
        "\\hat{\\beta} & = \\frac{1}{S} \\sum_s \\beta_s \\\\\n",
        "\\end{align}\n",
        "\n",
        "* We are using the hat to denote the posterior means, obtained by averaring over  the samples of the trace.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZZR9-AXB4GL",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# extract the samples\n",
        "idata = az.extract_dataset(trace)\n",
        "\n",
        "#Mean regression line\n",
        "alpha_c_mean =  idata.alpha_c.values.mean()\n",
        "beta_mean   =  idata.beta.values.mean()\n",
        "plt.plot(temperature_c, alpha_c_mean + beta_mean * temperature_c,\n",
        "         c='black',label='y = {:.1f} + {:.1f} * x'.format(alpha_c_mean, beta_mean))\n",
        "plt.legend();\n",
        "\n",
        "#We plot the  other plausible lines\n",
        "#using the sampled values of alpha_c and beta.\n",
        "# This shows the uncertainty in the estimated parameters (epistemic uncertainty)\n",
        "#To quickly compute the plot, we pick one sample every 10.\n",
        "\n",
        "chosen_samples = range(0, len(idata.alpha_c.values),10)\n",
        "\n",
        "#The uncertainty is shown by plotting a different line for each sample of (alpha_c, beta).\n",
        "for i in range(len(chosen_samples)):\n",
        "  plt.plot(temperature_c,  idata.alpha_c.values[i] + idata.beta.values[i] * temperature_c, c='gray', alpha=0.01)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkedhuFAwBbl"
      },
      "source": [
        "# Discussion\n",
        "\n",
        "* There seems to be only small uncertainty about the estimated model, i.e.,  *epistemic uncertainty* seems to be low.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHneE2L0B4GM"
      },
      "source": [
        "#  Evaluating the  association between $X$ and $Y$\n",
        "\n",
        "* In our visual examination of the plausible linear models, all showed positive slope.\n",
        "\n",
        "* Indeed, there is large posterior probability of $\\beta$>0:\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0kqkviMB4GM"
      },
      "outputs": [],
      "source": [
        "#  Evaluating the positive association between $x$ and $y$\n",
        "# the 95% HDI of $\\beta$  is about 80-100 and  contains only positive values.\n",
        "# We reject the hypothesis of beta being 0.\n",
        "az.summary(trace, var_names=\"beta\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qMuPTZrB4GM"
      },
      "outputs": [],
      "source": [
        "az.plot_posterior(trace, var_names=\"beta\", ref_val=0);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtBPqKVHB4GM"
      },
      "source": [
        "# Making predictions\n",
        "\n",
        "\n",
        "*  Predict the number of rides  for the a temperature of 66 F.\n",
        "\n",
        "\n",
        "* First, we center the value: $x_c = 66 - \\bar{x} = 66 - 63.3 = 2.7$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuKI6Df3B4GM"
      },
      "source": [
        "# Making predictions\n",
        "\n",
        "A probabilistic prediction accounts for:\n",
        "\n",
        "   * the *epistemic* uncertainty about the value of the  parameters\n",
        "   * the *aleatory* uncertainty due to effect of the noise $\\epsilon$\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6c2uoRpB4GN"
      },
      "source": [
        "# Posterior predictive distribution\n",
        "\n",
        "We loop on the rows of the trace.\n",
        "At the row $s$ of the trace we make a prediction using parameter $<\\alpha_s, \\beta_s, \\sigma_s>$:\n",
        "\n",
        "$$y_{_{\\text{pred},s}} \\sim N(\\alpha_{s} + \\beta_s \\cdot 2.7,  \\sigma_s)$$\n",
        "\n",
        "$$\n",
        "\\left[\n",
        "\\begin{array}{lll}\n",
        "\\alpha_{c,1} & \\beta_1 & \\sigma_1 \\\\\n",
        "\\alpha_{c,2} & \\beta_2 & \\sigma_2 \\\\\n",
        "\\vdots & \\vdots & \\vdots \\\\\n",
        "\\alpha_{c,4000} & \\beta_{4000} & \\sigma_{4000} \\\\\n",
        "\\end{array}\n",
        "\\right]\n",
        "\\;\\; \\longrightarrow \\;\\;\n",
        "\\left[\n",
        "\\begin{array}{l}\n",
        "y_{\\text{pred},1} \\\\\n",
        "y_{\\text{pred},2} \\\\\n",
        "\\vdots \\\\\n",
        "y_{\\text{pred},4000} \\\\\n",
        "\\end{array}\n",
        "\\right]\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9xz1zXXB4GN"
      },
      "source": [
        "# Traditional vs Bayesian estimation\n",
        "\n",
        "\n",
        "* Traditional least squares regression: you consider  only a single  parameter estimate, ignoring other plausible models.\n",
        "\n",
        "* In the Bayesian approach, we consider all the plausible models represented in the posterior samples.\n",
        "\n",
        "# Quantifying epistemic uncertainty and aleatory uncertainty\n",
        "\n",
        "$$ \\sigma^2_{\\text{total}} = \\sigma^2_{\\text{epistemic}} +  \\sigma^2_{\\text{aleatory}}$$\n",
        "\n",
        "* $ \\sigma^2_{\\text{total}}$: total variance of the predictive distribution.\n",
        "\n",
        "* $ \\sigma^2_{\\text{epistemic}}$: variance of the expected value $\\hat{\\mu}$, without accouting  for $\\sigma$:\n",
        "\n",
        "$$\n",
        "\\\\\n",
        "\\left[\n",
        "\\begin{array}{ll}\n",
        "\\alpha_{c,1} + x \\beta_1  \\\\\n",
        "\\alpha_{c,2} + x \\beta_2  \\\\\n",
        "\\vdots & \\vdots  \\\\\n",
        "\\alpha_{c,4000} + x \\beta_{4000} \\\\\n",
        "\\end{array}\n",
        "\\right]\n",
        "\\;\\; = \\;\\;\n",
        "\\left[\n",
        "\\begin{array}{l}\n",
        "\\hat{\\mu}_{1} \\\\\n",
        "\\hat{\\mu}_{2} \\\\\n",
        "\\vdots \\\\\n",
        "\\hat{\\mu}_{4000} \\\\\n",
        "\\end{array}\n",
        "\\right]\n",
        "\\\\\n",
        "$$\n",
        "\n",
        "* The variance of the left-hand side is $\\sigma^2_{\\text{epistemic}}$, the variance of the right-hand side is $\\sigma^2_{\\text{total}}$.\n",
        "\n",
        "* We obtain $\\sigma^2_{\\text{aleatory}}$ as:   \n",
        "$$ \\sigma^2_{\\text{aleatory}} = \\sigma^2_{\\text{total}} - \\sigma^2_{\\text{epistemic}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7vHcAx4B4GN"
      },
      "outputs": [],
      "source": [
        "#centered value of x_test\n",
        "x_test = 2.7\n",
        "\n",
        "post = az.extract_dataset(trace)\n",
        "\n",
        "#samples of the different variables. The vector of samples are shown with the underscore _s\n",
        "sigma_s = post.sigma.values\n",
        "alpha_s = post.alpha_c.values\n",
        "beta_s  = post.beta.values\n",
        "\n",
        "y_test = np.zeros(shape=len(sigma_s))\n",
        "\n",
        "#predictive distribution\n",
        "preds  = np.random.normal (loc = alpha_s + beta_s * x_test, scale = sigma_s)\n",
        "\n",
        "#epistemic uncertainty, i.e., variance of the expected value\n",
        "mu_hat  =  alpha_s + beta_s * x_test\n",
        "epistemic_var = np.var (mu_hat)\n",
        "aleatory_var =  np.var(preds) - epistemic_var\n",
        "\n",
        "print(\"aleatory var: \" + str (np.round(aleatory_var,0) ) + \"\\n\" + \"epistemic var: \" + str( np.round(epistemic_var,0)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX8FgCvPB4GN"
      },
      "source": [
        "## Discussion\n",
        "\n",
        "* The aleatory uncertainty is much larger than epistemic uncertainty.\n",
        "\n",
        "* Collecting further samples will not reduce much the uncertainty.\n",
        "\n",
        "* To reduce uncertainty, we better add further covariates (e.g., weekend vs weekday) which provide information currenly missing in our minimal model, in order to reduce $\\sigma$ and thus the aleatory uncertainty.\n",
        "\n",
        "* Try to add the weekend covariate and see how the uncertainty of the model changes.\n",
        "\n",
        "# How to set the priors if background information is not available\n",
        "\n",
        "* Sometimes no background information is available.\n",
        "\n",
        "\n",
        "* We use in this case default priors which are based on the scale of the data. These priors are broad  and provides  information about the order of magnitude of the variables.\n",
        "\n",
        "\n",
        "* See for https://cran.r-project.org/web/packages/rstanarm/vignettes/priors.html for more details.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9BdwaeZB4GN"
      },
      "source": [
        "# Default priors in case of no background knowledge\n",
        "\n",
        "* These priors are broad, but nevertheless informativa about the order of magnitude of the parameter.\n",
        "\n",
        "\n",
        "*  $\\alpha_c \\sim N(\\bar{y}, 2 s_y )$:\n",
        "    * we expect $\\alpha_c$  to be close to  $\\bar{y}$, yet we allow large variability around it (while remaining in the same scale of the data)\n",
        "    * $\\bar{y}$ are the  mean and  the standard deviation of  $Y$ in the sample.\n",
        "    \n",
        "\n",
        "* $\\beta \\sim N(0, 2.5 \\frac{s_y}{s_x})$\n",
        "    * a priori the relation has equal probability of being increasing or decreasing; hence the prior mean is 0;\n",
        "    * the prior is broad since in simple linear regression  $\\hat{\\beta} = R \\frac{s_y}{s_x}$, where $-1 < R < 1$ is the correlation between $X$ and $Y$.\n",
        "\n",
        "\n",
        "*  $\\sigma \\sim HN (1.5 s_y)$. The prior for $\\sigma$ is large compared to the standard deviation of the data, though remaining on the same scale.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFkTJHW_B4GN"
      },
      "source": [
        "# Robust linear regression\n",
        "\n",
        "\n",
        "* Gaussianity of the errors is often a reasonable approximation but it  fails  in the presence of outliers.\n",
        "\n",
        "\n",
        "* The Student's t-distribution provides  robust inference in presence of outliers.\n",
        "\n",
        "\n",
        "* This  idea can be applied also to linear regression.\n",
        "\n",
        "* We thus compare two linear models, one with Gaussian likelihood, one with Student likelihood. The Student likelihood has longer tails, it allows a few observations to be far from the fitted line.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK9-dlJeB4GN"
      },
      "source": [
        "# Case study\n",
        "\n",
        "* The data are perfectly aligned apart from an outlier.\n",
        "\n",
        "* This is third example of the  Anscombe quartet, which contains some peculiar X-Y data sets  (https://en.wikipedia.org/wiki/Anscombe%27s_quartet)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxt-r4wSB4GN"
      },
      "outputs": [],
      "source": [
        "x  = np.array([10.,  8., 13.,  9., 11., 14.,  6.,  4., 12.,  7.,  5.])\n",
        "y  = np.array([ 7.46,  6.77, 12.74,  7.11,  7.81,  8.84,  6.08,  5.39,  8.15, 6.42,  5.73])\n",
        "\n",
        "#we  center X\n",
        "x_c = (x - x.mean())\n",
        "plt.plot(x_c, y, '*');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXJ44JlCB4GO"
      },
      "source": [
        "# Models\n",
        "\n",
        "* We will use\n",
        "  * default priors with Gaussian likelihood\n",
        "  * default priors with Student likelihood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-TlRATNB4GO"
      },
      "outputs": [],
      "source": [
        "#used later for centering and for setting the default priors\n",
        "s_x = x_c.std()\n",
        "s_y = y.std()\n",
        "y_bar = y.mean()\n",
        "\n",
        "with pm.Model() as gaussian_model:\n",
        "    #we have no prior knowledge.\n",
        "    #Thus we adopt weakly informatiove priors, which are based on the scale\n",
        "    #of the data rather than on background knowledge\n",
        "    alpha   = pm.Normal ('alpha', mu=y_bar, sigma=2 * s_y)\n",
        "    beta    = pm.Normal ('beta',  mu=0,  sigma= 2.5 * s_y / s_x)\n",
        "    sigma   = pm.HalfNormal ('sigma', sigma= 1.5 * s_y)\n",
        "\n",
        "    y_pred   = pm.Normal ('y_pred', mu= alpha + beta * x_c,  sigma=sigma, observed=y)\n",
        "\n",
        "    #the kwargs is needed to store the log_likelihood associated to each sample. Will be used later for a statistical\n",
        "    #comaprison between the two models\n",
        "    gaussian_trace   = pm.sample(idata_kwargs={\"log_likelihood\": True})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzMj-2KdB4GO"
      },
      "outputs": [],
      "source": [
        "with pm.Model() as st_model:\n",
        "    #notice the use of weakly, data-dependent prior for *standardized* xdata\n",
        "    # st stands for Student\n",
        "    alpha_st   = pm.Normal ('alpha_st', mu=y_bar, sigma=2 * s_y)\n",
        "    beta_st    = pm.Normal ('beta_st',   mu=0,  sigma=2.5 * s_y / s_x)\n",
        "    sigma_st   = pm.HalfNormal ('sigma_st', sigma= s_y)\n",
        "    y_pred_st  = pm.StudentT  ('y_pred_st', mu= alpha_st + beta_st * x_c,  sigma=sigma_st,  nu=4, observed=y)\n",
        "    st_trace   = pm.sample(idata_kwargs={\"log_likelihood\": True})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dt8SX9x9B4GO"
      },
      "outputs": [],
      "source": [
        "#The estimate of beta is much different between the two models,\n",
        "# because of the effect of the outlier.\n",
        "#The estimated of the Gaussian model have larger uncertainty\n",
        "pd.concat( [az.summary(gaussian_trace), az.summary(st_trace)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqBa_MkEB4GO"
      },
      "source": [
        "# Visualizing the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqTRnQIIB4GO"
      },
      "outputs": [],
      "source": [
        "# For simplicity, we compare the two models  using only  the mean estimate of the parameters.\n",
        "#By using all plausible values, we would see a large uncertainty for the Gaussian model.\n",
        "\n",
        "#posterior mean of the parameters, robust model\n",
        "post_st    = az.extract_dataset(st_trace)\n",
        "a_st       = post_st.alpha_st.values.mean()\n",
        "b_st       = post_st.beta_st.values.mean()\n",
        "\n",
        "#posterior mean of the parameters, gaussian model\n",
        "post_gauss  = az.extract_dataset(gaussian_trace)\n",
        "a_gauss     = post_gauss.alpha.values.mean()\n",
        "b_gauss     = post_gauss.beta.values.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2j6PxVBB4GO"
      },
      "outputs": [],
      "source": [
        "#The Student has correctly identified the line\n",
        "plt.plot(x_c, a_st + b_st * x_c,  c='k', lw=3, label='robust likelihood')\n",
        "plt.plot(x_c, a_gauss + b_gauss * x_c,  c='b', lw=3, label='gaussian likelihood')\n",
        "plt.plot(x_c, y, '*', markersize=14, label='obs')\n",
        "plt.xlabel('$x$', fontsize=16)\n",
        "plt.ylabel('$y$', rotation=0, fontsize=16)\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare the epistemic uncertainty of the two models"
      ],
      "metadata": {
        "id": "c_HeH1N2pXio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compare the epistemic uncertainty of the two models\n",
        "\n",
        "chosen_samples = range(0, len(post_gauss.alpha.values),10)\n",
        "\n",
        "#The uncertainty is shown by plotting a different line for each sample of (alpha_c, beta).\n",
        "for i in range(len(chosen_samples)):\n",
        "  plt.plot(x_c,  post_gauss.alpha.values[i] + post_gauss.beta.values[i] *\n",
        "           x_c, c='b', alpha=0.01)\n",
        "  plt.plot(x_c,  post_st.alpha_st.values[i] + post_st.beta_st.values[i] *\n",
        "           x_c, c='k', alpha=0.01)\n",
        "\n",
        "# the light lines, representing different plausible models using the gaussian likelihood\n",
        "# are very spread.\n",
        "# the dark line is the effect of plotting different robust models, which are\n",
        "# identical to each other"
      ],
      "metadata": {
        "id": "q-2E5nFYpOTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr9r_7mKB4GO"
      },
      "outputs": [],
      "source": [
        "# we can also compare the models via WAIC, which we will discuss later in the course.\n",
        "# For the moment, consider it as a way to rank models based on how well they fit the data.\n",
        "# the highest-ranked model is the supposedly best, i.e., the one whose probabilistic predictions\n",
        "# are better. See the notebook on model selection for more details.\n",
        "\n",
        "# the Student model is ranked first\n",
        "compare_dict = {'gaussian':gaussian_trace, 'student': st_trace};\n",
        "az.compare(compare_dict, ic='waic')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOzrnm34B4GO"
      },
      "source": [
        "# Discussion\n",
        "\n",
        "* The model with the Gaussian likelihood is affected by the outlier.\n",
        "\n",
        "\n",
        "* The robust model automatically discards the outlier and fits the correct line.\n",
        "\n",
        "\n",
        "* The Student's t-distribution, due to its heavier tails, is able to give less importance to points that are far away from the bulk of the data, filtering outliers in an automatic way.\n",
        "\n",
        "# Your turn\n",
        "\n",
        "* Visualize the epistemic uncertainty of the gaussian and the student model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiKuwzK4B4GP"
      },
      "source": [
        "# Solutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW9QU9JGB4GP"
      },
      "source": [
        "# A model of bike rides as a function of humidity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU6MYfP4B4GP"
      },
      "source": [
        "# Prior for the $\\beta$ and $\\alpha_c$\n",
        "\n",
        "\n",
        "> For every one percentage point increase in humidity level, ridership tends to decrease by 10 rides, though the decrease could vary between 0 and 20.\n",
        "\n",
        "\n",
        "$$ \\beta  \\sim N(-10, 5^2)  $$\n",
        "\n",
        "\n",
        ">   On an average humidity day, there are typically around 5000 riders, the actual number varying between 1000 and 9000.\n",
        "\n",
        "$$\\alpha_c \\sim N(5000, 2000^2)$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQdbPMC6B4GP"
      },
      "source": [
        "# Prior for $\\sigma$\n",
        "\n",
        "> Ridership is only weakly related to humidity. At any given humidity, ridership will tend to vary with a large standard deviation of 2000 rides.\n",
        "\n",
        "* We can  set:\n",
        "\n",
        "$$ \\sigma  \\sim \\text{HalfNormal} (3000), $$\n",
        "whose  median is around 2000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4PD2I4jB4GP"
      },
      "outputs": [],
      "source": [
        "#obtained by trial and error\n",
        "from scipy.stats import halfnorm\n",
        "pd.DataFrame(halfnorm.rvs(scale=3000, size=10000)).describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT37kyI8B4GP"
      },
      "source": [
        "# The  model : regression of bike rides on humidity\n",
        "\n",
        "\n",
        "\\begin{align*}\n",
        "\\alpha_c & \\sim  N (5000, 2000) \\\\\n",
        "\\beta & \\sim N(-10, 5) \\\\\n",
        "\\sigma & \\sim \\text{HalfNormal} (3000)\\\\\n",
        "Y & \\sim N( \\alpha_c + \\beta X_c, \\sigma)\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YImCiUikB4GP"
      },
      "source": [
        "# Likelihood exercise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhEgjJ-PB4GP"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "x = np.array([1,5])\n",
        "y = np.array([11,50])\n",
        "beta = 10\n",
        "alpha = 2\n",
        "sigma = 1\n",
        "\n",
        "#vector\n",
        "mu = alpha + beta * x\n",
        "\n",
        "#lik of each observation\n",
        "#syntax: norm.pdf(value whose lik is to be computed, mean, standard deviation)\n",
        "lik_vector = norm.pdf(y, mu, sigma)\n",
        "\n",
        "#likelihood for both data points\n",
        "lik = np.prod(lik_vector)\n",
        "\n",
        "#to have a numerically stable approach, usually the log-likelihood is considered rather than the likelihood.\n",
        "print(lik_vector)\n",
        "print(lik)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__rUwXyPCQXK"
      },
      "source": [
        "# Visualizing the epistemic uncertainty of the Student model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2sI6NRZCVIR"
      },
      "outputs": [],
      "source": [
        "post_st    = az.extract_dataset(st_trace)\n",
        "a_st_hat   = post_st.alpha_st.values.mean()\n",
        "b_st_hat   = post_st.beta_st.values.mean()\n",
        "a_st       = post_st.alpha_st.values\n",
        "b_st       = post_st.beta_st.values\n",
        "\n",
        "samples = len(post_st.alpha_st.values)\n",
        "\n",
        "#mean pars\n",
        "plt.plot(x_c,  a_st_hat + b_st_hat * x_c, c='black')\n",
        "\n",
        "#The uncertainty is shown by plotting a different line for each sample of (alpha_c, beta).\n",
        "#In this case, there is almost no epistemic uncertainty: the lines mostly overlap.\n",
        "for i in range(samples):\n",
        "    plt.plot(x_c,  a_st[i] + b_st[i] * x_c, c='gray', alpha=0.01)\n",
        "\n",
        "plt.plot(x_c, y, '*', markersize=14, label='obs')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfI2EBICC5oC"
      },
      "source": [
        "# Visualizing the epistemic uncertainty of the normal model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zC2wzVlhC7sk"
      },
      "outputs": [],
      "source": [
        "#posterior mean of the parameters, gaussian model\n",
        "post_gauss     = az.extract_dataset(gaussian_trace)\n",
        "a_normal_hat   = post_gauss.alpha.values.mean()\n",
        "b_normal_hat   = post_gauss.beta.values.mean()\n",
        "\n",
        "#trace\n",
        "a_normal       = post_gauss.alpha.values\n",
        "b_normal       = post_gauss.beta.values\n",
        "\n",
        "#here we plot a line for each sample. If you want to speed up plotting, select randomly e.g. 1/3 of them\n",
        "plt.plot(x_c,  a_normal_hat + b_normal_hat * x_c, c='black')\n",
        "\n",
        "# The uncertainty is shown by plotting a different line for each sample of (alpha_c, beta).\n",
        "# There is a lot of epistemic uncertainty in this case\n",
        "for i in range(samples):\n",
        "    plt.plot(x_c,  a_normal[i] + b_normal[i] * x_c, c='gray', alpha=0.01)\n",
        "\n",
        "plt.plot(x_c, y, '*', markersize=14, label='obs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dznNZ89LDP4S"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}