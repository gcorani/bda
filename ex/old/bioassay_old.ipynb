{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from matplotlib import cm\n",
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([[-0.86, -0.30, -0.05, 0.73]])\n",
    "n = np.array([[5., 5., 5., 5.]])\n",
    "y = np.array([[0, 1, 3, 5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Probabilistic model:\n",
    "\n",
    "\\begin{align*}\n",
    "y_i | \\gamma_i &\\sim  \\mathrm{Bin}(n_i, \\gamma_i)\\\\\n",
    "\\rm{logit}(\\gamma_i) &= \\alpha + \\beta x_i\\\\\n",
    "\\theta = \n",
    "\\begin{bmatrix}\n",
    "\\alpha \\\\\n",
    "\\beta\n",
    "\\end{bmatrix} &\\sim \\mathcal{N}(\\mu_0, \\Sigma_0), \\qquad \\text{where } \n",
    "\\mu_0 = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "10\n",
    "\\end{bmatrix}, \\;\\;\n",
    "\\Sigma_0 = \\begin{bmatrix}\n",
    "4 & 12 \\\\\n",
    "12 & 100\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{align*}\n",
    "\\rm{logit}(z) &= \\log \\left( \\frac{p}{1-p} \\right),\\\\\n",
    "\\rm{logit}^{-1}(p) &= \\rm{sigm}(p) = \\frac{1}{1 + e^{-p}}\n",
    "\\end{align*}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us first obtain a deterministic estimate of $\\alpha$ and $\\beta$ by solving the maximum likelihood problem:\n",
    "\n",
    "$$\\alpha^{\\rm ml}, \\beta^{\\rm ml} = \\arg \\min_{\\alpha, \\beta} \\mathrm{Bin}(n_i, \\rm{sigm}(\\alpha + \\beta x_i))$$ "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get an initial deterministic estimate of alpha and beta (logistic regression)\n",
    "#logreg = sklearn.linear_model.LogisticRegression()\n",
    "#logreg.fit(x.transpose(), 1/5*y.reshape(-1)) # does not work with continuous labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sigmoid function, equivalent to logit^{-1}\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Likelihood of the 4 observations (neglecting a constant multiplicative factor).\n",
    "# The overall likelihood is the product of all terms.\n",
    "lik_fun = lambda alpha, beta: sigmoid(alpha+beta*x)**y * (1 - sigmoid(alpha+beta*x))**(n-y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us visualize the likelihood function in 2D"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dalpha = 0.01\n",
    "dbeta = 0.01\n",
    "ALPHA = np.arange(-4, 10, dalpha)\n",
    "BETA = np.arange(-10, 40, dbeta)\n",
    "\n",
    "AA, BB = np.meshgrid(ALPHA, BETA, indexing='xy')\n",
    "LL = lik_fun(AA[..., np.newaxis], BB[..., np.newaxis])\n",
    "LL = np.prod(LL, axis=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "c = ax.pcolormesh(AA, BB, LL, cmap=cm.coolwarm, shading='auto')\n",
    "fig.colorbar(c, ax=ax)\n",
    "ax.set_title(f\"Likelihood\");\n",
    "ax.set_xlabel(r\"$\\alpha$\");\n",
    "ax.set_ylabel(r\"$\\beta$\");"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.contour(AA, BB, LL); #, levels=[5, 15,  95]); # levels=[5, 15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "plt.xlabel(r\"$\\alpha$\");\n",
    "plt.ylabel(r\"$\\beta$\");\n",
    "plt.grid(True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us introduce a prior on the parameters $\\alpha$ and $\\beta$:\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "\\alpha \\\\\n",
    "\\beta\n",
    "\\end{bmatrix} \\sim \\mathcal{N}(\\mu_0, \\Sigma_0), \\qquad \\text{where } \n",
    "\\mu_0 = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "10\n",
    "\\end{bmatrix}, \\;\\;\n",
    "\\Sigma_0 = \\begin{bmatrix}\n",
    "4 & 12 \\\\\n",
    "12 & 100\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Now we can go for a full Bayesian estimation of $\\alpha$ and $\\beta$:\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\alpha, \\beta) = \\frac{p(y | \\alpha, \\beta) p(\\alpha, \\beta)}{p(y)}\n",
    "\\end{equation}\n",
    "\n",
    "There is no closed-form solution in our case, but we still have two options:\n",
    "\n",
    "1. Obtain $p(y | \\alpha, \\beta) p(\\alpha, \\beta)$ and normalize it to a valid distribution\n",
    "2. Use MCMC to get samples..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mu = np.array([0, 10])\n",
    "cov_0 = np.array([[4, 12], [12, 100]]) \n",
    "prior_fun = stats.multivariate_normal(mean=mu, cov=cov_0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dalpha = 0.01\n",
    "dbeta = 0.01\n",
    "ALPHA = np.arange(-4, 10, dalpha)\n",
    "BETA = np.arange(-10, 40, dbeta)\n",
    "\n",
    "AA, BB = np.meshgrid(ALPHA, BETA, indexing='xy')\n",
    "LL = lik_fun(AA[..., np.newaxis], BB[..., np.newaxis])\n",
    "LL = np.prod(LL, axis=-1) # Likelihood\n",
    "\n",
    "AABB = np.stack((AA, BB), axis=-1)\n",
    "PP = prior_fun.pdf(AABB) # Prior\n",
    "\n",
    "POST_UNSC = LL * PP\n",
    "normalizing_factor = np.sum(POST_UNSC)*dalpha*dbeta\n",
    "POST_SC = POST_UNSC/normalizing_factor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "c = ax.pcolormesh(AA, BB, POST_SC, cmap=cm.coolwarm, shading='auto')\n",
    "fig.colorbar(c, ax=ax)\n",
    "ax.set_title(f\"Posterior distribution\");\n",
    "ax.set_xlabel(r\"$\\alpha$\");\n",
    "ax.set_ylabel(r\"$\\beta$\");"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "In this case the "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}