{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from matplotlib import cm\n",
    "import sklearn.linear_model\n",
    "\n",
    "import arviz as az\n",
    "import pymc3 as pm\n",
    "#import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([-0.86, -0.30, -0.05, 0.73])\n",
    "n = np.array([5., 5., 5., 5.])\n",
    "y = np.array([0, 1, 3, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let us consider the following probabilistic model:\n",
    "\n",
    "\\begin{align*}\n",
    "y_i | p_i &\\sim  \\mathrm{Binomial}(n_i, p_i)\\\\\n",
    "p_i &= \\rm{sigm}(\\alpha + \\beta x_i)\\\\\n",
    "\\theta = \n",
    "\\begin{bmatrix}\n",
    "\\alpha \\\\\n",
    "\\beta\n",
    "\\end{bmatrix} &\\sim \\mathcal{N}(\\mu_0, \\Sigma_0), \\qquad \n",
    "\\mu_0 = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "10\n",
    "\\end{bmatrix}, \\;\\;\n",
    "\\Sigma_0 = \\begin{bmatrix}\n",
    "4 & 12 \\\\\n",
    "12 & 100\n",
    "\\end{bmatrix},\n",
    "\\end{align*}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{align*}\n",
    "\\rm{sigm}(z) = \\frac{1}{1 + e^{-z}}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: in the book of martin, the problem is considered as a repeaed bernoulli instead. What do we prefer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# sigmoid function, equivalent to logit^{-1}\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Likelihood of the 4 observations (neglecting a constant multiplicative factor).\n",
    "# The overall likelihood is the product of all terms.\n",
    "\n",
    "def lik_fun(alpha, beta):\n",
    "    gamma = sigmoid(alpha+beta*x)\n",
    "    lik = gamma**y * (1-gamma)**(n-y)\n",
    "    return np.prod(lik, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mu = np.array([0, 10])\n",
    "cov_0 = np.array([[4, 12], [12, 100]]) \n",
    "prior_fun = stats.multivariate_normal(mean=mu, cov=cov_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let us first obtain a point estimate of $\\alpha$ and $\\beta$ by solving the maximum likelihood problem:\n",
    "\n",
    "$$\\alpha^{\\rm ml}, \\beta^{\\rm ml} = \\arg \\max_{\\alpha, \\beta} \\mathcal{L}(\\theta)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood function $\\mathcal{L}(\\theta)$ is:\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = p(y|\\theta) = \\prod_i {{n_i}\\choose{y_i}} \\mathrm{sigm}(\\alpha + \\beta x_i)^{y_i} \\cdot (1- \\mathrm{sigm}(\\alpha + \\beta x_i))^{n_i - y_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get an initial deterministic estimate of alpha and beta (logistic regression)\n",
    "#logreg = sklearn.linear_model.LogisticRegression()\n",
    "#logreg.fit(x.transpose(), 1/5*y.reshape(-1)) # does not work with continuous labels\n",
    "# If we transform to repeated bernoulli we may use standard logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "minus_lik_fun_theta = lambda theta: -lik_fun(theta[0], theta[1])\n",
    "res = minimize(minus_lik_fun_theta, x0=[0, 10])\n",
    "theta_ml = res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also compute a MAP estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minus_p_post_unscaled = lambda theta: 100*minus_lik_fun_theta(theta)*prior_fun.pdf(theta)\n",
    "# note: the 100x factor (or change in default tolerance) is needed due to poor scaling of the function\n",
    "\n",
    "res = minimize(minus_p_post_unscaled, x0=[0, 2])\n",
    "theta_map = res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let us visualize the likelihood function in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dalpha = 0.01\n",
    "dbeta = 0.01\n",
    "ALPHA = np.arange(-4, 10, dalpha)\n",
    "BETA = np.arange(-10, 40, dbeta)\n",
    "\n",
    "AA, BB = np.meshgrid(ALPHA, BETA, indexing='xy')\n",
    "LL = lik_fun(AA[..., np.newaxis], BB[..., np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "c = ax.pcolormesh(AA, BB, LL, cmap=cm.coolwarm, shading='auto')\n",
    "plt.plot(theta_ml[0], theta_ml[1], \"kx\")\n",
    "fig.colorbar(c, ax=ax)\n",
    "ax.set_title(f\"Likelihood\");\n",
    "ax.set_xlabel(r\"$\\alpha$\");\n",
    "ax.set_ylabel(r\"$\\beta$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dalpha = 0.01\n",
    "dbeta = 0.01\n",
    "ALPHA = np.arange(-4, 10, dalpha)\n",
    "BETA = np.arange(-10, 40, dbeta)\n",
    "\n",
    "AA, BB = np.meshgrid(ALPHA, BETA, indexing='xy')\n",
    "LL = lik_fun(AA[..., np.newaxis], BB[..., np.newaxis])\n",
    "\n",
    "AABB = np.stack((AA, BB), axis=-1)\n",
    "PP = prior_fun.pdf(AABB) # Prior\n",
    "\n",
    "POST_UNSC = LL * PP\n",
    "normalizing_factor = np.sum(POST_UNSC)*dalpha*dbeta\n",
    "POST_SC = POST_UNSC/normalizing_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(15, 10))\n",
    "ax[0, 0].set_xlim([-4, 10]);\n",
    "ax[0, 0].set_ylim([-10, 40]);\n",
    "ax[0, 0].contour(AA, BB, PP); #, levels=[5, 15,  95]); # levels=[5, 15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "c = ax[0, 1].pcolormesh(AA, BB, PP, cmap=cm.coolwarm, shading='auto')\n",
    "fig.colorbar(c, ax=ax[0, 1])\n",
    "ax[0, 0].set_title(f\"Prior distribution, contours\");\n",
    "ax[0, 0].set_xlabel(r\"$\\alpha$\");\n",
    "ax[0, 0].set_ylabel(r\"$\\beta$\");\n",
    "ax[0, 0].set_title(f\"Prior distribution, heatmap\");\n",
    "ax[0, 1].set_xlabel(r\"$\\alpha$\");\n",
    "ax[0, 1].set_ylabel(r\"$\\beta$\");\n",
    "\n",
    "ax[1, 0].set_xlim([-4, 10]);\n",
    "ax[1, 0].set_ylim([-10, 40]);\n",
    "ax[1, 0].contour(AA, BB, POST_SC); #, levels=[5, 15,  95]); # levels=[5, 15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "c = ax[1, 1].pcolormesh(AA, BB, POST_SC, cmap=cm.coolwarm, shading='auto')\n",
    "ax[1, 1].plot(theta_map[0], theta_map[1], \"kx\")\n",
    "fig.colorbar(c, ax=ax[1, 1])\n",
    "ax[1, 1].set_title(f\"Posterior distribution, contours\");\n",
    "ax[1, 0].set_xlabel(r\"$\\alpha$\");\n",
    "ax[1, 0].set_ylabel(r\"$\\beta$\");\n",
    "ax[1, 1].set_title(f\"Posterior distribution, heatmap\");\n",
    "ax[1, 1].set_xlabel(r\"$\\alpha$\");\n",
    "ax[1, 1].set_ylabel(r\"$\\beta$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "In this case the, the likelihood is numerically well-posed (not too many samples, not too many multiplications). The scale 0-0.05 is not too bad!. In general, it is better to work with logarithms. Let us compute the *log-likelihood* $\\ell(\\theta)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\ell(\\theta) = \\log \\mathcal{L}(\\theta) = \\sum_i {{n_i}\\choose{y_i}} + \\sum_i y_i \\log \\mathrm{sigm}(\\alpha + \\beta x_i) +  (n_i - y_i) \\log (1- \\mathrm{sigm}(\\alpha + \\beta x_i)).$$\n",
    "\n",
    "The constant term $\\sum_i {{n_i}\\choose{y_i}}$ may be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_lik_fun(alpha, beta):\n",
    "    gamma = sigmoid(alpha+beta*x)\n",
    "    #log_lik = y*np.log(gamma) + (n-y)*np.log(1-gamma)\n",
    "    # nan_to_num handles the multiplication 0*np.inf and set it to 0, as required in our case...\n",
    "    log_lik = np.nan_to_num(y*np.log(gamma), nan=0) + np.nan_to_num((n-y)*np.log(1-gamma), nan=0)\n",
    "    return np.sum(log_lik, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dalpha = 0.01\n",
    "dbeta = 0.01\n",
    "ALPHA = np.arange(-4, 10, dalpha)\n",
    "BETA = np.arange(-10, 40, dbeta)\n",
    "\n",
    "AA, BB = np.meshgrid(ALPHA, BETA, indexing='xy')\n",
    "LOG_LL = log_lik_fun(AA[..., np.newaxis], BB[..., np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "#c = ax.pcolormesh(AA, BB, np.exp(LOG_LL), cmap=cm.coolwarm, shading='auto')\n",
    "c = ax.pcolormesh(AA, BB, LOG_LL, cmap=cm.coolwarm, shading='auto')\n",
    "fig.colorbar(c, ax=ax)\n",
    "ax.set_title(f\"Log-likelihood\");\n",
    "ax.set_xlabel(r\"$\\alpha$\");\n",
    "ax.set_ylabel(r\"$\\beta$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_post_fun(alpha, beta):\n",
    "    return log_lik_fun(alpha, beta) + prior_fun.logpdf([alpha, beta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_ratio_fun(alpha_propose, beta_propose, alpha_previous, beta_previous):\n",
    "    log_p_previous = log_post_fun(alpha_previous, beta_previous)\n",
    "    log_p_propose = log_post_fun(alpha_propose, beta_propose)\n",
    "    log_p_ratio = log_p_propose - log_p_previous # log(p_prop/p_prev) = log(p_prop) - log(p_prev)\n",
    "    p_ratio = np.exp(log_p_ratio)\n",
    "    return p_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ratio_fun(alpha_propose = 1.89, alpha_previous = 0.374, beta_propose = 24.76, beta_previous = 20.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ratio_fun(alpha_propose = 0.374, alpha_previous = 1.89, beta_propose = 20.04, beta_previous = 24.76)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run a Metropolis algorithm to sample from the posterior. The p_ratio function is all we need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_0 = np.array([0, 10])\n",
    "sigma = np.diag([1.0, 5.0])\n",
    "theta_step = theta_0\n",
    "N = 100_000\n",
    "thetas = []\n",
    "\n",
    "for idx in range(N):\n",
    "    thetas.append(theta_step)\n",
    "    theta_prop = theta_step + sigma @ np.random.randn(2)\n",
    "    #theta_prop = np.clip(theta_prop, 0, 1)\n",
    "    \n",
    "    p_ratio = p_ratio_fun(theta_prop[0], theta_prop[1], theta_step[0], theta_step[1])\n",
    "    accept_prob = np.minimum(1.0, p_ratio)\n",
    "    accept = (np.random.rand() < accept_prob)\n",
    "    \n",
    "    if accept:\n",
    "        theta_step = theta_prop\n",
    "thetas = np.stack(thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n",
    "ax[0].hist2d(x=thetas[:, 0], y=thetas[:, 1], bins=100, cmap=plt.cm.BuPu)\n",
    "ax[0].set_xlim([-4, 10]);\n",
    "ax[0].set_ylim([-10, 40]);\n",
    "ax[0].contour(AA, BB, POST_SC); #, levels=[5, 15,  95]); # levels=[5, 15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "c = ax[1].pcolormesh(AA, BB, POST_SC, cmap=cm.coolwarm, shading='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(thetas[10_000:, :], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(thetas.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(AA*POST_SC)*dalpha*dbeta, np.sum(BB*POST_SC)*dalpha*dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thetas[:,0])#px.scatter(thetas[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thetas[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us do the sampling with pymc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model():\n",
    "    theta = pm.MvNormal(\"theta\", mu=mu, cov=cov_0, shape=(2,))\n",
    "    p = pm.Deterministic(\"p\", pm.math.sigmoid(theta[0] + theta[1]*x))\n",
    "    y_var = pm.Binomial(\"y_var\", n=n, p=p, observed=y)\n",
    "    trace=pm.sample(10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_joint(trace, var_names=\"theta\", kind=\"kde\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
