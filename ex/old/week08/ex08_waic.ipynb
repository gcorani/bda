{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc3 as pm\n",
    "import arviz as az\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Exercise session 6: WAIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## WAIC Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let us denote by $\\Theta$ the samples $\\theta_0, \\theta_1, \\dots, \\theta_{S-1}$ in the trace and by $y$ the measurement(s). \n",
    "\n",
    "* The log-pointwise-predictive-density $\\rm lppd$ is defined as:\n",
    "\n",
    "$$ {\\rm lppd}(y, \\Theta) = \\sum_i \\log \\frac{1}{S} \\sum_s p(y_i | \\Theta_s),$$\n",
    "\n",
    "* The effective number of parameters $p_{{\\rm waic}}$ is defined as:\n",
    "\n",
    "$$ p_{{\\rm waic}} = \\sum_i {\\rm var}_{\\theta} \\log p(y_i|\\theta)$$\n",
    "\n",
    "* The WAIC criterion (the lower, the better) is:\n",
    "\n",
    "$${\\rm WAIC}(y, \\Theta) = -2(\\rm{lppd} - p_{\\rm waic}).$$\n",
    "\n",
    "* The expected log pointwise predictive density $\\rm{elpd}$ (the higher, the better) is defined as:\n",
    "\n",
    "$$\\rm{elpd} = (\\rm{lppd} - p_{\\rm waic}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WAIC for the beta-binomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let us implement and test the WAIC on the Beta-Binomial model: the *Hello World* probabilistic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We us consider a coin that falls heads with probability $\\theta=0.25$ (assumed unknown). \n",
    "\n",
    "Our prior knowledge of $\\theta$ is encoded in a Beta distributions with parameters $a=20$ and $b=10$:\n",
    "\n",
    "$$ p_{\\rm prior}(\\theta) = \\frac{1}{B(a,b)} \\theta^{a-1} (1-\\theta)^{b-1}, \\qquad \\theta \\in [0, 1],$$\n",
    "\n",
    "where $B(a,b)$ is a proper normalization constant such that $\\int_{0}^1 p_{\\rm prior}(\\theta) \\; d \\theta = 1.$ \n",
    "\n",
    "We toss the coin $n$ times and measure a number $y$ of heads. How does our belief of $\\theta$ change with the measurement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "a = 20 # prior: parameter a\n",
    "b = 10 # prior: parameter b\n",
    "n = 50 # likelihood: number of tosses\n",
    "y = 15 # likelihood: number of HEADs observed. (Fixed in this example. It could be sampled form a binomial instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as beta_binomial:\n",
    "    theta = pm.Beta(\"theta\", alpha=a, beta=b)\n",
    "    y_obs = pm.Binomial(\"y_obs\", n=n, p=theta, observed=y)\n",
    "    trace_bb = pm.sample(1000, random_seed=123, return_inferencedata=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with beta_binomial:\n",
    "    display(az.summary(trace_bb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "az.plot_trace(trace_bb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "az.plot_posterior(trace_bb, color=\"b\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## WAIC computation details\n",
    "\n",
    "Let us consider a scalar observation $y$ (as in the beta-binomial example). In the definitions above, the outer summations on $i$ disappear. \n",
    "\n",
    "Let us denote by ``lik_vec`` and ``loglik_vec`` vectors containing, respectively, the likelihood and log-likelihood of $y$, for the samples of $\\theta_s \\in \\Theta$:\n",
    "\n",
    "$$\\text{loglik_vec} = [\\log p(y | \\theta_0)\\;\\; \\log p(y | \\theta_1),\\;\\; \\dots,\\;\\; \\log p(y | \\theta_{S-1})]^\\top$$\n",
    "$$\\text{lik_vec} = [p(y | \\theta_0)\\;\\; p(y | \\theta_1),\\;\\; \\dots,\\;\\; p(y | \\theta_{S-1})]^\\top$$\n",
    "\n",
    "* $p_{{\\rm waic}}$ can be computed as the sample variance of ``loglik_vec``. In Python:\n",
    "\n",
    "``p_waic = np.var(loglik_vec)``\n",
    "\n",
    "* ${\\rm lppd}$ could be computed (in principle) as the logarithm of the average of ``lik_vec``. In Python:\n",
    "\n",
    "``lppd = np.log(np.mean(lik_vec))``\n",
    "\n",
    "* For better numerical precision, ${\\rm lppd}$ computation is typically worked out in the log-domain, exploiting the logsumexp trick:\n",
    "\n",
    "``lppd = scipy.special.logsumexp(log_lik_vec) - np.log(S)``\n",
    "\n",
    "This exploits the identity:\n",
    "\n",
    "$$\\log {{\\sum_s p(y_i | \\theta_s)}} =  \\log\\left(\\sum_s e^{\\log p(y_i | \\theta_s)}\\right),$$\n",
    "where at the right-hand-side we recognize the ``logsumexp`` operation applied to the vector ``loglik_vec``.\n",
    "\n",
    "Thus, in practice, all we need to compute the WAIC is the log-likelihood of the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## WAIC manual computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Write the likelihood and the log-likelihood function of the observations in closed form. Note: for these calculations, we cannot disregard multiplicative/additive terms not depending on $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The likelihood is:\n",
    "$$p(y | \\theta) = {{n}\\choose{y}} \\theta^{y} \\cdot (1-\\theta)^{n-y}.$$\n",
    "\n",
    "The log-likelihood is:\n",
    "$$\\log p(y | \\theta) = \\log {{n}\\choose{y}} + y \\log(\\theta) + (n-y)\\log(1-\\theta).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Obtain the likelihood and the log-likelihood as python functions (including multiplicative/additive factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def lik(theta):\n",
    "    return scipy.special.binom(n, y) * (theta ** y) * (1 - theta)**(n-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def log_lik(theta):\n",
    "    return np.log(scipy.special.binom(n, y)) + y*np.log(theta) + (n-y)*np.log(1 - theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Extract the samples $\\Theta$ from the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "theta_trace = np.array(trace_bb.posterior.theta).ravel()\n",
    "S = theta_trace.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Compute $\\rm{lppd}$ and $p_{\\rm waic}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "log_lik_vec = log_lik(theta_trace)\n",
    "# log_lik_pm = np.array(trace_bb.log_likelihood.y_obs).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# lppd = np.log(np.mean(lik(theta_trace)))\n",
    "lppd = scipy.special.logsumexp(log_lik_vec) - np.log(S) # sum is equivalent to logsumexp in log domain..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "p_waic = np.var(log_lik_vec) # correct\n",
    "p_waic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Compute $\\rm{waic}$ and ${\\rm elpd}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "waic = -2*(lppd - p_waic); waic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "elpd = (lppd - p_waic); elpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## WAIC Computation with arviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "arviz computes these quantity automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with beta_binomial:\n",
    "    display(az.waic(trace_bb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WAIC for the multiparameter model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.loadtxt(os.path.join(\"..\", \"data\", \"chemical_shifts.txt\"))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_kde(y, rug=\"True\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model_gaussian:\n",
    "    mu = pm.Uniform('mu', lower=40, upper=70)\n",
    "    sigma = pm.HalfNormal('sigma', sd=10)\n",
    "    y_obs = pm.Normal('y_obs', mu=mu, sd=sigma, observed=y)\n",
    "    trace_gaussian = pm.sample(1000, return_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_gaussian);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_gaussian:\n",
    "    display(az.summary(trace_gaussian))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_gaussian:\n",
    "    # select 500 samples <mu_s ,sigma_s> from the trace\n",
    "    # for each samples, compute a draw N(mu_s ,sigma_s) \n",
    "    # The ppc variable is a dictionary, with the keys being the name of the observed variable in our model and the values an array of shape (samples, size). \n",
    "    # The dictionary allows dealing with models with more than one observed variable. \n",
    "    ppc = pm.sample_posterior_predictive(trace_gaussian, samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ppc(az.from_pymc3(posterior_predictive=ppc, model=model_gaussian));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior predictive plot is not very convincing (outliers are not modeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with beta_binomial:\n",
    "    display(az.waic(trace_gaussian))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compute the WAIC manually:\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = p(y | \\theta) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n",
    "\n",
    "$$\\mathcal{\\ell}(\\theta) = \\log p(y|\\theta) = -\\log(\\sigma) -\\frac{1}{2} \\log ({2 \\pi}) - \\frac{(y-\\mu)^2}{2\\sigma^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_lik_fun(mu, sigma, y):\n",
    "    return -np.log(sigma) -1/2*np.log(2*np.pi) - (y - mu)**2/(2*sigma**2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_mc = np.array(trace_gaussian.posterior[\"mu\"]).ravel()[:, np.newaxis] # (n_samples, 1)\n",
    "sigma_mc = np.array(trace_gaussian.posterior[\"sigma\"]).ravel()[:, np.newaxis] # (n_samples, 1)\n",
    "y_ = y[np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_mc.shape, sigma_mc.shape, y_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lik_recalc = log_lik_fun(mu_mc, sigma_mc, y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lik_vec = np.array(trace_gaussian.log_likelihood.y_obs)\n",
    "log_lik_vec = log_lik_vec.reshape(-1, log_lik_vec.shape[-1]) # (mc sample, data sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lik_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_waic = np.var(log_lik_vec, axis=0, ddof=1) # correct\n",
    "p_waic = np.sum(p_waic)\n",
    "p_waic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lppd = scipy.special.logsumexp(log_lik_vec, axis=0) - np.log(S) # sum is equivalent to logsumexp in log domain...\n",
    "lppd = np.sum(lppd)\n",
    "lppd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lppd - p_waic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model_student:\n",
    "    μ = pm.Uniform('μ', 40, 75)\n",
    "    σ = pm.HalfNormal('σ', sd=10)\n",
    "    ν = pm.Exponential('ν', 1/30)\n",
    "    y = pm.StudentT('y', mu=μ, sd=σ, nu=ν, observed=y)\n",
    "    trace_student = pm.sample(1000, return_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_student);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_student:\n",
    "    display(az.summary(trace_student))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_student:\n",
    "    # select 500 samples <mu_s ,sigma_s> from the trace\n",
    "    # for each samples, compute a draw N(mu_s ,sigma_s) \n",
    "    # The ppc variable is a dictionary, with the keys being the name of the observed variable in our model and the values an array of shape (samples, size). \n",
    "    # The dictionary allows dealing with models with more than one observed variable. \n",
    "    ppc = pm.sample_posterior_predictive(trace_student, samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ppc(az.from_pymc3(posterior_predictive=ppc, model=model_student));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with factory_pooled, factory_separate, factory_hierarchical:\n",
    "comp_df = az.compare({\"model_gaussian\": trace_gaussian,\n",
    "                      \"model_student\": trace_student}, ic=\"waic\", method=\"BB-pseudo-BMA\")\n",
    "comp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_compare(comp_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
