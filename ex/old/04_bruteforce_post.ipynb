{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "theta = 0.25 # true unknown parameter, P(HEAD)\n",
    "a = 20 # parameter a of the prior\n",
    "b = 10 # parameter b of the prior\n",
    "n = 2000 # number of coin tosses. Try 5000!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dtheta = 1e-3 # discretization step for theta\n",
    "theta_vec = np.arange(0, 1, dtheta) # discretized theta range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Define the prior function:\n",
    "$$ p_{\\rm prior}(\\theta) = \\frac{1}{B(a,b)} \\theta^{a-1} (1-\\theta)^{b-1},$$\n",
    "\n",
    "Where $B(a,b)$ is a proper normalization constant corresponding to the Beta function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prior_fun = lambda theta: stats.beta.pdf(theta, a, b) # beta function available in scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(theta_vec, prior_fun(theta_vec))\n",
    "plt.xlabel(r\"$\\theta$\");\n",
    "plt.title(r\"Prior distribution of $\\theta$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y = np.random.binomial(n, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Approach 1: compute the posterior in closed-form (if you know an exact formula...)\n",
    "\n",
    "exact_post_fun = lambda theta: stats.beta.pdf(theta_vec, a+y, b+n-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(theta_vec, prior_fun(theta_vec), label=\"prior\")\n",
    "plt.plot(theta_vec, exact_post_fun(theta_vec), label=\"posterior\")\n",
    "plt.legend()\n",
    "plt.xlabel(r\"$\\theta$\");\n",
    "plt.title(r\"Distribution of $\\theta$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Approach 2: normalize the product likelihood * prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "By definition, the posterior distribution $p_{\\rm post}(\\theta)$ is:\n",
    "$$p_{\\rm post}(\\theta) = p(\\theta | y) = \\frac{\\overbrace{p(y|\\theta)}^{=\\mathcal{L}(\\theta)} \\cdot p_{\\rm prior}(\\theta)}{p(y)}$$\n",
    "\n",
    "The likelihood function $\\mathcal{L}(\\theta)$ is:\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = p(y|\\theta) = {{n}\\choose{y}} \\theta^{y} \\cdot (1-\\theta)^{n-y},$$\n",
    "seen as a function of $\\theta$. \n",
    "\n",
    "Thus, we have:\n",
    "$$p_{\\rm post}(\\theta) = Z \\cdot \\mathcal{L}(\\theta) p_{\\rm prior}(\\theta),$$\n",
    "\n",
    "where the normalization constant $Z$ must be chosen to satisfy:\n",
    "\n",
    "$$\\int_\\theta p_{\\rm post}(\\theta) \\; d\\theta = 1,$$\n",
    "thus\n",
    "\n",
    "$$Z = \\int_\\theta \\mathcal{L}(\\theta) p_{\\rm prior}(\\theta) \\; d\\theta = 1.$$\n",
    "\n",
    "Any numerical integration method can be used to approximate the integral above. Easiest choice: Riemann sum on a uniform grid, with step size $\\Delta theta$\n",
    "$$ Z \\approx \\Delta \\theta \\sum_{i} \\mathcal{L}(\\theta_i) p_{\\rm prior}(\\theta_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lik_y = (theta_vec ** y) * (1 - theta_vec)**(n-y) # likelihood (up to a multiplicative factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note: the binomial coefficient ${{n}\\choose{y}}$ does not depend on $\\theta$ and thus it is ignored in our definition of $\\mathcal{L}(\\theta)$. It will be dealt with by the multiplicative constant $Z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(theta_vec, lik_y)\n",
    "plt.xlabel(r\"$\\theta$\");\n",
    "plt.title(\"Likelihood\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "p_theta_post = lik_y * prior_fun(theta_vec)\n",
    "normalization_constant = (np.sum(p_theta_post) * dtheta)\n",
    "p_theta_post = p_theta_post/normalization_constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "NOTE only in one (and perhaps 2) dimensions it is reasonable to approximate the integral numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(theta_vec, p_theta_post, label=\"approx\")\n",
    "plt.plot(theta_vec, exact_post_fun(theta_vec), label=\"exact\")\n",
    "plt.plot(theta_vec, p_theta_post - exact_post_fun(theta_vec), label=\"difference\")\n",
    "plt.xlabel(r\"$\\theta$\");\n",
    "plt.legend()\n",
    "plt.title(\"Posterior distribution\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lik_y_scaled = lik_y/((np.sum(lik_y) * dtheta))\n",
    "\n",
    "plt.plot(theta_vec, prior_fun(theta_vec), label=\"prior\")\n",
    "plt.plot(theta_vec, lik_y_scaled, label=\"likelihood (scaled)\")\n",
    "plt.plot(theta_vec, exact_post_fun(theta_vec), label=\"posterior\")\n",
    "\n",
    "\n",
    "plt.xlabel(r\"$\\theta$\");\n",
    "plt.legend()\n",
    "plt.title(\"Posterior distribution\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note: the scaled likelihood is also equivalent to the posterior for a flat prior over $[0, 1]$. A flat prior is also equivalent to a $\\beta(a=1,b=1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "What happen for large $n$ (e.g. $n = 5000$)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We may instead compute the logarithm of the distributions. Starting from the definition:\n",
    "\n",
    "$$p_{\\rm post}(\\theta) = p(\\theta | y) = \\frac{{p(y|\\theta)} \\cdot p_{\\rm prior}(\\theta)}{p(y)},$$\n",
    "\n",
    "we obtain:\n",
    "$$ \\log p_{\\rm post}(\\theta) = \\overbrace{\\log p(y|\\theta)}^{=\\ell(\\theta)} + \\log p_{\\rm prior}(\\theta) + \\log p(y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let us compute the log-likelihood $\\ell(\\theta)$:\n",
    "\n",
    "$$\\ell(\\theta) = \\log \\mathcal{L}(\\theta) = \\log p(y|\\theta) = \\log{{N}\\choose{k}} + y \\log \\theta + (n-y) \\log (1-\\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "log_lik_y = y*np.log(theta_vec)  + (n-y)*np.log(1 - theta_vec) # log-likelihood, up to a constant additive factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(theta_vec, log_lik_y)\n",
    "plt.xlabel(r\"$\\theta$\");\n",
    "plt.title(\"Log-likelihood (up to a constant)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "log_prior = np.log(prior_fun(theta_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "log_posterior = log_prior + log_lik_y # again, up to an additive constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(log_prior, label=r\"$\\log p_{prior}(\\theta)$ (up to a constant)\")\n",
    "plt.plot(log_lik_y, label=r\"log-likelihood $\\ell(\\theta)$ (up to a constant)\")\n",
    "plt.plot(log_posterior, label=r\"$\\log p_{post}(\\theta)$ (up to a constant)\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the variable **log_posterior** we have a numerical value for $\\log p_{\\rm post}(\\theta)$, up to an *additive* constant. Thus, in principle, **np.exp(log_posterior)** we would have the posterior, up to a *multiplicative* constant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.exp(log_posterior));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This again may fail! Problems: numbers are too small! ($\\approx e^{-2000}$) which is below epsilon machine. Let us choose a better additive constant for the log-likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "log_post_scaled = log_posterior - np.max(log_posterior)\n",
    "plt.plot(log_post_scaled);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.title(\"Unnormalized posterior distribution\")\n",
    "plt.plot(theta_vec, np.exp(log_post_scaled));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now it works! The maximum is $e^{0} = 1$. We just need to normalize it to be a probability distribution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "post_unnorm = np.exp(log_post_scaled)\n",
    "Z = np.sum(post_unnorm)*dtheta\n",
    "post_norm = post_unnorm/Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.title(\"Posterior distribution\")\n",
    "plt.plot(theta_vec, post_norm, label=\"Normalized\")\n",
    "plt.plot(theta_vec, exact_post_fun(theta_vec), label=\"Exact\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
