{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 20 # parameter a of the beta prior\n",
    "b = 10 # parameter b of the beta prior\n",
    "theta = 0.25 # true unknown parameter, P(HEAD)\n",
    "n = 100 # number of coin tosses. Try 5000!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(42)\n",
    "#y = np.random.binomial(n, theta) # random number of heads from the distribution from a binomial distribution\n",
    "y = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise session 2: Beta-Binomial, Reporting, Gridding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider a coin that lands heads with probability $\\theta=0.25$ (unknown to us). \n",
    "\n",
    "Our prior knowledge of $\\theta$ is encoded in a Beta distribution (where \"success\" corresponds to the coin landing heads), with parameters $a=20$ and $b=10$:\n",
    "\n",
    "$$ \\theta \\sim \\rm{Beta}(a,b).$$\n",
    "\n",
    "\n",
    "We toss the coin $n=100$ times and observe a number $y=24$ of heads. How does our belief of $\\theta$ change with the measurement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you need the Beta probability distribution $\\rm Beta$ in Python, use `scipy.stats.beta`. If you need the beta function $B$ in Python, it is `scipy.special.beta`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0: Probabilistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Derive and comment the full probabilistic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full probabilistic model is:\n",
    "\n",
    "\\begin{align}\n",
    "\\theta \\sim \\rm{Beta}(a,b)\\\\\n",
    "y|\\theta \\sim \\rm{Bin}(n, \\theta).\n",
    "\\end{align}\n",
    "\n",
    "It is the classic Beta-binomial model. The random variable $y$ represents the number of success (heads) events over $n$ trials. The success probability $\\theta$ is constant in all flips, which are (conditionally) independent events. For a given value of $\\theta$, the probability of observing $y$ heads in $n$ trials is a binomial. The prior of $\\theta$ is a Beta with *fixed* coefficients $a$ and $b$.\n",
    "\n",
    "The Beta prior distribution is *conjugate* with the binomial likelihood, and thus the posterior of $\\theta$ has a close-form Beta structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Prior distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the probability density function (pdf) of the prior $f_{\\rm prior}(\\theta)$. Explain our prior belief on $\\theta$ in words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior distribution is:\n",
    "\n",
    "$$ f_{\\rm prior}(\\theta) = \\frac{1}{B(a,b)} \\theta^{a-1} (1-\\theta)^{b-1}, \\qquad \\theta \\in (0, 1),$$\n",
    "\n",
    "where $B(a,b)$ is the [beta function](https://en.wikipedia.org/wiki/Beta_function) returning the right normalization constant such that $\\int_{0}^1 f_{\\rm prior}(\\theta) \\; d \\theta = 1.$ <br/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_var = stats.beta(a, b) # prior random variable object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_dist(theta, a=20, b=10):\n",
    "    return 1/scipy.special.beta(a,b) * theta**(a-1) * (1-theta)**(b-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(np.allclose(prior_var.pdf(0.5) , prior_dist(0.5))) # our implementation is close to the true value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dtheta = 1e-3 # discretization step for theta\n",
    "theta_vec = np.arange(0, 1, dtheta) # discretized theta range\n",
    "plt.plot(theta_vec, prior_var.pdf(theta_vec))\n",
    "plt.xlabel(r\"$\\theta$\");\n",
    "plt.title(r\"Prior distribution of $\\theta$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute the mean, the mode, and the standard deviation of the prior distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See formulas in the lecture notes \"The beta-binomial model\"\n",
    "prior_mean = a/(a+b)\n",
    "prior_mode = (a-1)/(a + b - 2)\n",
    "prior_sd = np.sqrt(a*b/((a+b)**2*(a+b+1)))\n",
    "prior_mean, prior_mode, prior_sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute the 95\\% Highest Density Interval (HDI) of the prior distribution. \n",
    "\n",
    "    Hints: \n",
    "    * The distribution is unimodal. See slide 10 in lecture notes \"Reporting the results of a Bayesian analysis\". \n",
    "    * Quantiles of a distribution are given by the `.ppf` method of a `scipy.stats` distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "H = prior_var.ppf(1-alpha/2) # ppf = percent point function\n",
    "L = prior_var.ppf(alpha/2)\n",
    "HDI = [L, H]\n",
    "HDI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Verify through numerical integration that the interval above contains indeed approximately 95% of the distribution area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the prior distribution together with its mean, mode, and 95% HDI and comment the result. Is the prior representative of the true value of $\\theta$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## 2: Posterior distribution - exact derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Obtain the posterior distribution in closed form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Beta-binomial model:\n",
    "\\begin{align}\n",
    "\\theta &\\sim \\mathrm{Beta}(a,b)\\\\\n",
    "y|\\theta &\\sim \\mathrm{Bin}(n, \\theta)\n",
    "\\end{align}\n",
    "is **conjugate**. We have\n",
    "$$\\theta|y \\sim \\rm{Beta}(a+y, b + n - y)$$\n",
    "Which means:\n",
    "$$f_{\\rm post}(\\theta) = f(\\theta | \\beta) = \\frac{1}{B(a+y,b+n-y)} \\theta^{a+y-1} (1-\\theta)^{b+n-y-1}, \\qquad \\theta \\in (0, 1).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_post = a + y\n",
    "b_post = b + n - y\n",
    "post_var = stats.beta(a=a_post, b=b_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the posterior distribution together with the prior and the true value of $\\theta$. Comment on the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Obtain a mathematical expression of the likelihood function $\\mathcal{L}(\\theta) = P(y=24|\\theta)$.\n",
    "\n",
    "Hint: See Binomial distribution formula, pag. 30 of \"The beta-binomial\" lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute the likelihood function on a grid in [0,1]. You may disregard the multiplicative factor that does not depend on $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lik(theta):\n",
    "    return (theta ** y) * (1 - theta)**(n-y)\n",
    "\n",
    "lik_vec = lik(theta_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the likelihood function. What do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Point estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Obtain the maximum likelihood (ML) point estimate $\\theta^{\\rm ml}$.\n",
    "\n",
    "Hint: $\\theta^{\\rm ml}$ is the value of $\\theta$ corresponding to the maximum of the likelihood function $\\mathcal{L}(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Obtain the maximum a posteriori (MAP) point estimate $\\theta^{\\rm MAP}$.\n",
    "\n",
    "Hint: $\\theta^{\\rm MAP}$ is the value of $\\theta$ corresponding to the maximum of the posterior density $f_{\\rm post }(\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Obtain the posterior mean point estimate $E[\\theta|y]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Posterior distribution - gridding, aka brute-force approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "* Obtain a numerical approximation of the posterior $f_{\\rm post}(\\theta)$, by normalizing the product $\\mathcal{L}(\\theta) \\cdot f_{\\rm prior}(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "By definition, the posterior distribution $f_{\\rm post}(\\theta)$ is:\n",
    "$$f_{\\rm post}(\\theta) = f(\\theta | y) = \\frac{\\overbrace{P(y|\\theta)}^{=\\mathcal{L}(\\theta)} \\cdot f_{\\rm prior}(\\theta)}{P(y)},$$\n",
    "where $P(y) = \\int{P(y|\\theta)\\; d\\theta}$.\n",
    "\n",
    "Thus, $f_{\\rm post}(\\theta)$ corresponds to the product $\\mathcal{L}(\\theta) \\cdot f_{\\rm prior}(\\theta)$, up to a multiplicative constant to be determined.\n",
    "\n",
    "We have:\n",
    "$$f_{\\rm post}(\\theta) = \\frac{1}{Z} \\cdot \\mathcal{L}(\\theta) f_{\\rm prior}(\\theta),$$\n",
    "\n",
    "where the normalization constant $Z$ must be chosen to satisfy:\n",
    "\n",
    "$$\\int_\\theta f_{\\rm post}(\\theta) \\; d\\theta = 1,$$\n",
    "thus\n",
    "\n",
    "$$Z = \\int_\\theta \\mathcal{L}(\\theta) f_{\\rm prior}(\\theta) \\; d\\theta = 1.$$\n",
    "\n",
    "Any numerical integration method can be used to approximate the integral above. Easiest choice: Riemann sum on a uniform grid, with step size $\\Delta \\theta$\n",
    "$$ Z \\approx \\Delta \\theta \\sum_{i} \\mathcal{L}(\\theta_i) f_{\\rm prior}(\\theta_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "p_theta_post = lik_vec * prior_var.pdf(theta_vec)\n",
    "Z = (np.sum(p_theta_post) * dtheta)\n",
    "p_theta_post = p_theta_post/Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**NOTE**: The Beta-Binomial case is a very special and lucky (conjugate) combination where we can compute $f_{\\rm post}(\\theta)$ analytically. The gridding approach above is also applicable to non-conjugate prior/likelihood combination. However, it is only feasible in 1-3 dimensions  (curse of dimensionality!). For non-conjugate prior/likelihood combinations in higher dimensions, more advanced methods are needed. See following lectures!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Verify that the numerical approximation of the posterior is accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Obtain a scaled version of the likelihood function $\\mathcal{L}(\\theta)$, such that $\\int_{0}^{1} \\mathcal{L}(\\theta)\\; d\\theta = 1$. What does this function represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the posterior, the prior, and the scaled likelihood on the same axes. Comment the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Try to execute the code above with a larger value of n (ex. n = 5000). What happens? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter the logarithm: log-likelihood and log-posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To avoid numerical difficulties, we may instead compute the *logarithm* of the likelihood and the posterior density. Starting from the definition:\n",
    "\n",
    "$$f_{\\rm post}(\\theta) = f(\\theta | y) = \\frac{{P(y|\\theta)} \\cdot f_{\\rm prior}(\\theta)}{P(y)},$$\n",
    "\n",
    "we obtain the following formula for the log-posterior $g(\\theta)$:\n",
    "$$ g(\\theta) = \\log f_{\\rm post}(\\theta) = \\overbrace{\\log P(y|\\theta)}^{=\\ell(\\theta)} + \\log f_{\\rm prior}(\\theta) + \\log P(y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let us compute the log-likelihood $\\ell(\\theta)$:\n",
    "\n",
    "$$\\ell(\\theta) = \\log \\mathcal{L}(\\theta) = \\log P(y|\\theta) = \\log{{N}\\choose{k}} + y \\log \\theta + (n-y) \\log (1-\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluate and plot the log-likelihood $\\ell(\\theta)$ up to an additive constant over a grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def log_lik(theta):\n",
    "    theta = np.where(theta==0, 1e-12, theta) # avoids a numerical issue for theta=0\n",
    "    return y*np.log(theta)  + (n-y)*np.log(1 - theta)\n",
    "\n",
    "log_lik_vec = log_lik(theta_vec) # log-likelihood, up to a constant additive factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the log-likelihood and comment the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(theta_vec, log_lik_vec)\n",
    "plt.xlabel(r\"$\\theta$\");\n",
    "plt.title(\"Log-likelihood (up to a constant)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluate the log-posterior $g(\\theta)$ (up to an additive constant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "log_prior_vec = prior_var.logpdf(theta_vec) # or derive it analytically..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "log_post_vec = log_prior_vec + log_lik_vec # again, up to an additive constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(log_prior_vec, label=r\"$\\log p_{prior}(\\theta)$ (up to a constant)\")\n",
    "plt.plot(log_lik_vec, label=r\"log-likelihood $\\ell(\\theta)$ (up to a constant)\")\n",
    "plt.plot(log_post_vec, label=r\"$\\log p_{post}(\\theta)$ (up to a constant)\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Obtain the normalized posterior $f(\\theta|y)$ starting from the unnormalized log-posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative implementation using the logsumexp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous implementation, we have added a constant to the log-posterior for numerical reasons, then normalized the posterior to sum up to 1. It is possible to obtain the same result with a single operation exploiting the logsumexp function.\n",
    "\n",
    "The logsumexp function is defined for a vector $\\mathrm{x} = [x_0\\; x_1\\; \\dots \\; x_{n-1}]^\\top$ as:\n",
    "\n",
    "$$\\mathrm{logsumexp}(\\mathrm{x}) = \\log \\sum_i e^{x_i}.$$\n",
    "\n",
    "Consider the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSE = scipy.special.logsumexp(log_post_vec)\n",
    "post_norm_ = 1/dtheta * np.exp(log_post_vec - LSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(theta_vec, post_norm_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have found a magic constant that provides a good scaling and the right normalization constant in one shot! How does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us denote as $\\tilde g(\\theta)$ the log-posterior up to an additive constant $K$:\n",
    "\n",
    "$$ \\tilde g(\\theta) = \\log f_{\\rm post}(\\theta) + K \\Rightarrow f_{\\rm post}(\\theta) = \\frac{1}{Z} e^{\\tilde g(\\theta)}.$$\n",
    "\n",
    "The constant $Z$ is such that $Z e^{\\tilde g(\\theta)}$ integrates to 1. Then,\n",
    "\n",
    "$$ f_{\\rm post}(\\theta) = \\frac{e^{\\tilde g(\\theta)}}{ \\int{e^{\\tilde g(\\theta_i)}}\\; d\\theta}\n",
    "\\approx  \\frac{e^{\\tilde g(\\theta)}}{\\Delta \\theta \\sum_i{e^{\\tilde g(\\theta_i)}}} = \n",
    "\\frac{e^{\\tilde g(\\theta)}}{\\Delta \\theta e^{\\log \\sum_i{e^{\\tilde g(\\theta_i)}}}} = \\frac{1}{\\Delta \\theta} \\frac{e^{\\tilde g(\\theta)}}{e^{\\rm logsumexp(G)}} = \\frac{1}{\\Delta \\theta}  e^{\\tilde g(\\theta) - \\mathrm{logsumexp}(G)},$$\n",
    "where $\\mathrm{G} = [\\tilde g(\\theta_0), \\tilde g(\\theta_1), \\dots, \\tilde g(\\theta_{n-1})]$ is the vector of the samples of $\\tilde g$ at the grid points used for numerical integration of $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The logsumexp magic\n",
    "\n",
    "* Internally, the logsumexp function avoid exponentiation with very small/very large numbers exploiting the identity:\n",
    "\n",
    "$$ {\\rm logsumexp}(x) = \\log \\sum_i e^{x_i} = c + \\log \\sum_i e^{(x_i - c)}.$$\n",
    "\n",
    "* A robust implementation of logsumexp sets $c = \\max(x_0, \\dots, x_{n-1})$. This ensures that the largest exponentiated term is always $e^0=1$!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x):\n",
    "    c = x.max()\n",
    "    return c + np.log(np.sum(np.exp(x - c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Whenever we have a summation in the original probability/likelihood domain, we can alternatively perform a logsumexp operation in the log-probability/log-likelihood domain. \n",
    "\n",
    "* Imagine we have a vector $x$ of log-probabilities and want to compute the sum of the probabilities. We have:\n",
    "$$\\sum_{i} e^{x_i}\n",
    "% = \\log e^{\\overbrace{\\log \\sum_{i} e^{x_i}}^{= {\\rm logsumexp}(x)}} \n",
    "= e^{{\\rm logsumexp}(x)}.\n",
    "%\\log \\left(\\sum_{i} e^{x_i}\\right) \n",
    "% = \\log e^{\\overbrace{\\log \\sum_{i} e^{x_i}}^{= {\\rm logsumexp}(x)}} \n",
    "%= {{\\rm logsumexp}(x)}.\n",
    "$$\n",
    "\n",
    "    * If the probabilities $e^{x_i}$ are small (i.e., $x_i$ are negative values with a large modulus), direct evaluation of the left-hand side may fail and the logsumexp implementation at the right-hand side is to be preferred. \n",
    "    \n",
    "    * If we are interested in a result in the log-domain, the exponentiation at the right-hand side (which could also be problematic) is avoided.\n",
    "\n",
    "* In general, it is suggested to perform as many operations as possible in the log-domain and go back to the original domain only when it is strictly necessary (and numerically safe!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
