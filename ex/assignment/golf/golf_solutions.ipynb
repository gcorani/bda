{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: Golf putting probability\n",
    "\n",
    "In golf, the term [putt](https://en.wikipedia.org/wiki/Golf_swing#Putt) refers to a precision stroke that is executed by the player from a relatively short distance, aiming at sending the ball directly in the hole. \n",
    "\n",
    "Consider the following dataset of putt strokes executed by professional golf players, where $x$ is the distance from the hole discretized in 19 *levels*; $n$ is the total number of shots recorded at each distance level; and $y$ the number of successful putts, among the $n$ recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"golf.csv\")\n",
    "data[\"x\"] = data[\"x\"] # normalized units. True distance (in feet) is 20*x\n",
    "data[\"y\"] = data[\"y\"]\n",
    "data[\"n\"] = data[\"n\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the distance $x$ is provided in 0-1 normalized units for numerical convenience. The true distance (in feet) may be obtained by multiplying $x$ by 20. Please stick with the normalized units in the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "source": [
    "## Data exploration\n",
    "\n",
    "We may compute and visualize the fraction of successful puts at all distance levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[\"x\"].values\n",
    "y = data[\"y\"].values\n",
    "n = data[\"n\"].values\n",
    "frac = y/n # fraction of successful puts at a given distance level\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, frac, \"*\")\n",
    "plt.xlabel(\"Distance from hole (normalized units)\")\n",
    "plt.ylabel(\"Fraction of successful putts (-)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the probability of making the shot decreases with the distance from the hole. We aim at building a probabilistic model that relates the probability of successful putts with the distance from the hole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling assumptions\n",
    "\n",
    "For the probabilistic model, we make the following assumptions:\n",
    "\n",
    "1. The outcome of the $n_i$ strokes at each distance levels $x_i$ are *independent*. Each stroke in the group has probability $p_i$ of success.\n",
    "\n",
    "2. The probability of success $p_i$ depends on the distance $x_i$ as follows:\n",
    "    $$p_i =  \\rm{sigm}(\\alpha x + \\beta)$$ \n",
    "    where \n",
    "    $$\n",
    "    \\rm{sigm}(z) = \\frac{1}{1 + e^{-z}}.\n",
    "    $$\n",
    "3. The prior probability of the parameters \n",
    "$\\theta \\triangleq \\begin{bmatrix}\n",
    "\\alpha \\\\\n",
    "\\beta\n",
    "\\end{bmatrix}$\n",
    "is Gaussian: \n",
    "\\begin{align}\n",
    "\\alpha &\\sim N(\\mu_\\alpha, \\sigma^2_\\alpha), \\qquad \\mu_\\alpha = 0, \\sigma_\\alpha=1\\\\\n",
    "\\beta &\\sim N(\\mu_\\beta, \\sigma^2_\\beta), \\qquad \\mu_\\beta=0, \\sigma_\\beta=1.\n",
    "\\end{align}\n",
    "4. The outcomes of the 19 distance levels are independent of each other, given $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1: Probabilistic model\n",
    "\n",
    "* Derive and comment the full probabilistic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting together probabilistic assumptions 1-3, we obtain:\n",
    "\n",
    "\\begin{align*}\n",
    "y_i | p_i &\\sim  \\mathrm{Binomial}(n_i, \\rm{sigm}(\\alpha x_i + \\beta))\\\\\n",
    "%p_i &= \\rm{sigm}(\\alpha + \\beta x_i) \\\\\n",
    "\\alpha &\\sim N(0, 1)\\\\\n",
    "\\beta &\\sim N(0, 1).\n",
    "\\end{align*}\n",
    "\n",
    "Furthermore, according to assumption 4:\n",
    "\n",
    "$$P(y|\\theta) = \\prod_i P(y_i|\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2: Maximum Likelihood estimation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Derive an analytical expression of the likelihood function $\\mathcal{L}(\\theta) = P(y|\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood function $\\mathcal{L}(\\theta)$ is $P(y|\\theta)$, seen as a function of $\\theta$, with $y$ fixed to the observed outcome. <br/>Since the individual observations $y_i$ are independent, we have:\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = P(y|\\theta) = \\prod_{i=1}^N {{n_i}\\choose{y_i}} \\mathrm{sigm}(\\alpha + \\beta x_i)^{y_i} \\cdot (1- \\mathrm{sigm}(\\alpha + \\beta x_i))^{n_i - y_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Derive an analytical expression of the log-likelihood function $\\ell(\\theta)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\ell(\\theta) = \\log \\mathcal{L}(\\theta) = \\sum_i {{n_i}\\choose{y_i}} + \\sum_i y_i \\log \\mathrm{sigm}(\\alpha + \\beta x_i) +  (n_i - y_i) \\log (1- \\mathrm{sigm}(\\alpha + \\beta x_i)).$$\n",
    "\n",
    "The constant term $\\sum_i {{n_i}\\choose{y_i}}$ may be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write a Python function corresponding to the log-likelihood function $\\ell(\\theta)$, up to an additive term that does not depend on $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_lik(alpha, beta):\n",
    "    alpha = np.atleast_1d(alpha)[..., np.newaxis]  # useful to handle grid data\n",
    "    beta = np.atleast_1d(beta)[..., np.newaxis]  # useful to handle grid data\n",
    "    p = sigmoid(alpha * x + beta)\n",
    "    # log_lik = y*np.log(gamma) + (n-y)*np.log(1-gamma)\n",
    "    # nan_to_num handles the multiplication 0*np.inf and set it to 0, as required in our case...\n",
    "    log_lik = np.nan_to_num(y * np.log(p), nan=0) + np.nan_to_num(\n",
    "        (n - y) * np.log(1 - p), nan=0\n",
    "    )\n",
    "    return np.sum(log_lik, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visualize the log-likelihood $\\ell(\\theta)$ up to an additive term in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dalpha = 0.01\n",
    "dbeta = 0.01\n",
    "ALPHA = np.arange(-6.0, -4.00, dalpha)\n",
    "BETA = np.arange(1.5, 3.0, dbeta)\n",
    "AA, BB = np.meshgrid(ALPHA, BETA, indexing=\"xy\")\n",
    "\n",
    "LOG_LIK = log_lik(AA, BB)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "c = ax.pcolormesh(AA, BB, LOG_LIK, cmap=cm.coolwarm, shading=\"auto\")\n",
    "fig.colorbar(c, ax=ax)\n",
    "ax.set_title(f\"Likelihood\")\n",
    "ax.set_xlabel(r\"$\\alpha$\")\n",
    "ax.set_ylabel(r\"$\\beta$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visualize the likelihood $\\mathcal{L}(\\theta)$ up to a multiplicative term in 2D. Explain the steps and comment the results.\n",
    "\n",
    "HINT: you may (carefully) exponentiate the log-likelihood computed on the grid to solve the previous point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIK_SAFE = np.exp(LOG_LIK - np.max(LOG_LIK))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "c = ax.pcolormesh(AA, BB, LIK_SAFE, cmap=cm.coolwarm, shading=\"auto\")\n",
    "fig.colorbar(c, ax=ax)\n",
    "ax.set_title(f\"Likelihood\")\n",
    "ax.set_xlabel(r\"$\\alpha$\")\n",
    "ax.set_ylabel(r\"$\\beta$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute the maximum likelihood (ML) estimate $\\alpha^{\\rm ml}, \\beta^{\\rm ml}$ of the parameters $\\alpha, \\beta$ through numerical optimizations. \n",
    "\n",
    "    Hints:\n",
    "     * You may use the Python function `scipy.optimize.minimize`. \n",
    "     * You may look at the figures above to choose a good starting point for optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "log_lik_theta = lambda theta: log_lik(theta[0], theta[1])\n",
    "nll_theta = lambda theta: -log_lik_theta(theta)  # negative log-likelihood function.\n",
    "res = minimize(nll_theta, x0=[4, -8])\n",
    "theta_ml = res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visualize the likelihood together with the ML estimate. Comment the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "c = ax.pcolormesh(AA, BB, LIK_SAFE, cmap=cm.coolwarm, shading=\"auto\")\n",
    "plt.plot(theta_ml[0], theta_ml[1], \"kx\", label=\"ML\")\n",
    "fig.colorbar(c, ax=ax)\n",
    "ax.set_title(f\"Likelihood\")\n",
    "ax.set_xlabel(r\"$\\alpha$\")\n",
    "ax.set_ylabel(r\"$\\beta$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2: Maximum A Posteriori Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Derive an analytical expression of the *unnormalized* posterior $f(\\theta | y)$, i.e. up to a multiplicative term that does not depend on $\\theta$.\n",
    "\n",
    "Hint: exploit the already-obtained likelihood and the functional form of the Gaussian pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(\\theta | y) = \\frac{P(y | \\theta) f(\\theta)}{P(y)} \\propto \\mathcal{L}(\\theta)\n",
    "\\exp\\left(-\\frac{1}{2} \\frac{(\\alpha - \\mu_\\alpha)^2}{\\sigma^2_\\alpha} \\right ) \n",
    "\\exp\\left(-\\frac{1}{2} \\frac{(\\beta - \\mu_\\beta)^2}{\\sigma^2_\\beta} \\right ). $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Derive an analytical expression of the *unnormalized* log-posterior $\\log f(\\theta | y)$, i.e. up to an additive term that does not depend on $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write a Python function corresponding to the  unnormalized log-posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_alpha = 0.0\n",
    "sigma_alpha = 1.0\n",
    "mu_beta = 0.0\n",
    "sigma_beta = 1.0\n",
    "\n",
    "#prior_alpha = stats.norm(loc=mu_alpha, scale=sigma_alpha)\n",
    "#prior_beta = stats.norm(loc=mu_beta, scale=sigma_beta)\n",
    "\n",
    "\n",
    "def log_post_unscaled(alpha, beta):\n",
    "    log_lik_val = log_lik(alpha, beta)\n",
    "    # return lik_val * prior_alpha.pdf(alpha) * prior_beta.pdf(beta)\n",
    "    return (\n",
    "        log_lik_val\n",
    "        - 0.5 * (alpha - mu_alpha) ** 2 / sigma_alpha**2\n",
    "        - 0.5 * (beta - mu_beta) ** 2 / sigma_beta**2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute the maximum a posteriori (MAP) estimate $\\alpha^{\\rm MAP}, \\beta^{\\rm MAP}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "minus_logpost = lambda theta: -log_post_unscaled(theta[0], theta[1])\n",
    "res = minimize(minus_logpost, x0=[-5, 2])\n",
    "theta_map = res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visualize the MAP and ML estimates in 2D, together with the unnormalized posterior. Comment the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_POST_UNSC = log_post_unscaled(AA, BB)\n",
    "POST_UNSC = np.exp(LOG_POST_UNSC - np.max(LOG_POST_UNSC))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "c = ax.pcolormesh(AA, BB, POST_UNSC, cmap=cm.coolwarm, shading=\"auto\")\n",
    "plt.plot(theta_map[0], theta_map[1], \"kx\", label=\"MAP\")\n",
    "plt.plot(theta_ml[0], theta_ml[1], \"ko\", label=\"ML\")\n",
    "fig.colorbar(c, ax=ax)\n",
    "ax.set_title(f\"Unnormalized posterior\")\n",
    "ax.set_xlabel(r\"$\\alpha$\")\n",
    "ax.set_ylabel(r\"$\\beta$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Brute-force posterior estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute (and visualize) a gridding approximation of the *normalized* posterior, i.e. with the correct normalization constant. Explain the steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have:\n",
    "    $$ \\tilde f(\\theta | y) = \\mathcal{L}(\\theta) \\exp\\left(-\\frac{1}{2} \n",
    "(\\theta - \\mu)^{\\top} \\Sigma_0^{-1} (\\theta - \\mu)^{\\top} \\right) = Z f(\\theta | y),$$\n",
    "where $Z$ is the to-be-determined normalization constant and it must be chosen such that:\n",
    "$$\\iint f(\\theta | y) d\\alpha\\; d\\beta = 1.$$\n",
    "Thus,\n",
    "$$Z = \\iint f(\\theta | y) d\\alpha\\; d\\beta.$$\n",
    "\n",
    "The integral above is intractable, but a gridding approximation may be used. Using an equi-spaced gridding, a Riemann Sum approximation is:\n",
    "\n",
    "$$Z \\approx \\Delta \\alpha \\Delta \\beta \\sum_i f(\\theta_i | y),$$\n",
    "\n",
    "where $\\Delta \\alpha$ and $\\Delta \\beta$ are the discretization steps of the 2D grid and $\\theta_i$ are the grid points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repreated from above for completeness\n",
    "LOG_POST_UNSC = log_post_unscaled(AA, BB)\n",
    "POST_UNSC = np.exp(LOG_POST_UNSC - np.max(LOG_POST_UNSC))\n",
    "\n",
    "normalizing_factor = np.sum(POST_UNSC) * dalpha * dbeta\n",
    "POST_SC = POST_UNSC / normalizing_factor\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "c = ax.pcolormesh(AA, BB, POST_SC, cmap=cm.coolwarm, shading=\"auto\")\n",
    "plt.plot(theta_map[0], theta_map[1], \"kx\")\n",
    "plt.plot(theta_ml[0], theta_ml[1], \"ko\")\n",
    "fig.colorbar(c, ax=ax)\n",
    "ax.set_title(f\"Normalized posterior\")\n",
    "ax.set_xlabel(r\"$\\alpha$\")\n",
    "ax.set_ylabel(r\"$\\beta$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using the grid-based approximation of the posterior, compute the posterior mean of $\\alpha$ and $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By definition, we have:\n",
    "\n",
    "$$E[\\theta] = \\iint \\theta p(\\theta | y) d\\alpha\\; d\\beta.$$\n",
    "\n",
    "Using the grid-based approximation above:\n",
    "\n",
    "$$E[\\theta] = \\Delta \\alpha \\Delta \\beta \\sum \\theta_i p(\\theta_i | y).$$\n",
    "\n",
    "Software implementation below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_mean = np.sum(AA * POST_SC) * dalpha * dbeta\n",
    "b_mean = np.sum(BB * POST_SC) * dalpha * dbeta\n",
    "a_mean, b_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = np.linspace(0, 1, 100)\n",
    "p_ml = sigmoid(theta_ml[0] * x_range + theta_ml[1])\n",
    "p_map = sigmoid(theta_map[0] * x_range + theta_map[1])\n",
    "plt.figure()\n",
    "plt.plot(x_range, p_ml)\n",
    "plt.plot(x_range, p_map)\n",
    "plt.plot(x, frac, \"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Monte Carlo estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Obtain a sample-based approximation of the posterior $f(\\theta | y)$ by implementing the Metropolis algorithm from scratch.\n",
    "   * Run (at least) two chains\n",
    "   * Diagnose the outcome of the different chains by overlapping the corresponding density plots. Is the algorithm sampling correctly?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_ratio_fun(alpha_propose, beta_propose, alpha_previous, beta_previous):\n",
    "    log_p_previous = log_post_unscaled(alpha_previous, beta_previous)\n",
    "    log_p_propose = log_post_unscaled(alpha_propose, beta_propose)\n",
    "    log_p_ratio = log_p_propose - log_p_previous # log(p_prop/p_prev) = log(p_prop) - log(p_prev)\n",
    "    p_ratio = np.exp(log_p_ratio)\n",
    "    return p_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ratio_fun(alpha_propose = -5.0, alpha_previous = -5.1, beta_propose = 2.0, beta_previous = 2.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run a Metropolis algorithm to sample from the posterior. The `p_ratio_fun` function is all we need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20_000 # number of Metropolis steps\n",
    "warmup = 1_000\n",
    "chains = 5\n",
    "alpha_0 = mu_alpha # initial value for alpha. Another idea would be to start from the MAP...\n",
    "beta_0 = mu_beta # initial value for alpha\n",
    "\n",
    "alpha_step = alpha_0\n",
    "beta_step = beta_0\n",
    "sigma_prop_alpha = 0.2\n",
    "sigma_prop_beta = 0.2\n",
    "\n",
    "trace = []\n",
    "for chain in range(chains):\n",
    "    alphas = []\n",
    "    betas = []\n",
    "    for idx in range(N):\n",
    "        alphas.append(alpha_step)\n",
    "        betas.append(beta_step)\n",
    "\n",
    "        alpha_prop = alpha_step + sigma_prop_alpha * np.random.randn()\n",
    "        beta_prop = beta_step + sigma_prop_beta * np.random.randn()\n",
    "    \n",
    "        p_ratio = p_ratio_fun(alpha_prop, beta_prop, alpha_step, beta_step)\n",
    "        accept_prob = np.minimum(1.0, p_ratio)\n",
    "        accept = (np.random.rand() < accept_prob)\n",
    "        \n",
    "        if accept:\n",
    "            alpha_step = alpha_prop\n",
    "            beta_step = beta_prop\n",
    "\n",
    "    alphas = np.stack(alphas)\n",
    "    betas = np.stack(betas)\n",
    "    trace_chain = np.c_[alphas, betas]\n",
    "    trace.append(trace_chain)\n",
    "trace = np.stack(trace, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
    "plt.suptitle(\"Trace\")\n",
    "for var in range(2):\n",
    "    for chain in range(chains):\n",
    "        #ax[var, 0].hist(trace[chain, warmup:, var], density=True, alpha=0.4)\n",
    "        sns.kdeplot(trace[chain, warmup:, var], ax=ax[var, 0])\n",
    "        ax[var, 1].plot(trace[chain, :, var])\n",
    "    ax[var, 1].axvline(x=warmup, color=\"red\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = np.concatenate(trace[:, warmup:, :])\n",
    "x_range = np.linspace(0, 1, 100)\n",
    "p_samples = sigmoid(np.expand_dims(traces[:, 0], axis=-1) * x_range + np.expand_dims(traces[:, 1], axis=-1))\n",
    "plt.figure()\n",
    "plt.plot(x_range, p_samples[:1000, :].T, \"b\", alpha=1.0)\n",
    "plt.plot(x_range, p_map, \"k\")\n",
    "plt.plot(x, frac, \"*\")\n",
    "#plt.plot(x, prop, \"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compare the results of gridding and Metropolis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n",
    "ax[0].hist2d(x=trace_chain[warmup:, 0], y=trace_chain[warmup:, 1], bins=100, cmap=plt.cm.BuPu)\n",
    "ax[0].set_xlim([-6, -4]);\n",
    "ax[0].set_ylim([1.5, 3]);\n",
    "ax[0].contour(AA, BB, POST_SC); #, levels=[5, 15,  95]); # levels=[5, 15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "c = ax[1].pcolormesh(AA, BB, POST_SC, cmap=cm.coolwarm, shading='auto')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
