{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from scipy.special import logsumexp\n",
    "from matplotlib import cm\n",
    "import arviz as az\n",
    "import pymc as pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assignment 1: Babies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "babies = pd.read_csv('babies.csv').sample(10, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_obs = babies[\"Month\"].values # x variable\n",
    "length_obs = babies[\"Length\"].values # y variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Problem and data\n",
    "\n",
    "The length of human babies $y$ increase with the month from birth $x$ of the baby. For simplicity, we hypothize a *linear* dependency between $x$ and $y$, see scatterplot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(babies[\"Month\"], babies[\"Length\"], 'C0.', alpha=1.0, label=\"samples\")\n",
    "#ax.plot(0, alpha, \"r*\", label=\"alpha\")\n",
    "ax.set_ylabel(\"Length $y$ (cm)\")\n",
    "ax.set_xlabel(\"Month $x$ (-)\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the linear hypotesis seems reasonable, the points do not lie exactly on a line. In order to better characterize the data, we introduce the following probabilistic assumptions:\n",
    "\n",
    "\n",
    "\n",
    "1. The average of the baby length $y_{\\mu}$ at a given month $x$ is linear with intercept and slope coefficients \n",
    "   $\\alpha$ and $\\beta$, respectively:  $y_{\\mu} = \\alpha + \\beta x$\n",
    "\n",
    "2. The actual baby length is Gaussian with mean $y_{\\mu}$ and standard deviation $\\sigma$\n",
    "3. The prior probability of $\\beta$ is Gaussian with mean $0$ and standard deviation $\\sigma_\\beta=0.5$\n",
    "4. The prior probability of $\\sigma$ is a Half-Normal with standard deviation $\\sigma_\\sigma=1.0$\n",
    "5. The intercept coefficient $\\alpha$ is considered known and equal to $55.0$\n",
    "6. Probabilities of different observations are independent of each other, given  \\beta, \\sigma$\n",
    "\n",
    "\n",
    "Note: Condition 5 is rather artificial. It has been introduced here to limit the number of random variables and allow implementation of different approximate inference techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilistic model constants\n",
    "alpha = 55.0\n",
    "sigma_sigma = 1.0\n",
    "sigma_beta = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.1: Probabilistic model\n",
    "\n",
    "* Derive and comment the full probabilistic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Putting together the probabilistic assumptions 1-3, we obtain:\n",
    "\n",
    "\\begin{align*}\n",
    "y_i &\\sim \\mathcal{N}(\\alpha + \\beta x_i, \\sigma)\\\\\n",
    "\\beta &\\sim \\mathcal{N}(0, \\sigma_\\beta)\\\\\n",
    "\\sigma &\\sim |\\mathcal{N}|(\\sigma_\\sigma)\\\\\n",
    "\\alpha &= 55.0\n",
    "\\end{align*}\n",
    "\n",
    "Furthermore, according to assumption 6:\n",
    "\n",
    "$$f(y|\\theta) = \\prod_i f(y_i|\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.2: Maximum Likelihood estimation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Derive an analytical expression of the likelihood function $\\mathcal{L}(\\beta, \\sigma) = f(y|\\beta, \\sigma)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The likelihood function $\\mathcal{L}(\\beta, \\sigma)$ is $P(y|\\beta, \\sigma)$, seen as a function of $\\beta, \\sigma$, with $y$ fixed to the observed outcome. <br/>Since the individual observations $y_i$ are independent, we have:\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = f(y|\\theta) = \\prod_{i=1}^N \\frac{1}{\\sigma \\sqrt{2\\pi}} \n",
    "e^{-\\frac{1}{2} \\bigg( \\frac{y_i - (\\alpha + \\beta x_i)}{\\sigma} \\bigg )^2 }  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Derive an analytical expression of the log-likelihood function $\\ell(\\beta, \\sigma) = \\log P(y|\\beta, \\sigma)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\\ell(\\beta, \\sigma) &= \\sum_{i=1}^N -log(\\sigma) -1/2 \\log(2\\pi) -\\frac{1}{2}\\bigg( \\frac{y_i - (\\alpha + \\beta x_i)}{\\sigma} \\bigg )^2\\\\\n",
    "&= -\\frac{N}{2} \\log (2\\pi) -N\\log \\sigma - \\frac{1}{2 \\sigma^2}\\sum_{i=1}^{N} \\big{(} y_i - (\\alpha + \\beta x_i)  \\big{)} ^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write a Python function corresponding to the log-likelihood function $\\ell(\\beta, \\sigma)$. You may ignore additive factors which do not depend on $\\beta, \\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_lik(beta, sigma):\n",
    "    x = month_obs\n",
    "    #x = np.sqrt(babies[\"Month\"].values)\n",
    "    y = length_obs  \n",
    "    y_mu = alpha + beta*x\n",
    "    N = x.shape[0]\n",
    "    log_lik = -N/2*np.log(2*np.pi) - N*np.log(sigma) -1/(2*sigma**2) * np.sum((y - y_mu)**2)\n",
    "    return log_lik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visualize the log-likelihood function $\\ell(\\beta, \\sigma)$ in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbeta = 0.005\n",
    "dsigma = 0.005\n",
    "b_min, b_max = 1.2, 1.7\n",
    "s_min, s_max = 0.9, 4.0\n",
    "BETA = np.arange(b_min, b_max, dbeta)\n",
    "SIGMA = np.arange(s_min, s_max, dsigma)\n",
    "\n",
    "\n",
    "BB, SS = np.meshgrid(BETA, SIGMA, indexing='xy')\n",
    "BBSS = np.stack((BB, SS), axis=-1)\n",
    "\n",
    "LL = np.empty((SIGMA.shape[0], BETA.shape[0]))\n",
    "for i in range(SIGMA.shape[0]):\n",
    "    for j in range(BETA.shape[0]):\n",
    "        LL[i, j] = log_lik(BETA[j], SIGMA[i])\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "c = ax.pcolormesh(BB, SS, LL, cmap=cm.coolwarm, shading='auto')\n",
    "\n",
    "fig.colorbar(c, ax=ax)\n",
    "ax.set_title(f\"Log-likelihood (up to an additive constant)\");\n",
    "ax.set_xlabel(r\"$\\beta$\");\n",
    "ax.set_ylabel(r\"$\\sigma$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Obtain the maximum likelihood estimate $\\beta^{\\rm ml}, \\sigma^{\\rm ml}$\n",
    "\n",
    "Hint: choose a good point to initialize the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "log_lik_theta = lambda theta: log_lik(theta[0], theta[1])\n",
    "nll_theta = lambda theta: -log_lik_theta(theta) # negative log-likelihood function.\n",
    "res = minimize(nll_theta, x0=[1.5, 1.5])\n",
    "theta_ml = res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visualize the log-likelihood together with the maximum likelihood estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "c = ax.pcolormesh(BB, SS, LL, cmap=cm.coolwarm, shading='auto')\n",
    "plt.plot(theta_ml[0], theta_ml[1], \"kx\")\n",
    "fig.colorbar(c, ax=ax)\n",
    "ax.set_title(f\"Log-likelihood (up to an additive constant)\");\n",
    "ax.set_xlabel(r\"$\\beta$\");\n",
    "ax.set_ylabel(r\"$\\sigma$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visualize the *likelihood* function $\\mathcal{L}(\\sigma, \\beta)$ in 2D up to a multiplicative constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIK_SC = np.exp(LL - np.max(LL)) # scaled to max_i LL_i = 1\n",
    "#LIK_SC = np.exp(LL - logsumexp(LL)) # scaled to sum_i LL_i = 1 \n",
    "LIK_SC = np.exp(LL) # no scaling, dangerous numerically. Fails if we have around 250 data points\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "c = ax.pcolormesh(BB, SS, LIK_SC, cmap=cm.coolwarm, shading='auto')\n",
    "plt.plot(theta_ml[0], theta_ml[1], \"kx\")\n",
    "fig.colorbar(c, ax=ax)\n",
    "ax.set_title(f\"Likelihood (up to a multiplicative constant)\");\n",
    "ax.set_xlabel(r\"$\\beta$\");\n",
    "ax.set_ylabel(r\"$\\sigma$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.2: Maximum A Posteriori Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Derive an analytical expression of the posterior $f(\\theta | y)$, up to a *multiplicative* factor that does not depend on $\\beta, \\sigma$. \n",
    "\n",
    "Hint: exploit the already-obtained likelihood and the functional form of the Gaussian pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(\\theta | y) = \\frac{P(y | \\theta) f(\\theta)}{P(y)} \\propto \\mathcal{L}(\\theta)\n",
    "\\exp\\left(-\\frac{1}{2} \\frac{\\beta^2}{\\sigma^2_\\beta} \\right )\n",
    "\\exp\\left(-\\frac{1}{2} \\frac{\\sigma^2}{\\sigma^2_\\sigma} \\right ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Derive an analytical expression of the log-posterior $\\log f(\\theta | y)$, up to an *additive* factor that does not depend on $\\beta, \\sigma$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "$$\\log f(\\theta | y) = \\log \\frac{P(y | \\theta) f(\\theta)}{P(y)} = \\log P(y | \\theta) + \\log f(\\beta) + \\log f(\\sigma) - \\log P(y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, \n",
    "\n",
    "$$\\log f(\\theta | y) = \\ell(\\theta) -\\frac{1}{2} \\frac{\\beta^2}{\\sigma_\\beta^2} -\\frac{1}{2} \\frac{\\sigma^2}{\\sigma_\\sigma^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Write the log-posterior (up to an additive factor) as Python functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def log_post_unscaled(beta, sigma):\n",
    "    log_lik_val = log_lik(beta, sigma)\n",
    "    return log_lik_val -0.5*beta**2/sigma_beta**2 -0.5* sigma**2/sigma_sigma**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Compute the maximum a posteriore estimate $\\beta^{\\rm MAP}, \\sigma^{\\rm MAP}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "minus_logpost = lambda theta: -log_post_unscaled(theta[0], theta[1])\n",
    "res = minimize(minus_logpost, x0=[1.4, 2])\n",
    "theta_map = res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Visualize the MAP estimate together with the log-posterior in 2D. Comment the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "LP_UNSC = np.empty((SIGMA.shape[0], BETA.shape[0]))\n",
    "for i in range(SIGMA.shape[0]):\n",
    "    for j in range(BETA.shape[0]):\n",
    "        LP_UNSC[i, j] = log_post_unscaled(BETA[j], SIGMA[i])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "c = ax.pcolormesh(BETA, SIGMA, LP_UNSC, cmap=cm.coolwarm, shading='auto')\n",
    "plt.plot(theta_ml[0], theta_ml[1], \"kx\")\n",
    "plt.plot(theta_map[0], theta_map[1], \"ko\")\n",
    "fig.colorbar(c, ax=ax)\n",
    "ax.set_title(f\"Log-posterior (up to an additive factor)\");\n",
    "ax.set_xlabel(r\"$\\beta$\");\n",
    "ax.set_ylabel(r\"$\\sigma$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.3 Brute-force posterior estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Compute a gridding approximation of the *normalized* posterior, with the correct normalization constant. Explain the passages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We have:\n",
    "    $$ \\tilde f(\\theta | y) = \\mathcal{L}(\\theta) \\exp\\left(-\\frac{1}{2} \n",
    "(\\theta - \\mu)^{\\top} \\Sigma_0^{-1} (\\theta - \\mu)^{\\top} \\right) = Z f(\\theta | y),$$\n",
    "where $Z$ is the to-be-determined normalization constant and it must be chosen such that:\n",
    "$$\\iint f(\\theta | y) d\\alpha\\; d\\beta = 1.$$\n",
    "Thus,\n",
    "$$Z = \\iint f(\\theta | y) d\\alpha\\; d\\beta.$$\n",
    "\n",
    "The integral above is intractable, but a gridding approximation may be used. Using an equi-spaced gridding, a Riemann Sum approximation is:\n",
    "\n",
    "$$Z \\approx \\Delta \\alpha \\Delta \\beta \\sum_i f(\\theta_i | y),$$\n",
    "\n",
    "where $\\Delta \\alpha$ and $\\Delta \\beta$ are the discretization steps of the 2D grid and $\\theta_i$ are the grid points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Fast solution, if we know all the tricks...\n",
    "P_SC = np.exp(LP_UNSC - logsumexp(LP_UNSC))\n",
    "P_SC = P_SC/(dbeta * dsigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visualize the *normalized* posterior, together with the ML and MAP estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "c = ax.pcolormesh(BETA, SIGMA, P_SC, cmap=cm.coolwarm, shading='auto')\n",
    "plt.plot(theta_ml[0], theta_ml[1], \"kx\")\n",
    "plt.plot(theta_map[0], theta_map[1], \"ko\")\n",
    "fig.colorbar(c, ax=ax)\n",
    "ax.set_title(f\"Normalized posterior\");\n",
    "ax.set_xlabel(r\"$\\beta$\");\n",
    "ax.set_ylabel(r\"$\\sigma$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Using the grid-based approximation of the posterior, compute the posterior mean of $\\beta$ and $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "By definition, we have:\n",
    "\n",
    "$$E[\\beta] = \\iint \\beta p(\\beta, \\sigma | y) d\\beta\\; d\\sigma.$$\n",
    "\n",
    "Using the grid-based approximation above:\n",
    "\n",
    "$$E[\\beta] = \\Delta \\beta \\Delta \\sigma \\sum \\beta_i p(\\beta_i, \\sigma_i | y).$$\n",
    "\n",
    "Similar formulas apply for $E[\\sigma]$. Software implementation below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "beta_mean = np.sum(BB*P_SC)*dbeta*dsigma\n",
    "sigma_mean = np.sum(SS*P_SC)*dbeta*dsigma\n",
    "beta_mean, sigma_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is yet another meaningful point estimate of the latent variables $\\beta, \\sigma$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.4 Monte-carlo estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Obtain a sample-based approximation of the posterior $f(\\beta, \\sigma | y)$ using pymc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model_baby_linear:\n",
    "    beta = pm.Normal('beta', sigma=sigma_beta)\n",
    "    \n",
    "    length_mean = pm.Deterministic(\"length_mean\", alpha  + beta * month_obs)\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=sigma_sigma)\n",
    "\n",
    "    length = pm.Normal(\"length\", mu=length_mean, sigma=sigma, observed=length_obs)\n",
    "    inf_data = pm.sample(draws=10_000, tune=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(inf_data, var_names=[\"beta\", \"sigma\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_data_flat = inf_data.posterior.stack(sample=(\"chain\", \"draw\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(inf_data_flat[\"beta\"], inf_data_flat[\"sigma\"])\n",
    "plt.xlabel(\"$beta$\")\n",
    "plt.ylabel(\"sigma\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Obtain a sample-based approximation of the posterior $f(\\beta, \\sigma | y)$ by implementing the Metropolis algorithm from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def p_ratio_fun(beta_propose, sigma_propose, beta_previous, sigma_previous):\n",
    "    log_p_previous = log_post_unscaled(beta_previous, sigma_previous)\n",
    "    log_p_propose = log_post_unscaled(beta_propose, sigma_propose)\n",
    "    log_p_ratio = log_p_propose - log_p_previous # log(p_prop/p_prev) = log(p_prop) - log(p_prev)\n",
    "    p_ratio = np.exp(log_p_ratio)\n",
    "    return p_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "p_ratio_fun(beta_propose=1.5, sigma_propose =1.5, beta_previous=1.4, sigma_previous =1.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let us run a Metropolis algorithm to sample from the posterior. The `p_ratio_fun` function is all we need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "draws = 100_000 # mc draws\n",
    "tune = 10_000 # mc burn-in \n",
    "beta_0 = 1.5 # initial value for beta\n",
    "sigma_0 = 1.5 # initial value for sigma\n",
    "sigma_prop_beta = 0.1\n",
    "sigma_prop_sigma = 0.1\n",
    "\n",
    "beta_step = beta_0\n",
    "sigma_step = sigma_0\n",
    "\n",
    "betas = []\n",
    "sigmas = []\n",
    "\n",
    "for idx in range(draws+tune):\n",
    "    betas.append(beta_step)\n",
    "    sigmas.append(sigma_step)\n",
    "                  \n",
    "    beta_prop = beta_step + sigma_prop_beta * np.random.randn()\n",
    "    sigma_prop = sigma_step + sigma_prop_sigma * np.random.randn()\n",
    "  \n",
    "    p_ratio = p_ratio_fun(beta_prop, sigma_prop, beta_step, sigma_step)\n",
    "    accept_prob = np.minimum(1.0, p_ratio)\n",
    "    accept = (np.random.rand() < accept_prob)\n",
    "    \n",
    "    if accept:\n",
    "        beta_step = beta_prop\n",
    "        sigma_step = sigma_prop\n",
    "                  \n",
    "betas = np.stack(betas)\n",
    "sigmas = np.stack(sigmas)\n",
    "thetas = np.c_[betas, sigmas]\n",
    "thetas = thetas[tune:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(thetas, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.cov(thetas.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.sum(BB*P_SC)*dbeta*dsigma, np.sum(SS*P_SC)*dbeta*dsigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(thetas[:,0])#px.scatter(thetas[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(thetas[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compare the posterior estimation results obtained with the different techniques (brute-force, pymc, metropolis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n",
    "ax[0].hist2d(x=thetas[:, 0], y=thetas[:, 1], bins=100, cmap=plt.cm.BuPu)\n",
    "ax[0].set_xlim([b_min, b_max]);\n",
    "ax[0].set_ylim([s_min, s_max]);\n",
    "ax[0].contour(BB, SS, P_SC); #, levels=[5, 15,  95]); # levels=[5, 15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "c = ax[1].pcolormesh(BB, SS, P_SC, cmap=cm.coolwarm, shading='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mdl = np.arange(25)\n",
    "y_mdl = alpha + beta_mean*x_mdl # basic deterministic model (just to check)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(babies[\"Month\"], babies[\"Length\"], 'C0.', alpha=1.0, label=\"samples\")\n",
    "ax.plot(x_mdl, y_mdl, \"k\")\n",
    "ax.set_ylabel(\"Length $y$ (cm)\")\n",
    "ax.set_xlabel(\"Month $x$ (-)\")\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
