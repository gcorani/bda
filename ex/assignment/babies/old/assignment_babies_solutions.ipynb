{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from matplotlib import cm\n",
    "import arviz as az\n",
    "import pymc as pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assignment 1: Toxicity bioassay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Problem and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The length of babies follows a square root formula..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "babies = pd.read_csv('babies.csv')\n",
    "babies.sample(5)\n",
    "month_obs = babies[\"Month\"].values\n",
    "length_obs = babies[\"Length\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#babies.groupby(\"Month\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(babies[\"Month\"], babies[\"Length\"], 'C0.', alpha=0.1)\n",
    "ax.set_ylabel(\"Length\")\n",
    "ax.set_xlabel(\"Month\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model_baby_linear_full:\n",
    "    alpha = pm.Normal('alpha', sigma=10)\n",
    "    beta = pm.Normal('beta', sigma=10)\n",
    "    \n",
    "    # Use dot product instead of expanded multiplication\n",
    "    length_mean = pm.Deterministic(\"length_mean\", alpha  + beta * np.sqrt(babies[\"Month\"]))\n",
    "    #μ = pm.Deterministic(\"μ\", pm.math.dot(babies[[\"Intercept\", \"Month\"]], β))\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=10)\n",
    "\n",
    "    length = pm.Normal(\"length\", mu=length_mean, sigma=sigma, observed=babies[\"Length\"])\n",
    "    #prior_check = pm.sample_prior_predictive(samples=50, random_seed=42)\n",
    "    inf_data_full = pm.sample(draws=2000, tune=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_data_full_flat = inf_data_full.posterior.stack(sample=(\"chain\", \"draw\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(inf_data_full_flat[\"alpha\"], inf_data_full_flat[\"beta\"])\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"beta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model_baby_linear:\n",
    "    alpha = 49.5 #pm.Normal('alpha', sigma=10)\n",
    "    beta = pm.Normal('beta', sigma=10)\n",
    "    \n",
    "    # Use dot product instead of expanded multiplication\n",
    "    length_mean = pm.Deterministic(\"length_mean\", alpha  + beta * np.sqrt(babies[\"Month\"]))\n",
    "    #μ = pm.Deterministic(\"μ\", pm.math.dot(babies[[\"Intercept\", \"Month\"]], β))\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=10)\n",
    "\n",
    "    length = pm.Normal(\"length\", mu=length_mean, sigma=sigma, observed=babies[\"Length\"])\n",
    "    #prior_check = pm.sample_prior_predictive(samples=50, random_seed=42)\n",
    "    inf_data = pm.sample(draws=2000, tune=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(inf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_data_flat = inf_data.posterior.stack(sample=(\"chain\", \"draw\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(inf_data_flat[\"beta\"], inf_data_flat[\"sigma\"])\n",
    "plt.xlabel(\"beta\")\n",
    "plt.ylabel(\"sigma\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Modeling assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For the probabilistic model, we make the following assumptions:\n",
    "\n",
    "1. The average of the baby length is .\n",
    "\n",
    "2. The standard deviation:\n",
    "3. The prior probability of the parameters\n",
    "4. Probabilities are independent...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.1: Probabilistic model\n",
    "\n",
    "* Derive and comment the full probabilistic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Putting together the probabilistic assumptions 1-3, we obtain:\n",
    "\n",
    "\\begin{align*}\n",
    "y_i &\\sim \\mathcal{N}(\\alpha + \\beta \\sqrt M_i, \\sigma)\\\\\n",
    "\\beta &\\sim \\mathcal{N}(0, 10)\\\\\n",
    "\\sigma &\\sim |\\mathcal{N}|(10)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Furthermore, according to assumption 4:\n",
    "\n",
    "$$f(y|\\theta) = \\prod_i f(y_i|\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.2: Maximum Likelihood estimation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Derive an analytical expression of the likelihood function $\\mathcal{L}(\\theta) = f(y|\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The likelihood function $\\mathcal{L}(\\theta)$ is $P(y|\\theta)$, seen as a function of $\\theta$, with $y$ fixed to the observed outcome. <br/>Since the individual observations $y_i$ are independent, we have:\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = f(y|\\theta) = \\prod_{i=1}^N \\frac{1}{\\sigma \\sqrt{2\\pi}} \n",
    "e^{-\\frac{1}{2} \\bigg( \\frac{y_i - (\\alpha + \\beta x_i)}{\\sigma} \\bigg )^2 }  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Derive an analytical expression of the log-likelihood function $\\ell(\\theta) = \\log P(y|\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\\ell(\\theta) &= \\sum_{i=1}^N -log(\\sigma) -1/2 \\log(2\\pi) -\\frac{1}{2}\\bigg( \\frac{y_i - (\\alpha + \\beta x_i)}{\\sigma} \\bigg )^2\\\\\n",
    "&= -\\frac{N}{2} \\log (2\\pi) -N\\log \\sigma - \\frac{1}{2 \\sigma^2}\\sum_{i=1}^{N} \\big{(} y_i - (\\alpha + \\beta x_i)  \\big{)} ^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Write a Python function corresponding to the likelihood function $\\mathcal{L}(\\theta)$. Ignore multiplicative factors which do not depend on $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def lik(beta, sigma):\n",
    "    pass\n",
    "    # ... TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Likelihood of the 4 observations (neglecting the multiplicative factor).\n",
    "# The overall likelihood is the product of all terms.\n",
    "\n",
    "def lik(beta, sigma):\n",
    "    beta = np.atleast_1d(beta)[..., np.newaxis]\n",
    "    sigma = np.atleast_1d(sigma)[..., np.newaxis]\n",
    "    m_sqrt = np.sqrt(babies[\"Month\"].values)\n",
    "    y = babies[\"Length\"].values\n",
    "\n",
    "    # add the spatial channels to the data\n",
    "    for idx in range(beta.ndim-1):\n",
    "        m_sqrt = m_sqrt[np.newaxis, :]\n",
    "        y = y[np.newaxis, :]\n",
    "    \n",
    "    y_mu = alpha + beta*m_sqrt\n",
    "    resid = y - y_mu\n",
    "    lik = 1/(sigma*np.sqrt(2*np.pi)) * np.exp(-1/2*(resid/sigma)**2)\n",
    "    lik = np.prod(lik, axis=-1)\n",
    "    return lik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Visualize the likelihood function in 2D and comment the obtained figure. \n",
    "\n",
    "   Hints:\n",
    "    * you may use the `pcolormesh` function of `matplotlib`\n",
    "    * appropriate ranges for $\\alpha$ and $\\beta$ are $[-4, 8]$ and $[-10, 40]$, respectively\n",
    "    * an appropriate step size for both $\\alpha$ and $\\beta$ is 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dbeta = 0.01\n",
    "dsigma = 0.01\n",
    "\n",
    "BETA = np.arange(5, 10, dbeta)\n",
    "SIGMA = np.arange(2, 4, dsigma)\n",
    "BB, SS = np.meshgrid(BETA, SIGMA, indexing='xy')\n",
    "BBSS = np.stack((BB, SS), axis=-1)\n",
    "LL = lik(BB, SS)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "c = ax.pcolormesh(BB, SS, LL, cmap=cm.coolwarm, shading='auto')\n",
    "#plt.plot(theta_ml[0], theta_ml[1], \"kx\")\n",
    "fig.colorbar(c, ax=ax)\n",
    "ax.set_title(f\"Likelihood\");\n",
    "ax.set_xlabel(r\"$\\alpha$\");\n",
    "ax.set_ylabel(r\"$\\beta$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_lik(beta, sigma):\n",
    "    x = np.sqrt(babies[\"Month\"].values)\n",
    "    y = babies[\"Length\"].values    \n",
    "    y_mu = alpha + beta*x\n",
    "    N = x.shape[0]\n",
    "    log_lik = -N/2*np.log(2*np.pi) - N*np.log(sigma) -1/(2*sigma**2) * np.sum((y - y_mu)**2)\n",
    "    return log_lik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_lik_vec(beta, sigma):\n",
    "    # Vectorized version of the previous\n",
    "    beta = np.atleast_1d(beta)[..., np.newaxis]\n",
    "    sigma = np.atleast_1d(sigma)[..., np.newaxis]\n",
    "    m_sqrt = np.sqrt(babies[\"Month\"].values)\n",
    "    y = babies[\"Length\"].values\n",
    "\n",
    "    # add the spatial channels to the data\n",
    "    for idx in range(beta.ndim-1):\n",
    "        m_sqrt = m_sqrt[np.newaxis, :]\n",
    "        y = y[np.newaxis, :]\n",
    "    \n",
    "    y_mu = alpha + beta*m_sqrt\n",
    "    resid = y - y_mu\n",
    "    log_lik =  - 1/(2*sigma**2)* resid**2 -np.log(sigma*np.sqrt(2*np.pi))\n",
    "    log_lik = np.sum(log_lik, axis=-1)\n",
    "    return log_lik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbeta = 0.002\n",
    "dsigma = 0.002\n",
    "\n",
    "BETA = np.arange(7.1, 7.5, dbeta)\n",
    "SIGMA = np.arange(2.6, 3.5, dsigma)\n",
    "BB, SS = np.meshgrid(BETA, SIGMA, indexing='xy')\n",
    "BBSS = np.stack((BB, SS), axis=-1)\n",
    "\n",
    "LL = np.empty((SIGMA.shape[0], BETA.shape[0]))\n",
    "for i in range(SIGMA.shape[0]):\n",
    "    for j in range(BETA.shape[0]):\n",
    "        LL[i, j] = log_lik(BETA[j], SIGMA[i])\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "c = ax.pcolormesh(BB, SS, LL, cmap=cm.coolwarm, shading='auto')\n",
    "\n",
    "fig.colorbar(c, ax=ax)\n",
    "ax.set_title(f\"Log-likelihood\");\n",
    "ax.set_xlabel(r\"$\\beta$\");\n",
    "ax.set_ylabel(r\"$\\sigma$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "log_lik_theta = lambda theta: log_lik(theta[0], theta[1])\n",
    "nll_theta = lambda theta: -log_lik_theta(theta) # negative log-likelihood function.\n",
    "res = minimize(nll_theta, x0=[7, 2])\n",
    "theta_ml = res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the log-likelihood together with the maximum likelihood estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "c = ax.pcolormesh(BB, SS, LL, cmap=cm.coolwarm, shading='auto')\n",
    "plt.plot(theta_ml[0], theta_ml[1], \"kx\")\n",
    "fig.colorbar(c, ax=ax)\n",
    "ax.set_title(f\"Scaled likelihood\");\n",
    "ax.set_xlabel(r\"$\\beta$\");\n",
    "ax.set_ylabel(r\"$\\sigma$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the likelihood function up to a normalization constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIK_SC = np.exp(LL - np.max(LL))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "c = ax.pcolormesh(BB, SS, LIK_SC, cmap=cm.coolwarm, shading='auto')\n",
    "plt.plot(theta_ml[0], theta_ml[1], \"kx\")\n",
    "fig.colorbar(c, ax=ax)\n",
    "ax.set_title(f\"Scaled log-likelihood\");\n",
    "ax.set_xlabel(r\"$\\beta$\");\n",
    "ax.set_ylabel(r\"$\\sigma$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.2: Maximum A Posteriori Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Derive an analytical expression of the posterior $f(\\theta | y)$, up to a multiplicative factor not depending on $\\theta$. \n",
    "\n",
    "Hint: exploit the already-obtained likelihood and the functional form of the Gaussian pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(\\theta | y) = \\frac{P(y | \\theta) f(\\theta)}{P(y)} \\propto \\mathcal{L}(\\theta) p(\\beta) p(\\sigma) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(\\theta | y) = K P(y|\\theta) = \\prod_{i=1}^N \\frac{1}{\\sigma \\sqrt{2\\pi}} \n",
    "e^{-\\frac{1}{2} \\bigg( \\frac{y_i - (\\alpha + \\beta x_i)}{\\sigma} \\bigg )^2 }  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Derive an analytical expression of the log-posterior $\\log f(\\theta | y)$, up to an additive factor not depending on $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "$$\\log f(\\theta | y) = \\log \\frac{P(y | \\theta) f(\\theta)}{P(y)} = \\log P(y | \\theta) + \\log f(\\beta) + \\log f(\\sigma) - \\log P(y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, \n",
    "\n",
    "$$\\log f(\\theta | y) = \\ell(\\theta) -\\frac{1}{2} \\frac{\\beta^2}{100} -\\frac{1}{2} \\frac{\\sigma^2}{100} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Write the unnormalized posterior and log-posterior (up to a multiplicative/additive factor, respectively) as Python functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def log_post_unscaled(beta, sigma):\n",
    "    log_lik_val = log_lik(beta, sigma)\n",
    "    return log_lik_val -0.5*beta**2/100 -0.5* sigma**2/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Compute the maximum a posteriore estimate $\\alpha^{\\rm MAP}, \\beta^{\\rm MAP}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "minus_logpost = lambda theta: -log_post_unscaled(theta[0], theta[1])\n",
    "res = minimize(minus_logpost, x0=[7, 2])\n",
    "theta_map = res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Visualize the MAP estimate together with the unnormalized posterior in 2D. Comment the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "LP_UNSC = np.empty((SIGMA.shape[0], BETA.shape[0]))\n",
    "for i in range(SIGMA.shape[0]):\n",
    "    for j in range(BETA.shape[0]):\n",
    "        LP_UNSC[i, j] = log_post_unscaled(BETA[j], SIGMA[i])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "c = ax.pcolormesh(BETA, SIGMA, POST_UNSC, cmap=cm.coolwarm, shading='auto')\n",
    "plt.plot(theta_map[0], theta_map[1], \"kx\")\n",
    "fig.colorbar(c, ax=ax)\n",
    "ax.set_title(f\"Unnormalized posterior\");\n",
    "ax.set_xlabel(r\"$\\alpha$\");\n",
    "ax.set_ylabel(r\"$\\beta$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.3 Brute-force posterior estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Compute a gridding approximation of the *normalized* posterior, with the correct normalization constant. Explain the passages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We have:\n",
    "    $$ \\tilde f(\\theta | y) = \\mathcal{L}(\\theta) \\exp\\left(-\\frac{1}{2} \n",
    "(\\theta - \\mu)^{\\top} \\Sigma_0^{-1} (\\theta - \\mu)^{\\top} \\right) = Z f(\\theta | y),$$\n",
    "where $Z$ is the to-be-determined normalization constant and it must be chosen such that:\n",
    "$$\\iint f(\\theta | y) d\\alpha\\; d\\beta = 1.$$\n",
    "Thus,\n",
    "$$Z = \\iint f(\\theta | y) d\\alpha\\; d\\beta.$$\n",
    "\n",
    "The integral above is intractable, but a gridding approximation may be used. Using an equi-spaced gridding, a Riemann Sum approximation is:\n",
    "\n",
    "$$Z \\approx \\Delta \\alpha \\Delta \\beta \\sum_i f(\\theta_i | y),$$\n",
    "\n",
    "where $\\Delta \\alpha$ and $\\Delta \\beta$ are the discretization steps of the 2D grid and $\\theta_i$ are the grid points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dalpha = 0.01\n",
    "dbeta = 0.01\n",
    "normalizing_factor = np.sum(POST_UNSC)*dalpha*dbeta\n",
    "POST_SC = POST_UNSC/normalizing_factor\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "c = ax.pcolormesh(AA, BB, POST_SC, cmap=cm.coolwarm, shading='auto')\n",
    "plt.plot(theta_map[0], theta_map[1], \"kx\")\n",
    "fig.colorbar(c, ax=ax)\n",
    "ax.set_title(f\"Normalized posterior\");\n",
    "ax.set_xlabel(r\"$\\alpha$\");\n",
    "ax.set_ylabel(r\"$\\beta$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Using the grid-based approximation of the posterior, compute the posterior mean of $\\alpha$ and $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "By definition, we have:\n",
    "\n",
    "$$E[\\theta] = \\iint \\theta p(\\theta | y) d\\alpha\\; d\\beta.$$\n",
    "\n",
    "Using the grid-based approximation above:\n",
    "\n",
    "$$E[\\theta] = \\Delta \\alpha \\Delta \\beta \\sum \\theta_i p(\\theta_i | y).$$\n",
    "\n",
    "Software implementation below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "a_mean = np.sum(AA*POST_SC)*dalpha*dbeta\n",
    "b_mean = np.sum(BB*POST_SC)*dalpha*dbeta\n",
    "a_mean, b_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is (yet another!) meaningful point estimate of $\\theta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.4 Monte-carlo estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Obtain a sample-based approximation of the posterior $f(\\theta | y)$ by implementing the Metropolis algorithm from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def p_ratio_fun(alpha_propose, beta_propose, alpha_previous, beta_previous):\n",
    "    log_p_previous = log_post_unscaled(alpha_previous, beta_previous)\n",
    "    log_p_propose = log_post_unscaled(alpha_propose, beta_propose)\n",
    "    log_p_ratio = log_p_propose - log_p_previous # log(p_prop/p_prev) = log(p_prop) - log(p_prev)\n",
    "    p_ratio = np.exp(log_p_ratio)\n",
    "    return p_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "p_ratio_fun(alpha_propose = 1.89, alpha_previous = 0.374, beta_propose = 24.76, beta_previous = 20.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "p_ratio_fun(alpha_propose = 0.374, alpha_previous = 1.89, beta_propose = 20.04, beta_previous = 24.76)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let us run a Metropolis algorithm to sample from the posterior. The `p_ratio_fun` function is all we need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "N = 100_000 # number of Metropolis steps\n",
    "alpha_0 = mu_alpha # initial value for alpha\n",
    "beta_0 = mu_beta # initial value for alpha\n",
    "\n",
    "alpha_step = alpha_0\n",
    "beta_step = beta_0\n",
    "sigma_prop_alpha = 1.0\n",
    "sigma_prop_beta = 5.0\n",
    "\n",
    "alphas = []\n",
    "betas = []\n",
    "for idx in range(N):\n",
    "    alphas.append(alpha_step)\n",
    "    betas.append(beta_step)\n",
    "\n",
    "    alpha_prop = alpha_step + sigma_prop_alpha * np.random.randn()\n",
    "    beta_prop = beta_step + sigma_prop_beta * np.random.randn()\n",
    "  \n",
    "    p_ratio = p_ratio_fun(alpha_prop, beta_prop, alpha_step, beta_step)\n",
    "    accept_prob = np.minimum(1.0, p_ratio)\n",
    "    accept = (np.random.rand() < accept_prob)\n",
    "    \n",
    "    if accept:\n",
    "        alpha_step = alpha_prop\n",
    "        beta_step = beta_prop\n",
    "\n",
    "alphas = np.stack(alphas)\n",
    "betas = np.stack(betas)\n",
    "thetas = np.c_[alphas, betas]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compare the Metropolis samples with the gridding-based approximation of the posterior distribution $f(\\theta | y)$ and comment the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n",
    "ax[0].hist2d(x=thetas[:, 0], y=thetas[:, 1], bins=100, cmap=plt.cm.BuPu)\n",
    "ax[0].set_xlim([-4, 10]);\n",
    "ax[0].set_ylim([-10, 40]);\n",
    "ax[0].contour(AA, BB, POST_SC); #, levels=[5, 15,  95]); # levels=[5, 15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "c = ax[1].pcolormesh(AA, BB, POST_SC, cmap=cm.coolwarm, shading='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(thetas[10_000:, :], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.cov(thetas.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.sum(AA*POST_SC)*dalpha*dbeta, np.sum(BB*POST_SC)*dalpha*dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(thetas[:,0])#px.scatter(thetas[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(thetas[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Obtain a sample-based approximation of the posterior $f(\\theta | y)$ using pymc3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as bioassay:\n",
    "    alpha = pm.Normal(\"alpha\", mu=mu_alpha, sigma=sigma_alpha)\n",
    "    beta = pm.Normal(\"beta\", mu=mu_beta, sigma=sigma_beta)\n",
    "    p = pm.Deterministic(\"p\", pm.math.sigmoid(alpha + beta*x))\n",
    "    y_var = pm.Binomial(\"y_var\", n=n, p=p, observed=y)\n",
    "    trace=pm.sample(10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Comment the results obtained with pymc3 and compare with previous results (gridding and Metropolis from scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with bioassay:\n",
    "    display(az.summary(trace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "az.plot_trace(trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "az.plot_posterior(trace, var_names=[\"alpha\", \"beta\"]);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
