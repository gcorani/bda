{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from matplotlib import cm\n",
    "import arviz as az\n",
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assignment 1: Toxicity bioassay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Problem and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A **bioassay** is biochemical test to estimate the potency of a sample compound. A typical bioassay involves a stimulus (ex. drugs) applied to a subject (ex. animals, tissues, plants). The corresponding response (ex. death) of the subject is thereby triggered and measured (Wikipedia).\n",
    "\n",
    "The following bioassay taken from Racine et al. (1986) is meant to evaluate the toxicity of a drug on animals. The effect of the drug is evaluated at $N=4$ dose levels. Each dose level $x_i$ is administered to a batch of $n_i$ animals. The number of deaths $y_i$ is the observed response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "| |Dose $x_i$ (log g/ml) | Number of animals $n_i$ | Number of deaths $y_i$ |\n",
    "| ---|--- | --- | --- |\n",
    "| 1|-0.86 | 5 | 0 |\n",
    "| 2|-0.30 | 5 | 1 |\n",
    "| 3|-0.05 | 5 | 3 |\n",
    "| 4|0.73  | 5 | 5 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For instance, the dose level $x_3=-0.05$ has been administered at $n_3=5$ animals. Out of the 5 animals, $y_3=3$ died. <br/>\n",
    "Note that the dose $x_i$ is measured on a logarithmic scale. Thus, negative concentration levels are present.\n",
    "\n",
    "We define for convenience (both as math symbols and as Python objects) the vectors $x$, $n$, and $y$ containing the quantities of the corresponding columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([-0.86, -0.30, -0.05, 0.73]) # dose levels\n",
    "n = np.array([5., 5., 5., 5.]) # number of subjects per dose level\n",
    "y = np.array([0, 1, 3, 5]) # number of deaths per dose level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Modeling assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For the probabilistic model, we make the following assumptions:\n",
    "\n",
    "1. The outcome of the $n_i$ animals within each group $i$ are *independent*. Each animal in the group has probability $p_i$ of death.\n",
    "\n",
    "2. The probability of death $p_i$ depends on the dose $x_i$ as follows:\n",
    "    $$p_i = \\rm{sigm}(\\alpha + \\beta x_i),$$ \n",
    "    where \n",
    "    \\begin{align*}\n",
    "    \\rm{sigm}(z) = \\frac{1}{1 + e^{-z}}.\n",
    "    \\end{align*}\n",
    "3. The prior probability of the parameters \n",
    "$\\theta = \\begin{bmatrix}\n",
    "\\alpha \\\\\n",
    "\\beta\n",
    "\\end{bmatrix}$\n",
    "is Gaussian: \n",
    "\\begin{align}\n",
    "\\alpha &\\sim \\mathcal{N}(\\mu_\\alpha, \\sigma^2_\\alpha), \\qquad \\mu_\\alpha = 0, \\sigma_\\alpha=2\\\\\n",
    "\\beta &\\sim \\mathcal{N}(\\mu_\\beta, \\sigma^2_\\beta), \\qquad \\mu_\\beta=10, \\sigma_\\beta=10.\n",
    "\\end{align}\n",
    "4. The outcomes in the four groups are independent of each other, given $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.1: Probabilistic model\n",
    "\n",
    "* Derive and comment the full probabilistic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Putting together the probabilistic assumptions 1-3, we obtain:\n",
    "\n",
    "\\begin{align*}\n",
    "y_i | p_i &\\sim  \\mathrm{Binomial}(n_i, \\rm{sigm}(\\alpha + \\beta x_i))\\\\\n",
    "%p_i &= \\rm{sigm}(\\alpha + \\beta x_i) \\\\\n",
    "\\alpha &\\sim \\mathcal{N}(0, 4)\\\\\n",
    "\\beta &\\sim \\mathcal{N}(10, 100).\n",
    "\\end{align*}\n",
    "\n",
    "Furthermore, according to assumption 4:\n",
    "\n",
    "$$P(y|\\theta) = \\prod_i P(y_i|\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.2: Maximum Likelihood estimation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Derive an analytical expression of the likelihood function $\\mathcal{L}(\\theta) = P(y|\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The likelihood function $\\mathcal{L}(\\theta)$ is $P(y|\\theta)$, seen as a function of $\\theta$, with $y$ fixed to the observed outcome. <br/>Since the individual observations $y_i$ are independent, we have:\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = P(y|\\theta) = \\prod_{i=1}^N {{n_i}\\choose{y_i}} \\mathrm{sigm}(\\alpha + \\beta x_i)^{y_i} \\cdot (1- \\mathrm{sigm}(\\alpha + \\beta x_i))^{n_i - y_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Write a Python function corresponding to the likelihood function $\\mathcal{L}(\\theta)$. Ignore multiplicative factors which do not depend on $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def lik(alpha, beta):\n",
    "    pass\n",
    "    # ... TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Likelihood of the 4 observations (neglecting the multiplicative factor).\n",
    "# The overall likelihood is the product of all terms.\n",
    "\n",
    "def lik(alpha, beta):\n",
    "    alpha = np.atleast_1d(alpha)[..., np.newaxis]\n",
    "    beta  = np.atleast_1d(beta)[..., np.newaxis]\n",
    "    gamma = sigmoid(alpha+beta*x)\n",
    "    lik = gamma**y * (1-gamma)**(n-y)\n",
    "    return np.prod(lik, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Visualize the likelihood function in 2D and comment the obtained figure. \n",
    "\n",
    "   Hints:\n",
    "    * you may use the `pcolormesh` function of `matplotlib`\n",
    "    * appropriate ranges for $\\alpha$ and $\\beta$ are $[-4, 8]$ and $[-10, 40]$, respectively\n",
    "    * an appropriate step size for both $\\alpha$ and $\\beta$ is 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dalpha = 0.01\n",
    "dbeta = 0.01\n",
    "ALPHA = np.arange(-4, 8, dalpha)\n",
    "BETA = np.arange(-10, 40, dbeta)\n",
    "AA, BB = np.meshgrid(ALPHA, BETA, indexing='xy')\n",
    "AABB = np.stack((AA, BB), axis=-1)\n",
    "LL = lik(AA, BB)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "c = ax.pcolormesh(AA, BB, LL, cmap=cm.coolwarm, shading='auto')\n",
    "#plt.plot(theta_ml[0], theta_ml[1], \"kx\")\n",
    "fig.colorbar(c, ax=ax)\n",
    "ax.set_title(f\"Likelihood\");\n",
    "ax.set_xlabel(r\"$\\alpha$\");\n",
    "ax.set_ylabel(r\"$\\beta$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Parameters $\\alpha$ and $\\beta$ are positively correlated, the maximum likelihood estimate is around (1, 8), the numerical scale is small (0 ... 0.0025), but not extremely critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Derive an analytical expression of the log-likelihood function $\\ell(\\theta)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "In this case, the likelihood is numerically well-posed (not too many samples, not too many multiplications). The scale 0-0.05 is not too bad! In general, it is better to work with logarithms. Let us compute the *log-likelihood* $\\ell(\\theta)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "$$\\ell(\\theta) = \\log \\mathcal{L}(\\theta) = \\sum_i {{n_i}\\choose{y_i}} + \\sum_i y_i \\log \\mathrm{sigm}(\\alpha + \\beta x_i) +  (n_i - y_i) \\log (1- \\mathrm{sigm}(\\alpha + \\beta x_i)).$$\n",
    "\n",
    "The constant term $\\sum_i {{n_i}\\choose{y_i}}$ may be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Write a Python function corresponding to the log-likelihood function $\\ell(\\theta)$ (possibly up to an additive factor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def log_lik(alpha, beta):\n",
    "    alpha = np.atleast_1d(alpha)[..., np.newaxis] # useful to handle grid data\n",
    "    beta  = np.atleast_1d(beta)[..., np.newaxis] # useful to handle grid data\n",
    "    gamma = sigmoid(alpha+beta*x)\n",
    "    #log_lik = y*np.log(gamma) + (n-y)*np.log(1-gamma)\n",
    "    # nan_to_num handles the multiplication 0*np.inf and set it to 0, as required in our case...\n",
    "    log_lik = np.nan_to_num(y*np.log(gamma), nan=0) + np.nan_to_num((n-y)*np.log(1-gamma), nan=0)\n",
    "    return np.sum(log_lik, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visualize the log-likelihood function in 2D and comment the obtained figure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "LOG_LL = log_lik(AA, BB)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "c = ax.pcolormesh(AA, BB, LOG_LL, cmap=cm.coolwarm, shading='auto')\n",
    "#plt.plot(theta_ml[0], theta_ml[1], \"kx\")\n",
    "fig.colorbar(c, ax=ax)\n",
    "ax.set_title(f\"Likelihood\");\n",
    "ax.set_xlabel(r\"$\\alpha$\");\n",
    "ax.set_ylabel(r\"$\\beta$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Compute the maximum likelihood estimate $\\alpha^{\\rm ml}, \\beta^{\\rm ml}$ of the parameters $\\alpha, \\beta$ through numerical optimizations. \n",
    "\n",
    "    Hints:\n",
    "     * You may use the Python function `scipy.optimize.minimize`. \n",
    "     * You may look at the figures above to define a good starting point for optimization \n",
    "     * You may either minimize the likelihood or the log-likelihood. What is your choice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "log_lik_theta = lambda theta: log_lik(theta[0], theta[1])\n",
    "nll_theta = lambda theta: -log_lik_theta(theta) # negative log-likelihood function.\n",
    "res = minimize(nll_theta, x0=[1, 8])\n",
    "theta_ml = res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Visualize the likelihood function in 2D together with the ML estimate. Comment the obtained figure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "c = ax.pcolormesh(AA, BB, LL, cmap=cm.coolwarm, shading='auto')\n",
    "plt.plot(theta_ml[0], theta_ml[1], \"kx\")\n",
    "fig.colorbar(c, ax=ax)\n",
    "ax.set_title(f\"Likelihood\");\n",
    "ax.set_xlabel(r\"$\\alpha$\");\n",
    "ax.set_ylabel(r\"$\\beta$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.2: Maximum A Posteriori Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Derive an analytical expression of the posterior $f(\\theta | y)$, up to a multiplicative factor not depending on $\\theta$. \n",
    "\n",
    "Hint: exploit the already-obtained likelihood and the functional form of the Gaussian pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "$$f(\\theta | y) = \\frac{P(y | \\theta) f(\\theta)}{P(y)} \\propto \\mathcal{L}(\\theta)\n",
    "\\exp\\left(-\\frac{1}{2} \\frac{(\\alpha - \\mu_\\alpha)^2}{\\sigma^2_\\alpha} \\right ) \n",
    "\\exp\\left(-\\frac{1}{2} \\frac{(\\beta - \\mu_\\beta)^2}{\\sigma^2_\\beta} \\right ). $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Derive an analytical expression of the log-posterior $\\log f(\\theta | y)$, up to an additive factor not depending on $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "$$\\log f(\\theta | y) = \\log \\frac{P(y | \\theta) f(\\theta)}{P(y)} = \\log P(y | \\theta) + \\log f(\\theta) - \\log P(y) = \\ell(\\theta) - \\frac{1}{2} (\\theta - \\mu)^{\\top} \\Sigma_0^{-1} (\\theta - \\mu)^{\\top} + \\rm{cnst}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Write the unnormalized posterior and log-posterior (up to a multiplicative/additive factor, respectively) as Python functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mu_alpha = 0\n",
    "sigma_alpha = 2\n",
    "mu_beta = 10\n",
    "sigma_beta = 10\n",
    "\n",
    "prior_alpha = stats.norm(loc=mu_alpha, scale=sigma_alpha)\n",
    "prior_beta = stats.norm(loc=mu_beta, scale=sigma_beta)\n",
    "\n",
    "#post_unscaled = lambda theta: lik_theta(theta)*prior_fun.pdf(theta)\n",
    "#log_post_unscaled = lambda theta: log_lik_theta(theta) + prior_fun.logpdf(theta)\n",
    "\n",
    "def post_unscaled(alpha, beta):\n",
    "    lik_val = lik(alpha, beta)\n",
    "    #return lik_val * prior_alpha.pdf(alpha) * prior_beta.pdf(beta)    \n",
    "    return lik_val * np.exp(-0.5 * (alpha - mu_alpha)**2/sigma_alpha**2) * \\\n",
    "        np.exp(-0.5 * (beta - mu_beta)**2/sigma_beta**2)\n",
    "\n",
    "def log_post_unscaled(alpha, beta):\n",
    "    log_lik_val = log_lik(alpha, beta)\n",
    "    #return lik_val * prior_alpha.pdf(alpha) * prior_beta.pdf(beta)    \n",
    "    return log_lik_val -0.5 * (alpha - mu_alpha)**2/sigma_alpha**2 - 0.5 * (beta - mu_beta)**2/sigma_beta**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Compute the maximum a posteriore estimate $\\alpha^{\\rm MAP}, \\beta^{\\rm MAP}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "minus_logpost = lambda theta: -log_post_unscaled(theta[0], theta[1])\n",
    "res = minimize(minus_logpost, x0=[0, 10])\n",
    "theta_map = res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Visualize the MAP estimate together with the unnormalized posterior in 2D. Comment the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#def p_prior_unscaled(alpha, beta):\n",
    "#    return np.exp(-0.5 * (alpha - mu_alpha)**2/sigma_alpha**2) * \\\n",
    "#        np.exp(-0.5 * (beta - mu_beta)**2/sigma_beta**2)\n",
    "#\n",
    "#PP = p_prior_unscaled(AA, BB) # Prior\n",
    "#POST_UNSC = LL * PP\n",
    "POST_UNSC = post_unscaled(AA, BB)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "c = ax.pcolormesh(AA, BB, POST_UNSC, cmap=cm.coolwarm, shading='auto')\n",
    "plt.plot(theta_map[0], theta_map[1], \"kx\")\n",
    "fig.colorbar(c, ax=ax)\n",
    "ax.set_title(f\"Unnormalized posterior\");\n",
    "ax.set_xlabel(r\"$\\alpha$\");\n",
    "ax.set_ylabel(r\"$\\beta$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.3 Brute-force posterior estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Compute a gridding approximation of the *normalized* posterior, with the correct normalization constant. Explain the passages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We have:\n",
    "    $$ \\tilde f(\\theta | y) = \\mathcal{L}(\\theta) \\exp\\left(-\\frac{1}{2} \n",
    "(\\theta - \\mu)^{\\top} \\Sigma_0^{-1} (\\theta - \\mu)^{\\top} \\right) = Z f(\\theta | y),$$\n",
    "where $Z$ is the to-be-determined normalization constant and it must be chosen such that:\n",
    "$$\\iint f(\\theta | y) d\\alpha\\; d\\beta = 1.$$\n",
    "Thus,\n",
    "$$Z = \\iint f(\\theta | y) d\\alpha\\; d\\beta.$$\n",
    "\n",
    "The integral above is intractable, but a gridding approximation may be used. Using an equi-spaced gridding, a Riemann Sum approximation is:\n",
    "\n",
    "$$Z \\approx \\Delta \\alpha \\Delta \\beta \\sum_i f(\\theta_i | y),$$\n",
    "\n",
    "where $\\Delta \\alpha$ and $\\Delta \\beta$ are the discretization steps of the 2D grid and $\\theta_i$ are the grid points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dalpha = 0.01\n",
    "dbeta = 0.01\n",
    "normalizing_factor = np.sum(POST_UNSC)*dalpha*dbeta\n",
    "POST_SC = POST_UNSC/normalizing_factor\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "c = ax.pcolormesh(AA, BB, POST_SC, cmap=cm.coolwarm, shading='auto')\n",
    "plt.plot(theta_map[0], theta_map[1], \"kx\")\n",
    "fig.colorbar(c, ax=ax)\n",
    "ax.set_title(f\"Normalized posterior\");\n",
    "ax.set_xlabel(r\"$\\alpha$\");\n",
    "ax.set_ylabel(r\"$\\beta$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Using the grid-based approximation of the posterior, compute the posterior mean of $\\alpha$ and $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "By definition, we have:\n",
    "\n",
    "$$E[\\theta] = \\iint \\theta p(\\theta | y) d\\alpha\\; d\\beta.$$\n",
    "\n",
    "Using the grid-based approximation above:\n",
    "\n",
    "$$E[\\theta] = \\Delta \\alpha \\Delta \\beta \\sum \\theta_i p(\\theta_i | y).$$\n",
    "\n",
    "Software implementation below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "a_mean = np.sum(AA*POST_SC)*dalpha*dbeta\n",
    "b_mean = np.sum(BB*POST_SC)*dalpha*dbeta\n",
    "a_mean, b_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is (yet another!) meaningful point estimate of $\\theta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.4 Monte-carlo estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Obtain a sample-based approximation of the posterior $f(\\theta | y)$ by implementing the Metropolis algorithm from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def p_ratio_fun(alpha_propose, beta_propose, alpha_previous, beta_previous):\n",
    "    log_p_previous = log_post_unscaled(alpha_previous, beta_previous)\n",
    "    log_p_propose = log_post_unscaled(alpha_propose, beta_propose)\n",
    "    log_p_ratio = log_p_propose - log_p_previous # log(p_prop/p_prev) = log(p_prop) - log(p_prev)\n",
    "    p_ratio = np.exp(log_p_ratio)\n",
    "    return p_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "p_ratio_fun(alpha_propose = 1.89, alpha_previous = 0.374, beta_propose = 24.76, beta_previous = 20.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "p_ratio_fun(alpha_propose = 0.374, alpha_previous = 1.89, beta_propose = 20.04, beta_previous = 24.76)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let us run a Metropolis algorithm to sample from the posterior. The `p_ratio_fun` function is all we need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "N = 100_000 # number of Metropolis steps\n",
    "alpha_0 = mu_alpha # initial value for alpha\n",
    "beta_0 = mu_beta # initial value for alpha\n",
    "\n",
    "alpha_step = alpha_0\n",
    "beta_step = beta_0\n",
    "sigma_prop_alpha = 1.0\n",
    "sigma_prop_beta = 5.0\n",
    "\n",
    "alphas = []\n",
    "betas = []\n",
    "for idx in range(N):\n",
    "    alphas.append(alpha_step)\n",
    "    betas.append(beta_step)\n",
    "\n",
    "    alpha_prop = alpha_step + sigma_prop_alpha * np.random.randn()\n",
    "    beta_prop = beta_step + sigma_prop_beta * np.random.randn()\n",
    "  \n",
    "    p_ratio = p_ratio_fun(alpha_prop, beta_prop, alpha_step, beta_step)\n",
    "    accept_prob = np.minimum(1.0, p_ratio)\n",
    "    accept = (np.random.rand() < accept_prob)\n",
    "    \n",
    "    if accept:\n",
    "        alpha_step = alpha_prop\n",
    "        beta_step = beta_prop\n",
    "\n",
    "alphas = np.stack(alphas)\n",
    "betas = np.stack(betas)\n",
    "thetas = np.c_[alphas, betas]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compare the Metropolis samples with the gridding-based approximation of the posterior distribution $f(\\theta | y)$ and comment the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n",
    "ax[0].hist2d(x=thetas[:, 0], y=thetas[:, 1], bins=100, cmap=plt.cm.BuPu)\n",
    "ax[0].set_xlim([-4, 10]);\n",
    "ax[0].set_ylim([-10, 40]);\n",
    "ax[0].contour(AA, BB, POST_SC); #, levels=[5, 15,  95]); # levels=[5, 15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "c = ax[1].pcolormesh(AA, BB, POST_SC, cmap=cm.coolwarm, shading='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(thetas[10_000:, :], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.cov(thetas.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.sum(AA*POST_SC)*dalpha*dbeta, np.sum(BB*POST_SC)*dalpha*dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(thetas[:,0])#px.scatter(thetas[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(thetas[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Obtain a sample-based approximation of the posterior $f(\\theta | y)$ using pymc3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as bioassay:\n",
    "    alpha = pm.Normal(\"alpha\", mu=mu_alpha, sigma=sigma_alpha)\n",
    "    beta = pm.Normal(\"beta\", mu=mu_beta, sigma=sigma_beta)\n",
    "    p = pm.Deterministic(\"p\", pm.math.sigmoid(alpha + beta*x))\n",
    "    y_var = pm.Binomial(\"y_var\", n=n, p=p, observed=y)\n",
    "    trace=pm.sample(10_000, return_inferencedata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Comment the results obtained with pymc3 and compare with previous results (gridding and Metropolis from scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with bioassay:\n",
    "    display(az.summary(trace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "az.plot_trace(trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "az.plot_posterior(trace, var_names=[\"alpha\", \"beta\"]);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
