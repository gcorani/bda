{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Quarto Presentations\"\n",
        "subtitle: \"Create beautiful interactive slide decks with Reveal.js\"\n",
        "format:\n",
        "  revealjs: \n",
        "    slide-number: true\n",
        "    chalkboard: \n",
        "      buttons: false\n",
        "    preview-links: auto\n",
        "    logo: images/quarto.png\n",
        "    css: styles.css\n",
        "resources:\n",
        "  - demo.pdf\n",
        "---"
      ],
      "id": "69815ab2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import pymc as pm\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import arviz as az\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "az.style.use('arviz-darkgrid')\n",
        "np.random.seed(44)"
      ],
      "id": "5043286b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        }
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set_theme()\n",
        "plt.rcParams['font.size'] = 15\n",
        "plt.rcParams['legend.fontsize'] = 'medium'\n",
        "plt.rcParams.update({\n",
        "    \"figure.figsize\": [7, 3],\n",
        "    'figure.facecolor': '#fffff8',\n",
        "    'axes.facecolor': '#fffff8',\n",
        "    'figure.constrained_layout.use': True,\n",
        "    'font.size': 14.0,\n",
        "    'hist.bins': 'auto',\n",
        "    'lines.linewidth': 3.0,\n",
        "    'lines.markeredgewidth': 2.0,\n",
        "    'lines.markerfacecolor': 'none',\n",
        "    'lines.markersize': 8.0, \n",
        "})\n",
        "sns.set(rc={'figure.figsize':(7,3)})"
      ],
      "id": "f9b41a23",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Giorgio Corani <br/>\n",
        "\n",
        "\n",
        "*Bayesian Data Analysis and Probabilistic Programming*\n",
        "<br/>\n",
        "<br/>\n",
        "``giorgio.corani@supsi.ch``\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Based on.. \n",
        "\n",
        "\n",
        "*   Chapter 3 of O. Martin, *Bayesian Analysis with Python, Second Edition*.\n",
        "\n",
        "\n",
        "* Chapter 8 of *the Bayes rule book* https://www.bayesrulesbook.com/chapter-10.html\n",
        "\n",
        "\n",
        "*  Notebook by G. Corani\n",
        "\n",
        "# Linear regression\n",
        "\n",
        "* We want to predict the value of $Y$ given the observation of $X$.\n",
        "\n",
        "\n",
        "* $X$ and $Y$ are random variables:\n",
        "    *  $Y$ is the *dependent* (or  *response*) variable\n",
        "    *  $X$  is the *independent*  variable   (or *explanatory variable*   or *covariate*)\n",
        "\n",
        "# Simple linear regression: a single explanatory variable\n",
        "\n",
        "$$Y = \\alpha +  \\beta X  + \\epsilon$$\n",
        "\n",
        "\n",
        "* $\\alpha$  (*intercept*):  predicted value of $Y$ for $X$ = 0; can be   seen as  a constant which calibrates the shift along the y-axis.\n",
        "\n",
        "\n",
        "* $\\beta$ (*slope*):  predicted change in $Y$  for a unit change in  $X$. \n",
        "\n",
        "\n",
        "* $\\epsilon$ is a noise which the  scatters the  observations  around the line. \n",
        "\n",
        "# Simple linear regression\n",
        "\n",
        "<img src='img/linreg.png' width=600 align=\"center\" > \n",
        "\n",
        "\n",
        "*  The noise  $\\epsilon$ implies a deviation from the  linear model. It captures anything that may affect $Y$ other than $X$.\n",
        "\n",
        "* We assume  $\\epsilon \\sim N(0, \\sigma^2)$.\n",
        "\n",
        "# The effect of $\\sigma$\n",
        "\n",
        "\n",
        "<img src='img/sigma-effect.png' width=700 align=\"center\" > \n",
        "\n",
        "* $\\sigma$ is the std dev of the  noise $\\epsilon$.\n",
        "\n",
        "* Large $\\sigma$: large variability of the observations around the linear model, weak relationship. \n",
        "\n",
        "* Small $\\sigma$: the observations deviates  little from the  model; strong relationship.\n",
        "\n",
        "# The effect of $\\sigma$\n",
        "\n",
        "\n",
        "<img src='img/sigma-effect.png' width=700 align=\"center\" > \n",
        "\n",
        "\n",
        "* Under the normal assumption:\n",
        "     *  about 68% of the observations  lie  in an interval of $\\pm 1 \\sigma$ around the regression line.\n",
        "    *  how many  observations will lie in an interval of $\\pm 2 \\sigma$ around the regression line?\n",
        "\n",
        "\n",
        "# Multiple linear regression\n",
        "\n",
        "* Linear regression with $k$ explanatory variables:\n",
        "\n",
        "\\begin{align}\n",
        " Y = \\alpha+ \\sum_i β_i X_i +ε,\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "* The coefficients $\\beta_1,…,\\beta_k$ measure the *marginal effects* of each  explanatory variable, i.e. the effect of explanatory variable while keeping fixed all the remanining  ones.\n",
        "\n",
        "# Linear regression is not a causal model\n",
        "\n",
        "Consider the model\n",
        "$$ \\text{earnings} = −26.0 + 0.6 ∗ \\text{height} + \\text{error}$$\n",
        "\n",
        ">To say that “the effect of height on earnings” is 600 is to suggest that, if we were to increase someone’s height by one inch, his or her earnings would increase by an expected amount of $600. But the model has captured instead an  an observational pattern, that taller people in the sample have higher earnings on average.\n",
        "\n",
        "* The correct interpretation is that the average difference in earnings, comparing two groups of people whose height differ by 1 inch, is 0.6.\n",
        "\n",
        "\n",
        "# Probabilistic regression\n",
        "\n",
        "* The prediction account for the uncertainty of the estimated parameters ($\\alpha, \\beta, \\sigma$)\n",
        "\n",
        "* Hierarchical regression: learning related regression models for different sources of data e.g.,  different hospitals applying the same treatment.\n",
        "\n",
        "\n",
        "*  Possibility of adopting a   *robust regression* to deal with outliers.\n",
        "\n",
        "# Probabilistic  regression\n",
        "\n",
        "\n",
        "\\begin{align*}\n",
        "Y &\\sim N (\\mu=\\alpha + X \\beta,  \\sigma) \\\\\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "*  We must specify a prior distribution for each of  parameter: $\\alpha$, $\\beta$, $\\sigma$.\n",
        "\n",
        "# Probabilistic  regression\n",
        "\n",
        "* We  independently specify the prior of each parameter. \n",
        "\n",
        "\n",
        "* Ideally, we define the priors based on background information\n",
        "\n",
        "\n",
        "* If this is not possible, it is recommended setting  *weakly informative*  which define the order of magnitude of the parameters, after having scaled  the  data.\n",
        "    \n",
        "\n",
        "# Using background information \n",
        "\n",
        "Define  a regression model for a bike sharing company, based on:\n",
        "\n",
        "\n",
        "*    For every one degree increase in temperature, rides  increases by about 100 rides; the average increase is between  20 and 180.\n",
        "\n",
        "\n",
        "*    On an average temperature day (65 - 70 degrees), there are around 5000 riders, though this varies between 3000 and 7000.\n",
        "\n",
        "\n",
        "*    At any given temperature, daily ridership varies with a moderate standard deviation of 1250 rides.\n",
        "\n",
        "  \n",
        "\n",
        "# Prior for the slope $\\beta$\n",
        "\n",
        "\n",
        "> For every one degree increase in temperature, ridership typically increases by 100 rides; the average increase is between  20 and 180.\n",
        "\n",
        "$$ \\beta  \\sim N(100, 40)  $$\n",
        "\n",
        "\n",
        "# Prior for the intercept $\\alpha$\n",
        "\n",
        "* We have no  information about the intercept, i.e., the average number of rides when the temperature is 0).\n",
        "\n",
        "\n",
        "* We know however the expected number of rides given an average temperature.\n",
        "\n",
        "\n",
        ">  On an average temperature day, there are around 5000 riders, though this  could vary between 3000 and 7000.\n",
        "   \n",
        "\n",
        "\n",
        "*  To use this information, we *center* the temperature $X$:\n",
        "\n",
        "\\begin{align*}\n",
        "X_c = X - \\bar{x}\n",
        "\\end{align*}\n",
        "\n",
        "where $\\bar{x}$ is the average temperature in the sample.\n",
        "\n",
        "# Regression with centered data\n",
        "   \n",
        "\\begin{align*}\n",
        "Y & = \\alpha + \\beta X  \\\\\n",
        "Y & = \\alpha + \\beta \\underbrace{(X - \\bar{x})}_{X_c} + \\beta \\bar{x}   \\\\\n",
        "Y & = \\alpha + \\beta X_c + \\beta \\bar{x}   \\\\\n",
        "Y & =  \\underbrace{\\alpha +  \\beta \\bar{x}}_{\\alpha_c} + \\beta X_c   \\\\\n",
        "Y & = \\alpha_c + \\beta X_c    \\\\\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "* The intercept with centered data $\\alpha_c$ is the expected value of $Y$ when $X_c = 0$ i.e., when $X = \\bar{x}$. \n",
        "\n",
        "* $\\alpha_c$ is the average value of $Y$ when $X$ is at its mean.\n",
        "\n",
        "* The meaning of $\\beta$ does not change if we center the explanatory variables.\n",
        "\n",
        "# Centering yields also better sampling\n",
        "\n",
        "* Centering the explanatory variables helps to numerically sample the posterior distribution of the parameters.\n",
        "\n",
        "\n",
        "\n",
        "* Sampling the posterior of the regression model on the raw data might be  slow and inefficient (low ESS, equivalent sample size).\n",
        "\n",
        "# Prior for $\\alpha_c$\n",
        "\n",
        "\n",
        ">  On an average temperature day (65 - 70 degrees), there are around 5000 riders, though this  could vary between 3000 and 7000.\n",
        "   \n",
        "\n",
        "* $\\alpha_c \\sim N(5000, 1000)$.\n"
      ],
      "id": "866feccd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "# Prior for sigma\n",
        "\n",
        "# > At any given temperature, daily ridership will tend to vary with a moderate standard deviation of 1250 rides.\n",
        "\n",
        "# We can  set:\n",
        "# sigma ~  HalfNormal (1850), as the median of this distribution is around 1250.\n",
        "\n",
        "from scipy.stats import halfnorm \n",
        "pd.DataFrame(halfnorm.rvs(scale=1850, size=10000)).describe()"
      ],
      "id": "5ea32df2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The resulting model: bike rides as a function of temperature\n",
        "\n",
        "\n",
        "\\begin{align*}\n",
        "\\alpha_c & \\sim  N (5000, 1000) \\\\\n",
        "\\beta & \\sim N(100, 40) \\\\\n",
        "\\sigma & \\sim \\text{HalfNormal} (1850)\\\\ \n",
        "Y & \\sim N( \\alpha_c + \\beta X_c, \\sigma)\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "* To  fit the model it is necessary to use the centered covariate $X_c = X - \\bar{x}$, where  $\\bar{x}$ is the sample mean.\n",
        "\n",
        "# Linear regression in PyMC3\n"
      ],
      "id": "5987b4c6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#load and check the data\n",
        "#500 rows of data\n",
        "bike_data = pd.read_csv('data/bikes.csv')\n",
        "rides = bike_data[\"rides\"]\n",
        "temperature = bike_data[\"temp_actual\"]\n",
        "\n",
        "bike_data"
      ],
      "id": "08583295",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "#centered covariate\n",
        "temperature_c = temperature - temperature.mean()\n",
        "\n",
        "with pm.Model() as reg_model:\n",
        "    alpha_c  = pm.Normal ('alpha_c', mu=5000,  sigma= 1000)\n",
        "    beta     = pm.Normal ('beta',   mu=100,     sigma= 40)\n",
        "    sigma    = pm.HalfNormal ('sigma', sigma=1850 )\n",
        "        \n",
        "    y_pred = pm.Normal('y_pred', mu=alpha_c + beta * temperature_c, sigma=sigma, observed=rides)\n",
        "    \n",
        "    trace = pm.sample(return_inferencedata=True)"
      ],
      "id": "c37b6242",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "#posterior values are reasonably close to our prior guess. Prior information was reliable.\n",
        "az.summary(trace)"
      ],
      "id": "1301f7b6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "az.plot_trace(trace);"
      ],
      "id": "349a6f66",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Your turn: define a model of probabilistic regression \n",
        "\n",
        "\n",
        "*    Ridership tends to decrease as humidity increases: for every one percentage point increase in humidity level, ridership tends to decrease by 10 rides, though the decrease could vary between 0 and 20.\n",
        "\n",
        "\n",
        "*    On an average humidity day, there are typically around 5000 riders, the actual number varying between 1000 and 9000.\n",
        "\n",
        "\n",
        " *  Ridership is only weakly related to humidity. At any given humidity, ridership will tend to vary with a large standard deviation of 2000 rides.\n",
        "\n",
        "# Comprehension question\n",
        "\n",
        "* Why is a Normal prior a reasonable choice for $\\alpha$ and $\\beta$ ?\n",
        "* Why isn’t a Normal prior a reasonable choice for $\\sigma$ ?\n",
        "\n",
        "# Priors and likelihood\n",
        "\n",
        "# Priors\n",
        "\n",
        "* We assume independence of the parameters.\n",
        "\n",
        "\n",
        "* In any case, the  joint prior  of the parameters is thus the product of their marginal pdfs:\n",
        "\n",
        "\\begin{align}\n",
        "f(\\alpha,\\beta,\\sigma)=f(\\alpha)f(\\beta)f(\\sigma)\n",
        "\\end{align}\n",
        "\n",
        "# Likelihood  \n",
        "\n",
        "\n",
        "* It is the probability of observing our data $\\mathbf{y}$ given a specific value of the parameters $\\alpha,\\beta,\\sigma$\n",
        "\n",
        "\n",
        "* Assuming   independence of the observations, we multiply the normal density of each observation:\n",
        "\n",
        "$$ f(\\mathbf{y}|\\alpha,\\beta,\\sigma)=∏_{i=1}^n N( y_i|  \\alpha + \\beta x_i, \\sigma) $$\n",
        "\n",
        "# Your turn\n",
        "\n",
        "* Given $\\alpha$=2, $\\beta=10$, $\\sigma$=1\n",
        "\n",
        "* Consider the observations: \n",
        "\n",
        "|$X$|$Y$|\n",
        "|:---:|:---:|       \n",
        "| 1 | 11 |\n",
        "| 5 | 50 |\n",
        "\n",
        "\n",
        "* Compute the likelihood function.\n",
        "\n",
        "# Posterior\n",
        "\n",
        "The three-dimensional posterior distribution is computed as:\n",
        "\n",
        "\\begin{align}\n",
        "\\underbrace{\n",
        "f ( (\\alpha,\\beta,\\sigma)| \\mathbf{y})  \n",
        "}_{\\text{posterior}}\n",
        "\\propto \\text{prior} \\cdot \\text{likelihood} & =\n",
        "f(\\alpha)f(\\beta)f(\\sigma) \\left[ ∏_i^n f(y_i|\\alpha,\\beta,\\sigma) \\right] \n",
        "\\end{align}\n",
        "\n",
        "* That is, we shall evaluate  prior and  likelihood for each possible value of the parameters and eventually normalize the result. We did  this using Pymc.\n",
        "\n",
        "# Visualizing the model\n",
        "\n",
        "* After having sampled the posterior, we have many posterior samples of type  $<\\alpha_s,\\beta_s, \\sigma_s>$ with $s= 1,2, ....4000$.\n",
        "\n",
        "* For visualizing the model we ignore $\\sigma$.\n",
        "\n",
        "* Each sample  $<\\alpha_s,\\beta_s>$ yields a plausible line.\n",
        "\n",
        "*  We can visualize the average line using the posterior mean of $\\alpha_c$ and $\\beta$:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{\\alpha}_c & =  \\frac{1}{S} \\sum_s \\alpha_{c_s} \\\\\n",
        "\\hat{\\beta} & = \\frac{1}{S} \\sum_s \\beta_s \\\\\n",
        "\\end{align}\n",
        "\n",
        "* We are using the hat to denote the posterior means, obtained by averaring over  the samples of the trace.\n"
      ],
      "id": "dcb19fc4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "# Extracting the posterior mean of the parameters\n",
        "\n",
        "# to work with samples, we first  extract the dataset of samples.\n",
        "idata = az.extract_dataset(trace)\n",
        "\n",
        "#idata.alpha_c.values access the samples of alpha_c, etc\n",
        "alpha_c_hat =  idata.alpha_c.values.mean() #3486\n",
        "beta_hat    =  idata.beta.values.mean() # 89.5\n",
        "\n",
        "\n",
        "# The posterior mean relation is thus:  y (rides) = 3486 + 89.5 * temperature_c"
      ],
      "id": "1fb5ff11",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "#We plot the average line (based on the posterior mean of alpha_c and beta) and  other plausible lines \n",
        "#using the sampled values of alpha_c and beta.\n",
        "# this shows the uncertainty in the estimated parameters\n",
        "#To quickly compute the plot, we pick one sample every 10.\n",
        "\n",
        "chosen_samples = range(0, len(idata.alpha_c.values),10)\n",
        "\n",
        "#The uncertainty is shown by plotting a different line for each sample of (alpha_c, beta).\n",
        "for i in range(len(chosen_samples)):\n",
        "    plt.plot(temperature_c,  idata.alpha_c.values[i] + idata.beta.values[i] * temperature_c, c='gray', alpha=0.5)\n",
        "\n",
        "#Mean value of the regression line\n",
        "alpha_hat =  idata.alpha_c.values.mean()\n",
        "beta_hat   =  idata.beta.values.mean()\n",
        "plt.plot(temperature_c, alpha_c_hat + beta_hat * temperature_c, c='k',label='y = {:.1f} + {:.1f} * x'.format(alpha_hat, beta_hat))\n",
        "plt.legend();"
      ],
      "id": "a3ade2c9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  Evaluating the  association between $X$ and $Y$\n",
        "\n",
        "* We have large evidence of positive association between $x$ and $y$, i.e., that $\\beta$>0:\n",
        "    * in our visual examination of the posterior plausible scenarios, all exhibited positive associations.\n"
      ],
      "id": "0eb58060"
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "#  Evaluating the positive association between $x$ and $y$\n",
        "# the 95% HDI of $\\beta$  is about 80-100 and  contains only positive values.\n",
        "# We reject the hypothesis of beta being 0.\n",
        "az.summary(trace, var_names=\"beta\")"
      ],
      "id": "542eb8d4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "az.plot_posterior(trace, var_names=\"beta\", ref_val=0);"
      ],
      "id": "a1010764",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Making predictions\n",
        "\n",
        "\n",
        "*  Predict the number of rides  for the a temperature of 66 F.\n",
        "\n",
        "\n",
        "* First, we center the value: $x^* = 66 - \\bar{x} = 66 - 63.3 = 2.7$ \n",
        "\n",
        "# Making predictions\n",
        "\n",
        "* A simple way to make predictions is to use the posterior mean of the parameters:\n",
        "$$ \\hat{y} = \\hat{\\alpha}_c + \\hat{\\beta} x^*= 3486 + 89.5 \\cdot 2.7 = 3727.6 $$\n",
        "\n",
        "\n",
        "This ignores two sources of variability:\n",
        "   * the effect of the noise $\\epsilon$\n",
        "   * the uncertainty about the value of the  parameters \n",
        "\n",
        "# Posterior predictive distribution\n",
        "\n",
        "To account for parameter uncertainty, we draw a prediction for each parameter sample $<\\alpha_s, \\beta_s, \\sigma_s>$:\n",
        "\n",
        "$$y_{s} \\sim N(\\alpha_{s} + \\beta_s x^*,  \\sigma_s)$$\n",
        "\n",
        "$$\n",
        "\\left[\n",
        "\\begin{array}{lll} \n",
        "\\alpha_{c,1} & \\beta_1 & \\sigma_1 \\\\\n",
        "\\alpha_{c,2} & \\beta_2 & \\sigma_2 \\\\\n",
        "\\vdots & \\vdots & \\vdots \\\\\n",
        "\\alpha_{c,4000} & \\beta_{4000} & \\sigma_{4000} \\\\\n",
        "\\end{array}\n",
        "\\right]\n",
        "\\;\\; \\longrightarrow \\;\\;\n",
        "\\left[\n",
        "\\begin{array}{l} \n",
        "y_1 \\\\\n",
        "y_2 \\\\\n",
        "\\vdots \\\\\n",
        "y_{4000} \\\\\n",
        "\\end{array}\n",
        "\\right]\n",
        "$$\n"
      ],
      "id": "b03b5246"
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "#centered value of x_test\n",
        "x_test = 2.7\n",
        "\n",
        "post = az.extract_dataset(trace)\n",
        "\n",
        "#samples of the different variables. The vector of samples are shown with the underscore _s\n",
        "sigma_s = post.sigma.values\n",
        "alpha_s = post.alpha_c.values\n",
        "beta_s  = post.beta.values\n",
        "\n",
        "y_test = np.zeros(shape=len(sigma_s))\n",
        "\n",
        "#computation of the predictive distribution can be vectorized\n",
        "preds  = np.random.normal (loc = alpha_s + beta_s * x_test, scale = sigma_s)\n",
        "\n",
        "#describe the posterior predictive distribution for the provided x_test\n",
        "pd.DataFrame(preds).describe (percentiles=[.025, .05, .5, .95, .975])"
      ],
      "id": "96d3a2ee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# If background information is not available\n",
        "\n",
        "* Sometimes no background information is available.\n",
        "\n",
        "\n",
        "* We use in this case priors based on the scale of the data (*data-dependent* priors). These are broad priors  which provides  information about the order of magnitude of the variables., and which   anyway  improve  the fitting of the model compared to uniformative flat priors.\n",
        "\n",
        "\n",
        "* See for https://cran.r-project.org/web/packages/rstanarm/vignettes/priors.html for more details.\n",
        "\n",
        "# Recommended priors in case of no background knowledge\n",
        "\n",
        "If we have no prior knowledge, we can set this *weakly informative* priors, which at least  are based on the scale of the data:\n",
        "\n",
        "*  $\\alpha_c \\sim N(\\bar{y}, 2 s_y )$:\n",
        "    * we expect $\\alpha_c$  to be close to  $\\bar{y}$, yet we allow large variability around it (while remaining in the same scale of the data)\n",
        "    * $\\bar{y}$ are the  mean and  the standard deviation of  $Y$ in the sample.\n",
        "    \n",
        "\n",
        "* $\\beta \\sim N(0, 2.5 \\frac{s_y}{s_x})$\n",
        "    * a priori the relation has equal probability of being increasing or decreasing; this justifies the prior mean being 0;\n",
        "    * the prior is broad since in simple linear regression  $\\hat{\\beta} = R \\frac{s_y}{s_x}$, where $R$ is the correlation between $X$ and $Y$.\n",
        "\n",
        "\n",
        "*  $\\sigma \\sim HN (1.5 s_y)$. The prior for $\\sigma$ is large compared to the standard deviation of the data, though remaining on the same scale.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Robust linear regression\n",
        "\n",
        "# Robust linear regression\n",
        "\n",
        "\n",
        "* Gaussianity is often a reasonable approximation but it  fails  in the presence of outliers. \n",
        "\n",
        "\n",
        "* The Student's t-distribution provides  robust inference in presence of outliers. \n",
        "\n",
        "\n",
        "* This  idea can be applied also to linear regression.\n",
        "\n",
        "# Case study\n",
        "\n",
        "* The data are perfectly aligned apart from an outlier.\n",
        "\n",
        "* This is third example of the  Anscombe quartet, which contains some peculiar X-Y data sets  (https://en.wikipedia.org/wiki/Anscombe%27s_quartet)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src='img/anscombe3.png' width=400 align=\"center\" >\n"
      ],
      "id": "1d70ad00"
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "data = pd.read_csv('data/anscombe.csv')\n",
        "\n",
        "#by x_3 and y_3 we denote the explanatory and response variable for the Anscombe data set.\n",
        "#The 3 refers to the fact that this is the third example within the original paper by Anscombe.\n",
        "x = np.array(data[data.group == 'III']['x'].values)\n",
        "y = np.array(data[data.group == 'III']['y'].values)\n",
        "\n",
        "#we  center X\n",
        "x_c = (x - x.mean())\n",
        "plt.plot(x_c, y, 'b.');"
      ],
      "id": "6d583939",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Application\n",
        "\n",
        "* We will fit the model using the recommended data-dependent priors, as there is no background information.\n",
        "\n",
        "\n",
        "* We will fit the model using the Gaussian and the Student likelihood.\n"
      ],
      "id": "d024e2ed"
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "#implementation with data-dependent priors\n",
        "#s_x is the standard deviation of the centered X\n",
        "s_x = x_c.std()\n",
        "s_y = y.std()\n",
        "\n",
        "with pm.Model() as gaussian_model:\n",
        "    #priors on intercept, slope and standard deviation of noise for standardized data\n",
        "    alpha   = pm.Normal ('alpha', mu=0, sigma=2 * s_y)\n",
        "    beta    = pm.Normal ('beta',   mu=0,  sigma= 2.5 * s_y / s_x)\n",
        "    sigma   = pm.HalfNormal ('sigma', sigma= 1.5 * s_y)\n",
        "    \n",
        "    y_pred   = pm.Normal ('y_pred', mu= alpha + beta * x_c,  sigma=sigma, observed=y)    \n",
        "\n",
        "    #the kwargs is needed to store the log_likelihood associated to each sample\n",
        "    #https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/model_comparison.html\n",
        "    gaussian_trace   = pm.sample(idata_kwargs={\"log_likelihood\": True})"
      ],
      "id": "433c06be",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "with pm.Model() as st_model:\n",
        "    #notice the use of weakly, data-dependent prior for *standardized* data\n",
        "    # st stands for Student\n",
        "    alpha_st   = pm.Normal ('alpha_st', mu=0, sigma=2 * s_y)\n",
        "    beta_st    = pm.Normal ('beta_st',   mu=0,  sigma=2.5 * s_y / s_x)\n",
        "    sigma_st   = pm.HalfNormal ('sigma_st', sigma= s_y)\n",
        "    y_pred_st  = pm.StudentT  ('y_pred_st', mu= alpha_st + beta_st * x_c,  sigma=sigma_st,  nu=4, observed=y)\n",
        "    st_trace   = pm.sample(idata_kwargs={\"log_likelihood\": True})"
      ],
      "id": "f7091e8a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "# We are fitting a line to a perfectly aligned set of points, apart from the outlier.\n",
        "#The gaussian model reports  a much larger uncertainty on the estimates and a higher estimate of beta to fit the outlier.\n",
        "pd.concat( [az.summary(gaussian_trace), az.summary(st_trace)])"
      ],
      "id": "34377b08",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualizing the models\n"
      ],
      "id": "092b0539"
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "# For simplicity, we compare the two models  using only  the mean estimate of the parameters without considering the other plausible values.\n",
        "#By using all plausible values, we would see a large uncertainty for the Gaussian model.\n",
        "\n",
        "#posterior mean of the parameters, robust model\n",
        "post_st    = az.extract_dataset(st_trace)\n",
        "a_st       = post_st.alpha_st.values.mean()\n",
        "b_st       = post_st.beta_st.values.mean()\n",
        "\n",
        "#posterior mean of the parameters, gaussian model\n",
        "post_gauss  = az.extract_dataset(gaussian_trace)\n",
        "a           = post_gauss.alpha.values.mean()\n",
        "b           = post_gauss.beta.values.mean()"
      ],
      "id": "f18187a6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "#The Student model perfectly fits the non-outliers \n",
        "plt.plot(x_c, a_st + b_st * x_c,  c='k', lw=3, label='robust')\n",
        "plt.plot(x_c,  a+ b * x_c,  c='b', lw=3, label='gauss')\n",
        "plt.plot(x_c, y, 'g.', markersize=14, label='obs')\n",
        "plt.xlabel('$x$', fontsize=16)\n",
        "plt.ylabel('$y$', rotation=0, fontsize=16)\n",
        "plt.legend();"
      ],
      "id": "95b4bd7d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "# we can also compare the models via WAIC\n",
        "\n",
        "# the Student model is ranked first\n",
        "compare_dict = {'gaussian':gaussian_trace, 'student': st_trace}\n",
        "az.compare(compare_dict, ic='waic')"
      ],
      "id": "04a5f0dd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Discussion\n",
        "\n",
        "* The model with the Gaussian likelihood is affected by the outlier.\n",
        "\n",
        "\n",
        "* The robust model automatically discards the outlier and fits the correct line. \n",
        "\n",
        "\n",
        "* The Student's t-distribution, due to its heavier tails, is able to give less importance to points that are far away from the bulk of the data, filtering outliers in an automatic way.\n",
        "\n",
        "# Solutions\n",
        "\n",
        "# A model of bike rides as a function of humidity\n",
        "\n",
        "# Prior for the $\\beta$ and $\\alpha_c$\n",
        "\n",
        "\n",
        "> For every one percentage point increase in humidity level, ridership tends to decrease by 10 rides, though the decrease could vary between 0 and 20.\n",
        "\n",
        "\n",
        "$$ \\beta  \\sim N(-10, 5)  $$\n",
        "\n",
        "\n",
        ">   On an average humidity day, there are typically around 5000 riders, the actual number varying between 1000 and 9000.\n",
        "\n",
        "$$\\alpha_c \\sim N(5000, 2000)$$\n",
        "\n",
        "# Prior for $\\sigma$\n",
        "\n",
        "> Ridership is only weakly related to humidity. At any given humidity, ridership will tend to vary with a large standard deviation of 2000 rides.\n",
        "\n",
        "* We can  set:\n",
        "\n",
        "$$ \\sigma  \\sim \\text{HalfNormal} (3000), $$\n",
        "whose  median is around 2000.\n"
      ],
      "id": "681dae51"
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "#obtained by trial and error\n",
        "from scipy.stats import halfnorm \n",
        "pd.DataFrame(halfnorm.rvs(scale=3000, size=10000)).describe()"
      ],
      "id": "d34b4f1a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The resulting model (bike rides as a function of humidity)\n",
        "\n",
        "\n",
        "\\begin{align*}\n",
        "\\alpha_c & \\sim  N (5000, 2000) \\\\\n",
        "\\beta & \\sim N(-10, 5) \\\\\n",
        "\\sigma & \\sim \\text{HalfNormal} (3000)\\\\ \n",
        "Y & \\sim N( \\alpha_c + \\beta X_c, \\sigma)\n",
        "\\end{align*}\n"
      ],
      "id": "7718eb16"
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "#solution of the likelihood exercise\n",
        "\n",
        "from scipy.stats import norm\n",
        "\n",
        "x = np.array([1,5])\n",
        "y = np.array([11,50])\n",
        "beta = 10\n",
        "alpha = 2\n",
        "sigma = 1\n",
        "\n",
        "#vector\n",
        "mu = alpha + beta * x\n",
        "\n",
        "#lik of each observation\n",
        "#syntax: norm.pdf(value whose lik is to be computed, mean, standard deviation)\n",
        "lik_vector = norm.pdf(y, mu, sigma)\n",
        "\n",
        "#likelihood for both data points\n",
        "lik = np.prod(lik_vector)\n",
        "\n",
        "#to have a numerically stable approach, usually the log-likelihood is considered rather than the likelihood.\n",
        "print(lik_vector)\n",
        "print(lik)"
      ],
      "id": "1dc6a414",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}