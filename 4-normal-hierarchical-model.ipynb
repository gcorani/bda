{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import arviz as az\n",
    "from IPython.display import display, Markdown\n",
    "az.style.use('arviz-darkgrid')\n",
    "import numpy as np\n",
    "np.random.seed(44)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 15\n",
    "plt.rcParams['legend.fontsize'] = 'medium'\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': [8.0, 3.0],\n",
    "    'figure.facecolor': '#fffff8',\n",
    "    'axes.facecolor': '#fffff8',\n",
    "    'figure.constrained_layout.use': True,\n",
    "    'font.size': 14.0,\n",
    "    'hist.bins': 'auto',\n",
    "    'lines.linewidth': 3.0,\n",
    "    'lines.markeredgewidth': 2.0,\n",
    "    'lines.markerfacecolor': 'none',\n",
    "    'lines.markersize': 8.0, \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Normal Hierarchical  Models\n",
    "\n",
    "\n",
    "Giorgio Corani <br/>\n",
    "*Bayesian Data Analysis and Probabilistic Programming*\n",
    "<br/>\n",
    "<br/>\n",
    "``giorgio.corani@supsi.ch``\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Based on \n",
    "\n",
    "* Alicia A. Johnson, Miles Q. Ott, Mine Dogucu, Bayes Rules! An Introduction to Applied Bayesian Modeling, Chapter 16,  *Hierarchical Models without Predictors*, https://www.bayesrulesbook.com/chapter-16.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modeling songs popularity\n",
    "\n",
    "* Spotify  provides a data bases of songs and their *popularity* score,  which varies over  the 0-100 scale.  A priori we can expect the average popularity rating to be around 50.\n",
    "\n",
    "\n",
    "* Higher popularity usually means the song has been played often, especially  recently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Research questions \n",
    "\n",
    "\n",
    "*   What’s the typical popularity of a Spotify song?\n",
    "\n",
    "*   How does popularity vary from artist to artist?\n",
    "\n",
    "\n",
    "*   For any single artist, how much might popularity vary from song to song?\n",
    "\n",
    "\n",
    "*  We look for answers with a quantification of the related uncertainty; hence we use probabilistic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>title</th>\n",
       "      <th>popularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alok</td>\n",
       "      <td>On &amp; On</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alok</td>\n",
       "      <td>All The Lies</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alok</td>\n",
       "      <td>Hear Me Now</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alok</td>\n",
       "      <td>The Wall</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alok</td>\n",
       "      <td>Hear Me Now</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist         title  popularity\n",
       "0   Alok       On & On          79\n",
       "1   Alok  All The Lies          56\n",
       "2   Alok   Hear Me Now          75\n",
       "3   Alok      The Wall          65\n",
       "4   Alok   Hear Me Now          52"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the full data set is available from the bayesrule package for R. This is a reduced version which only contains song, author and popularity.\n",
    "#The data set contains 350 songs by 44 artists\n",
    "spotify = pd.read_csv(\"data/spotify.csv\")\n",
    "\n",
    "spotify.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify[\"artist\"] = spotify[\"artist\"].astype(\"category\")\n",
    "# artists is a list containing the  name of each artist: [['Alok', 'Atlas Genius', 'Au/Ra', 'BUNT.', 'Beyoncé', ...]\n",
    "artists = list(spotify[\"artist\"].cat.categories) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hierarchical data set\n",
    "\n",
    "The data set is hierarchical:\n",
    "\n",
    "* it comprises  multiple songs for each of 44 artists \n",
    "* the artists  were sampled from the population of all artists that have songs on Spotify \n",
    "\n",
    "<img src='img/spotify-hierarchical-data-diagram.png' width=600 align=\"center\" >\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>popularity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>artist</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mia X</th>\n",
       "      <td>13.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chris Goldarg</th>\n",
       "      <td>16.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soul&amp;Roll</th>\n",
       "      <td>24.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Honeywagon</th>\n",
       "      <td>31.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Röyksopp</th>\n",
       "      <td>33.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               popularity\n",
       "artist                   \n",
       "Mia X           13.250000\n",
       "Chris Goldarg   16.400000\n",
       "Soul&Roll       24.200000\n",
       "Honeywagon      31.666667\n",
       "Röyksopp        33.250000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean popularity of the songs of each artist.\n",
    "# There are major differences between authors, both in the number of produced songs and in their  popularity.\n",
    "\n",
    "artist_popularity=spotify.groupby(['artist']).mean().sort_values('popularity').head()\n",
    "artist_popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>popularity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>artist</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alok</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Atlas Genius</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Au/Ra</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BUNT.</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beyoncé</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              title  popularity\n",
       "artist                         \n",
       "Alok             19          19\n",
       "Atlas Genius      4           4\n",
       "Au/Ra             5           5\n",
       "BUNT.             3           3\n",
       "Beyoncé          25          25"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of the songs of each artist, which varies between 2 and 40.\n",
    "artist_count=spotify.groupby(['artist']).count()\n",
    "artist_count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The structure of the data\n",
    "* The $j$ subscript refers to the $j$-th  artist.\n",
    "\n",
    "* Each song is written by an artist; artist are indexed by $j$.\n",
    "\n",
    "\n",
    "\n",
    "*  $y_{ij}$  represents the popularity of the $i$-th song of artist $j$\n",
    "    * $i \\in \\{1,2,…,n_j\\}$ and $j \\in \\{1,2,…,44\\}$.\n",
    "\n",
    "\n",
    "\n",
    "* $n_j$ is  the number of total songs composed by  artist $j$ , $j \\in  \\{1,2,…,44\\}$. \n",
    "\n",
    "\n",
    "\n",
    "* The sample is the collection of 44 smaller samples (*sub-populations* or *clusters*), one for  each artist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modelling approaches\n",
    "\n",
    "* *Pooled*: ignore artists and merge the data from all songs.\n",
    "     * it  does not model differences between artists\n",
    "     \n",
    "     \n",
    "* *Unpooled*:  analyze  independently the population of songs from each artist\n",
    "    * the prediction of an artist which has published  few songs is very uncertain\n",
    "    * cannot predict a novel artist\n",
    "\n",
    "\n",
    "* *Partial pooling*  (hierarchical model):  model the population of  artists and the population of songs from each artist.\n",
    "    * each artist has his own  estimated popularity\n",
    "    * the model jointly analyzes  information regarding different artistists\n",
    "    * it can  predict also artists for which there are no songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pooled model\n",
    "\n",
    "* The  pooled model ignores the clustering (or *grouping*) structure implied by the different authors.\n",
    "*  It treats  all songs as a sample from the same population, without modelling the  presence of sub-populations corresponding to the different authors.\n",
    "* For simplicity,  we assume  the ratings to be normally distributed.\n",
    "\n",
    "<img src='img/spotify-density.png' width=400 align=\"center\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pooled model\n",
    "\n",
    "* We assume the  popularity of every song to be   normally distributed $N(\\mu,\\sigma)$.  Thus $\\mu$ and $\\sigma$ are shared by every song;  we refer to them as   *global* parameters since they do not vary by artist.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu & \\sim N(\\mu', \\sigma_{\\mu} ) \\\\\n",
    "\\sigma & \\sim \\text{HalfNormal}(\\xi)\\\\\n",
    "y_{ij} & \\sim N(\\mu, \\sigma) \\;\\;  \\forall i,j \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Quiz yourself\n",
    "\n",
    "* What would be the  prediction of the  pooled model for:\n",
    "\n",
    "    * a new song of  Mia X, the artist with the lowest mean popularity  (13) ? \n",
    "    * Beyoncé, the artist with nearly the highest mean popularity in our sample (70)?\n",
    "    * Mohsen Beats, a group not present in the sample?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Answer\n",
    "* The  complete pooled model ignores the artist-specific information.\n",
    "* It  predicts the same  popularity for any new song of  every artist, included or not included in the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The prior on $\\mu$\n",
    "\n",
    "* A priori, we believe $\\mu$ to be  around 50; we think that with 95% probability $\\mu$ lies in  (10, 90).\n",
    "\n",
    "\n",
    "* Interpreting this interval as $\\mu \\pm 2 \\sigma$, we set  a standard deviation of (90-50)/2 $\\approx$ 20. \n",
    "\n",
    "$$ \\mu \\sim N(50, 20) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The prior on $\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24.089939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18.145484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.004981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.808766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>20.401381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>34.519428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>114.781916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "count  10000.000000\n",
       "mean      24.089939\n",
       "std       18.145484\n",
       "min        0.004981\n",
       "25%        9.808766\n",
       "50%       20.401381\n",
       "75%       34.519428\n",
       "max      114.781916"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A priori, we believe a plausible value of  sigma to be  around 20\n",
    "# This  would let the popularity vary in the range 50 +- 3*20 (-10 , 110), with a broad support slightly in excess of the allowed range 0-100.\n",
    "# Using  scale 30, we get the median of the distribution close to 20.\n",
    "from scipy.stats import halfnorm \n",
    "pd.DataFrame(halfnorm.rvs(scale=30, size=10000)).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The pooled model\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu    & \\sim N(50, 20) \\\\\n",
    "\\sigma & \\sim\\text{HalfNormal} (30) \\\\\n",
    "y_{ij} | \\mu, \\sigma & \\sim N(\\mu, \\sigma) \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [global_sigma, global_mean]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='8000' class='' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [8000/8000 00:02<00:00 Sampling 4 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giorgio/opt/anaconda3/envs/bda/lib/python3.9/site-packages/scipy/stats/_continuous_distns.py:624: RuntimeWarning: overflow encountered in _beta_ppf\n",
      "  return _boost._beta_ppf(q, a, b)\n",
      "/Users/giorgio/opt/anaconda3/envs/bda/lib/python3.9/site-packages/scipy/stats/_continuous_distns.py:624: RuntimeWarning: overflow encountered in _beta_ppf\n",
      "  return _boost._beta_ppf(q, a, b)\n",
      "/Users/giorgio/opt/anaconda3/envs/bda/lib/python3.9/site-packages/scipy/stats/_continuous_distns.py:624: RuntimeWarning: overflow encountered in _beta_ppf\n",
      "  return _boost._beta_ppf(q, a, b)\n",
      "/Users/giorgio/opt/anaconda3/envs/bda/lib/python3.9/site-packages/scipy/stats/_continuous_distns.py:624: RuntimeWarning: overflow encountered in _beta_ppf\n",
      "  return _boost._beta_ppf(q, a, b)\n",
      "Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 16 seconds.\n"
     ]
    }
   ],
   "source": [
    "with pm.Model() as pooled_model:\n",
    "    # prior\n",
    "    global_mean    =   pm.Normal ('global_mean', 50,  20)\n",
    "    global_sigma   =  pm.HalfNormal ('global_sigma', sd = 30)\n",
    "\n",
    "    #likelihood\n",
    "    popularity          =  pm.Normal ('popularity', mu = global_mean,  sigma = global_sigma, observed = spotify[\"popularity\"])\n",
    "    trace_pooled     = pm.sample(return_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_3%</th>\n",
       "      <th>hdi_97%</th>\n",
       "      <th>mcse_mean</th>\n",
       "      <th>mcse_sd</th>\n",
       "      <th>ess_bulk</th>\n",
       "      <th>ess_tail</th>\n",
       "      <th>r_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>global_mean</th>\n",
       "      <td>58.377</td>\n",
       "      <td>1.126</td>\n",
       "      <td>56.237</td>\n",
       "      <td>60.480</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.013</td>\n",
       "      <td>3594.0</td>\n",
       "      <td>2696.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_sigma</th>\n",
       "      <td>20.708</td>\n",
       "      <td>0.795</td>\n",
       "      <td>19.174</td>\n",
       "      <td>22.143</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.009</td>\n",
       "      <td>3936.0</td>\n",
       "      <td>2923.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  \\\n",
       "global_mean   58.377  1.126  56.237   60.480      0.019    0.013    3594.0   \n",
       "global_sigma  20.708  0.795  19.174   22.143      0.013    0.009    3936.0   \n",
       "\n",
       "              ess_tail  r_hat  \n",
       "global_mean     2696.0    1.0  \n",
       "global_sigma    2923.0    1.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The posterior summary shows  that Spotify songs have an average popularity of about 58; \n",
    "# there is  a relatively large standard deviation from song to song (global_sigma) of about 20 points.\n",
    "az.summary(trace_pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prediction the popularity of the next song\n",
    "\n",
    "* The prediction is the same for any artist.\n",
    "\n",
    "\n",
    "\n",
    "* The trace contains some thousands (e.g., 4000) parameter samples  $<\\mu_s, \\sigma_s>$. \n",
    "\n",
    "\n",
    "\n",
    "* The probabilistic prediction for the next observation is computed  by drawing  a simulated value from each sample:\n",
    "\n",
    "$$\n",
    "y_s^{\\text{new}} | \\mu_s, \\sigma_s \\; \\sim \\; N\\left(\\mu_s, \\sigma_s\\right)\\;\\; s=1,2...4000$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Posterior prediction\n",
    "\n",
    "* Each parameter set  $<\\mu_i, \\sigma_i >$  yields a different prediction:\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{ll} \n",
    "\\mu_1 &   \\sigma_1 \\\\\n",
    "\\mu_2 & \\sigma_2 \\\\\n",
    "\\vdots & \\vdots  \\\\\n",
    "\\mu_{4000}  & \\sigma_{4000} \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\;\\; \\longrightarrow \\;\\;\n",
    "\\left[\n",
    "\\begin{array}{l} \n",
    "y^{\\text{new}}_{1} \\\\\n",
    "y^{\\text{new}}_2 \\\\\n",
    "\\vdots \\\\\n",
    "y^{\\text{new}}_{4000} \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "The resulting collection of 4,000 predictions $Y_{\\text{new}}$ approximates the posterior prediction for a new measurement, accounting for two sources of uncertainty:\n",
    "\n",
    "   * uncertainty in the estimation of $\\mu, \\sigma$\n",
    "   * uncertainty in the realization of the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Implementation of  the posterior prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [prediction, global_sigma, global_mean]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='8000' class='' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [8000/8000 00:04<00:00 Sampling 4 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giorgio/opt/anaconda3/envs/bda/lib/python3.9/site-packages/scipy/stats/_continuous_distns.py:624: RuntimeWarning: overflow encountered in _beta_ppf\n",
      "  return _boost._beta_ppf(q, a, b)\n",
      "/Users/giorgio/opt/anaconda3/envs/bda/lib/python3.9/site-packages/scipy/stats/_continuous_distns.py:624: RuntimeWarning: overflow encountered in _beta_ppf\n",
      "  return _boost._beta_ppf(q, a, b)\n",
      "/Users/giorgio/opt/anaconda3/envs/bda/lib/python3.9/site-packages/scipy/stats/_continuous_distns.py:624: RuntimeWarning: overflow encountered in _beta_ppf\n",
      "  return _boost._beta_ppf(q, a, b)\n",
      "Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 19 seconds.\n"
     ]
    }
   ],
   "source": [
    "with pm.Model() as pooled_model:\n",
    "    # prior\n",
    "    global_mean    =   pm.Normal ('global_mean', 50,  13)\n",
    "    global_sigma   =  pm.HalfNormal ('global_sigma', sd = 30)\n",
    "\n",
    "    #likelihood\n",
    "    popularity          =  pm.Normal ('popularity', mu = global_mean,  sigma = global_sigma, observed = spotify[\"popularity\"])\n",
    "    \n",
    "    #prediction, computed every time a new value of global_mean and global_sigma is sampled.\n",
    "    prediction          =  pm.Normal('prediction', mu=global_mean, sigma=global_sigma )\n",
    "    trace_pooled     = pm.sample(return_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_3%</th>\n",
       "      <th>hdi_97%</th>\n",
       "      <th>mcse_mean</th>\n",
       "      <th>mcse_sd</th>\n",
       "      <th>ess_bulk</th>\n",
       "      <th>ess_tail</th>\n",
       "      <th>r_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prediction</th>\n",
       "      <td>58.117</td>\n",
       "      <td>21.556</td>\n",
       "      <td>17.368</td>\n",
       "      <td>97.819</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.221</td>\n",
       "      <td>5198.0</td>\n",
       "      <td>2751.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              mean      sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  \\\n",
       "prediction  58.117  21.556  17.368   97.819      0.299    0.221    5198.0   \n",
       "\n",
       "            ess_tail  r_hat  \n",
       "prediction    2751.0    1.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#with pooled_model:\n",
    "az.summary(trace_pooled, var_names='prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unpooled model\n",
    "\n",
    "* This model independently analyzes the popularity of each artist.\n",
    "\n",
    "\n",
    "* The $j$-th artist has its own mean popularity $\\mu_j$.\n",
    "\n",
    "\n",
    "\n",
    "* The popularity of the songs produced by artist $j$ are distributed $N(\\mu_j, \\sigma)$.\n",
    "\n",
    "\n",
    "* One artist’s mean doesn’t tell us anything about another’s.\n",
    "\n",
    "\n",
    "* We assume that  $\\sigma$ is common to all artist and thus it lacks the  $j$ subscript.\n",
    "\n",
    "\n",
    "* That simplifies the sampling, but it could be  worth implementing a  variant with a specific $\\sigma$ for each artist and comparing the two models via WAIC.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unpooled model\n",
    "\n",
    "   * $\\mu_j$ = mean song popularity for artist $j$\n",
    "   * $\\sigma$:  standard deviation of popularity from song to song within each artist.\n",
    "\n",
    "\n",
    "<img src='img/spotify-unpooled.png' width=400 align=\"center\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unpooled model\n",
    "\n",
    "* The priors are the same of the pooled model. But every $\\mu_j$ is now an independent parameter.\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu_j    & \\sim N(50, 20)  \\; \\; \\; j=1,2...44\\\\\n",
    "\\sigma & \\sim \\text{HalfNormal}(30) \\\\\n",
    "y_{ij}  & \\sim N(\\mu_j, \\sigma^2) \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "345    43\n",
       "346    43\n",
       "347    43\n",
       "348    43\n",
       "349    43\n",
       "Name: artist_code, Length: 350, dtype: int8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_artist = len(artist_count) #44\n",
    "\n",
    "#array of string with the name\n",
    "artist       = spotify['artist'].values\n",
    "\n",
    "#We assign a numerical code to each author.\n",
    "#For each song there is a row in the DataFrame; the field 'artist' contains the numerical code of the artist of that song. \n",
    "spotify['artist_code']  = spotify['artist'].astype('category').cat.codes\n",
    "spotify['artist_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [global_sigma, mu_artist]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with pm.Model() as unpooled_model:\n",
    "    # We have a vector of means, one for each artist,  each independent from the others.\n",
    "    mu_artist            =  pm.Normal ('mu_artist', 50,  20, shape= n_artist)\n",
    "    global_sigma    =  pm.HalfNormal ('global_sigma', sd = 30)\n",
    "\n",
    "    #likelihood. A different mean for  each artist is inferred.\n",
    "    #Each value of popularity is normally distributed around the popularity of its artist.\n",
    "    popularity      =  pm.Normal ('popularity', mu = mu_artist[spotify['artist_code']],  sigma = global_sigma, observed = spotify[\"popularity\"])\n",
    "    \n",
    "    trace_unpooled  = pm.sample(return_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# assing_coords is  necessary to show the artist name\n",
    "posterior = trace_unpooled.posterior.assign_coords(mu_artist_dim_0=artists)\n",
    "az.summary(posterior).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#We show only the first 10 artists: we limit and label the graph using the coords option. \n",
    "#combined=True implies that we get a single plot for each artist, having merged the samples of the different chains referring to the same artist.\n",
    "\n",
    "axes = az.plot_forest (trace_unpooled,\n",
    "                       kind='ridgeplot',\n",
    "                       hdi_prob=0.68,\n",
    "                       var_names=\"mu_artist\",\n",
    "                       combined=True,\n",
    "                       coords={\"mu_artist_dim_0\": range(10)},\n",
    "                       figsize=(8, 6))\n",
    "artists_10 = artists[:10]\n",
    "axes[0].set_yticklabels(artists_10[::-1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MF_ans: if you meant the first 10 artists sorted in some way this is a possibility\n",
    "trace_unpooled_ = trace_unpooled.copy()\n",
    "trace_unpooled_.posterior = trace_unpooled.posterior.assign_coords(mu_artist_dim_0=artists)\n",
    "artist_means = trace_unpooled_.posterior[\"mu_artist\"].mean((\"chain\", \"draw\"))\n",
    "sorted_artists = trace_unpooled_.posterior[\"mu_artist_dim_0\"].sortby(artist_means, ascending=True)\n",
    "axes = az.plot_forest(trace_unpooled_,\n",
    "                       kind='ridgeplot',\n",
    "                       hdi_prob=0.68,\n",
    "                       var_names=\"mu_artist\",\n",
    "                       combined=True,\n",
    "                       figsize=(8,12),\n",
    "                       coords={\"mu_artist_dim_0\": sorted_artists[0:20]}\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Discussion\n",
    "\n",
    "* The no pooled allows some artists to be more popular than others.\n",
    "\n",
    "\n",
    "\n",
    "* Yet, it  does not model the *population* of  artists. Even though we have analyzed 44 artist, it is unable to say anything about a novel artist not yet present in the sample.\n",
    "\n",
    "\n",
    "\n",
    "* Some estimates   are also problematic as even the 68% prediction interval is rather large; those are artist with few published songs.\n",
    "\n",
    "\n",
    "* For example, our low posterior predictions for Mia X’s next song were based on a measly 4 songs. The other artists’ data suggests that these low ratings might just be a tough break – her next song might be more popular! Similarly, our high posterior predictions for Lil Skies’ next song were based on only 3 songs. In light of the other artists’ data, we might wonder whether this was beginner’s luck that will be tough to maintain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hierarchical model\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}{lrl}\n",
    "\\text{Layer 1:} & \\hspace{-0.05in} y_{ij} \\sim N( \\mu_j, \\sigma_y  ) & \\hspace{-0.075in} \\text{model of how song popularity varies WITHIN artist } j \\\\\n",
    "\\text{Layer 2:} & \\hspace{-0.05in} \\mu_j \\sim N( \\mu, \\sigma_\\mu) & \\hspace{-0.075in}  \\text{models the population of artists:  how  $\\mu_j$ varies BETWEEN artists}\\\\\n",
    "\\text{Layer 3:} & \\hspace{-0.05in} \\text{densities for } \\mu, \\sigma, \\sigma_\\mu  & \\hspace{-0.075in} \\sim \\text{prior models for shared global parameters} \\\\\n",
    "\\end{array}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Individual songs for each artist.\n",
    "\n",
    "* We assume that the popularity of songs  of  artist $j$ to be  normally distributed  $N(μ_j,σ_y)$\n",
    "\n",
    "\n",
    "*  $σ_y$ is assumed to be the same for all artistist; it expresses the spread in popularity between songs of the same artist\n",
    "\n",
    "\n",
    "* This is the likelihood of the model:\n",
    "\n",
    "\n",
    "$$  y_{ij} \\sim N(\\mu_j, \\sigma_y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modeling the population of artists\n",
    "\n",
    "\n",
    "* Here we model the distribution of the $\\mu_j$'s.\n",
    "\n",
    "\n",
    "\n",
    "* The hierarchical model  assumes intead  the 44  artists to be drawn from the population of Spotify artists:\n",
    "\n",
    "\n",
    "$$ \\mu_j \\sim N(\\mu', \\sigma_{\\mu}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modeling the population of artists\n",
    "\n",
    "$$ \\mu_j \\sim N(\\mu', \\sigma_{\\mu}) $$\n",
    "\n",
    "\n",
    "* $\\mu'$ :   average popularity within the population of artists\n",
    "\n",
    "\n",
    "* $\\sigma_\\mu$ :  standard deviation of popularity within the population of  artists.\n",
    "\n",
    "\n",
    "* We want to learn the population of artists, and hence both $\\mu'$ and $\\sigma_{\\mu}$ are  *parameters*, to which we assign a prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Notation alert\n",
    "\n",
    "* Notice the  difference between $μ_j$ (mean of the j-th artist) and $\\mu'$ (mean of the population of artists).\n",
    "\n",
    "\n",
    "* $\\sigma_y$ refers to the standard deviation of $y$ values within each group. \n",
    "\n",
    "\n",
    "* $\\sigma_{\\mu}$ refers to the standard deviation of the means $\\mu_j$ within the population of artists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Priors for the global parameters $\\mu, \\sigma_{\\mu}, \\sigma_y$\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu'           & \\sim N(50, 20) & \\text{(implies a range of +- 60 around 50)} \\\\\n",
    "\\sigma_y      & \\sim \\text{Half Normal}(15) & \\text{(discussed in next slide)}     & \\\\\n",
    "\\sigma_\\mu & \\sim \\text{Half Normal}(6)    & \\text{(uncertainty in the location of the global mean)}  \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# how to choose the priors on sigma_y\n",
    "# We assume the mean popularity to lie in a region of +- 20 points around 50, i.e., a median sigma_y could be 10.\n",
    "# A possible choice is given below; long tails allow to cover even much larger values for sigma_y than the median.\n",
    "pd.DataFrame(np.abs(np.random.normal(0, scale=15, size=10000))).describe(percentiles=[0.50, 0.75, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# how to choose the priors on sigma_mu\n",
    "# We assume the mean popularity to lie in a region of +- 8 points around 50.\n",
    "pd.DataFrame(np.abs(np.random.normal(0, scale=6, size=10000))).describe(percentiles=[0.50, 0.75, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n_artist = len(artist_count) #44\n",
    "\n",
    "with pm.Model() as hier_model:\n",
    "    \n",
    "    # layer 1: prior on the parameters of the population of artists (hyper-priors). \n",
    "    # prior beliefs about the location of the mean popularity within the population of artists\n",
    "    mu_p          = pm.Normal ('mu_p', 50,  20)\n",
    "    sigma_mu = pm.HalfNormal ('sigma_mu', 6)\n",
    "    sigma_y      = pm.HalfNormal ('sigma', 15)\n",
    "\n",
    "    \n",
    "    # layer 2: model of the population of artists.\n",
    "    mu_artist  =  pm.Normal ('mu_artist', mu_p,  sigma_mu, shape= n_artist)\n",
    "    sigma_y     = pm.HalfNormal ('sigma_y', sd = sigma_y)\n",
    "\n",
    "    #layer 3 (observation specific for each artist)\n",
    "    popularity    =  pm.Normal ('popularity', mu = mu_artist[spotify['artist_code']],  sigma = sigma_y, observed = spotify[\"popularity\"])\n",
    "    \n",
    "    trace_hier    = pm.sample(return_inferencedata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Before analyzing the  results, let us think a little bit more.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Grouping variable or covariate?\n",
    "\n",
    "* Suppose our dataset includes a categorical variable  $X$\n",
    "\n",
    "\n",
    "* Should $X$  be modeled  hierarchically or as a categorical covariate?\n",
    "\n",
    "    *   if the observed data on $X$ covers all categories of interest, it is  better treated as a covariate\n",
    "    \n",
    "    *  if the observed categories is a random sample from many possible ones, it is a potential grouping variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Grouping variable or covariate?\n",
    "\n",
    "\n",
    "* Our data set includes only  44 among thousands of artists on Spotify. Hence, treating artist as a predictor (as in the no pooled model) would limit our understanding to only this small number artists.\n",
    "\n",
    "\n",
    "\n",
    "* In contrast, treating it as a grouping variable (as in the hierarchical model) allows us to not only learn about the 44 artists in our data, but the broader population of artists from which they were sampled.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Grouping variable or covariate?\n",
    "\n",
    "* Consider a daily data set of bike-sharing covering two years; for each day we know whether it was a weekday or a weekend.\n",
    "\n",
    "* Is the weekend variable a grouping variable or  a covariate?\n",
    "\n",
    "    * There are only two possible categories: weekend and weekday.\n",
    "    * The dataset covers both categories, with many observations for each category.\n",
    "    * Hence  it is a covariate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Grouping variable or covariate?\n",
    "\n",
    "* Consider a data set containing math scores  from a sample of 10 Swiss primary schools. \n",
    "\n",
    "\n",
    "* These 10 schools are merely a small sample from the hundreds  of Swiss primary schools.\n",
    "    * The `school_id`, treated as a covariate (unpooled model) would only allow us to learn about our small sample of schools.\n",
    "    * Treating it as a grouping variable in a hierarchical model  would allow us to extend our conclusions to the broader population of all schools; this is a better modelling choice.\n",
    "    \n",
    "    \n",
    "* The same consideration applies if you want to analyze e.g. data of a certain disease collected by different hospitals, in which case the involved hospitals can be modelled as a grouping variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Grouping variable or covariate\n",
    "\n",
    "* In some cases the decision is not clear-cut. For instance you might have a categorical variable with different levels; for some of them many observations are available, for some other levels the number of observations is however very small.\n",
    "\n",
    "* In this case you might fit both models, and eventually choose according to WAIC.\n",
    "\n",
    "* WAIC is a model selection criterion, to be discussed more in detail later. For the moment, it is enough to know that we choose we model with lowest WAIC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conceptual exercise: modelling fuel prices\n",
    "\n",
    "* Define a hierarchical model for the fuel price which considers  different stations located in the same area over a period of e.g. one month.\n",
    "\n",
    "\n",
    " * Thus $y_{ij}$ is the price of gas at the $j$-th station in the $i$-th day.\n",
    "\n",
    "\n",
    "*  Discuss the meaning of all model parameters ( $\\mu_j$, $\\sigma_y$, $\\mu$, $\\sigma_{\\mu}$).\n",
    "\n",
    "\n",
    "*  Write the probabilistic model defining the priors based on your experience.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Back to the numerical example of Spotify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parameters\n",
    "* The hierarchical Spotify model  has  47 parameters:\n",
    "    * 44 artist-specific parameters $\\mu_j$\n",
    "    * 3 global parameters $\\mu', \\sigma_y,  \\sigma_\\mu\\$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "az.summary(trace_hier, var_names=[\"mu_p\", \"sigma_y\", \"sigma\", \"mu_artist\"])[[\"mean\",\"hdi_3%\",\"hdi_97%\",\"r_hat\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#estimated popularity of each artist, according to the unpooled and to the hierarhical model\n",
    "# To have a more readable plot, we use a rather short 68% hdi, corresponding to +-1sigma around the mean\n",
    "\n",
    "trace_unpooled_ = trace_unpooled.copy() # just to avoid touching the existing objects\n",
    "trace_unpooled_.posterior = trace_unpooled.posterior.assign_coords(mu_artist_dim_0=artists)\n",
    "trace_hier_ = trace_hier.copy()\n",
    "trace_hier_.posterior = trace_hier.posterior.assign_coords(mu_artist_dim_0=artists)\n",
    "\n",
    "axes = az.plot_forest([trace_unpooled_, trace_hier_], \n",
    "                      model_names=['unpooled_model', 'hier_model'], combined=True, kind='ridgeplot',\n",
    "                      var_names='mu_artist', hdi_prob=0.68, coords={\"mu_artist_dim_0\": artists[0:20]});\n",
    " \n",
    "#mean popularity of all songs\n",
    "grand_mean = spotify[\"popularity\"].mean()\n",
    "axes[0].axvline(x=grand_mean);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Shrinkage: pulling the estimates towards the grand mean\n",
    "\n",
    "*  The pooled  model ignores the fact that our data is grouped by artist;  the  posterior mean popularity was the same for each artist, which is roughly equivalent to the grand mean (i.e., the mean popularity of the 350 songs)\n",
    "\n",
    "\n",
    "*  The unpooled model separately analyzes each artist;  its predictive means are roughly equivalent to the sample mean popularity of  each artist.\n",
    "\n",
    "\n",
    "* In both cases, the estimates are regularized by the prior, hence they are not truly identical to the sample means\n",
    "\n",
    "\n",
    "* The hierarchical models is a more sophisticated approach. It pulls  (or *shrunk*) the predictions of the unpooled model  toward the global trends of the pooled model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Shrinkage\n",
    "\n",
    "* It the variances $\\sigma^2_y$ and  $\\sigma^2_{\\mu}$ are fixed and known, the posterior mean computed by the hierarchical model for  artist $j$  is: \n",
    "\n",
    "$$\n",
    "\\overline{y}_{j,\\text{hier}} =\n",
    "\\underbrace{\n",
    "\\frac{\\sigma^2_y}{\\sigma^2_y + n_j \\sigma^2_{\\mu}}\n",
    "}_{\\text{weight of the global model}}\n",
    "\\overline{y}_{\\text{global}} + \n",
    "\\underbrace{\n",
    "\\frac{n_j\\sigma^2_{\\mu}}{\\sigma^2_y + n_j \\sigma^2_{\\mu}}\n",
    "}_{\\text{weight of the local model}}\n",
    "\\overline{y}_j\n",
    "$$\n",
    "\n",
    "\n",
    "* This is not our case, as we treat both variances as parameters. Yet, we can use  the above formula to understand *shrinkage*.\n",
    "\n",
    "* The weight of   local model increases when we have more data ($n_j$) about artist $j$.\n",
    "* The weights also depend on $\\sigma_y$: the larger $\\sigma_y$ and hence the variability of popularity for the same artist, the larger the weight of the global model (we cannot trust much the means  $\\overline{y}_j$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Shrinkage\n",
    "\n",
    "$$\n",
    "\\overline{y}_{j,\\text{hier}} =\n",
    "\\underbrace{\n",
    "\\frac{\\sigma^2_y}{\\sigma^2_y + n_j \\sigma^2_{\\mu}}\n",
    "}_{\\text{weight of the global model}}\n",
    "\\overline{y}_{\\text{global}} + \n",
    "\\underbrace{\n",
    "\\frac{n_j\\sigma^2_{\\mu}}{\\sigma^2_y + n_j \\sigma^2_{\\mu}}\n",
    "}_{\\text{weight of the local model}}\n",
    "\\overline{y}_j\n",
    "$$\n",
    "\n",
    "* Scenarios in which the hierarchical posterior predictions shrinks towards the global mean:\n",
    "\n",
    "*  $n_j$ is low:  we rely more on global trends to understand a group for which we have little data.\n",
    "\n",
    "* the variability of popularity for songs of the same artist ($\\sigma_y$) is large in comparison to the variability the mean populatrity of the artists $\\sigma_{\\mu}$.\n",
    "\n",
    "*  the models learns that  artist are very similar to each other ($\\sigma_{\\mu}$ is low), it increases the weight of the pooled model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# showing the amount of shrinkage as a function of the number of songs for each artist\n",
    "\n",
    "n_j = spotify['artist'].value_counts(sort=False)\n",
    "mean_unpooled = az.summary(trace_unpooled, var_names=[\"mu_artist\"])[\"mean\"]\n",
    "mean_hier = az.summary(trace_hier, var_names=[\"mu_artist\"])[\"mean\"]\n",
    "shrinkage = pd.Series((mean_hier-mean_unpooled).values, index=artists)\n",
    "\n",
    "df = pd.concat([n_j, shrinkage], axis=1)\n",
    "df.columns = [\"n_j\", \"shrinkage\"]\n",
    "plt.plot(df[\"n_j\"], df[\"shrinkage\"].abs(), \"*\")\n",
    "plt.xlabel(\"n_j\")\n",
    "plt.ylabel(\"shrinkage\"); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Discussion\n",
    "\n",
    "* The pooled model is very rigid  and  miss the nuances in artists’ mean popularity. \n",
    "\n",
    "\n",
    "* No pooled models have the opposite problem. With the built-in flexibility to detect group-specific trends, they tend to have less bias than complete pooled models. But, since they’re tailored to the artists in our sample, it cannot produce predictions regarding  a different set of Spotify artists. Moreover, predictions about artists with limited number of songs have large uncertainty,\n",
    "\n",
    "\n",
    "* Hierarchical models offer a balanced alternative. Unlike complete pooled models, hierarchical models take group-specific trends into account, and thus will be less biased. And unlike no pooled models, hierarchical models take global trends into account, and thus will be less variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probabilistic prediction  with the hierarchical model\n",
    "\n",
    "* Let us consider e.g. artist n. 25.\n",
    "\n",
    "* Assuming to access the sample of the trace, our prediction about the popularity of the next song by artist 25 is done simulating, for each sample $s$,: \n",
    " $$y_{25,s}^{new} \\sim N( \\mu_{25,s}, \\sigma_{y,s}) $$\n",
    " \n",
    " * Implement the prediction and check the consistency of the results by:\n",
    "     * using the samples from the trace\n",
    "     * implementing the prediction within the pymc3 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probabilistic prediction  with the hierarchical model\n",
    "\n",
    "\n",
    "* Write a pseudo-code to predict the popularity of the first  song of a novel artist.\n",
    "\n",
    "\n",
    "* Implement the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# prediction of the popularity of a new song for artist 25 by  accessing the trace\n",
    "\n",
    "a = az.extract_dataset(trace_hier)\n",
    "y_new_25 = np.random.normal (loc = a[\"mu_artist\"][25],  scale = a[\"sigma_y\"])\n",
    "pd.DataFrame(y_new_25).describe(percentiles=[0.03,0.97])\n",
    "\n",
    "# a very few sampled data are negative. Can you trace the reason of this problem and provide a solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n_artist = len(artist_count) #44\n",
    "\n",
    "with pm.Model() as hier_model_with_preds:\n",
    "    \n",
    "    # layer 1: prior on the parameters of the population of artists (hyper-priors). \n",
    "    # prior beliefs about the location of the mean popularity within the population of artists\n",
    "    mu_population          = pm.Normal ('mu_p', 50,  20)\n",
    "    sigma_mu = pm.HalfNormal ('sigma_mu', 6)\n",
    "    sigma_y      = pm.HalfNormal ('sigma', 15)\n",
    "\n",
    "    \n",
    "    # layer 2: model of the population of artists.\n",
    "    mu_artist  =  pm.Normal ('mu_artist', mu_population,  sigma_mu, shape= n_artist)\n",
    "    sigma_y     = pm.HalfNormal ('sigma_y', sd = sigma_y)\n",
    "\n",
    "    #layer 3 (observation specific for each artist)\n",
    "    popularity    =  pm.Normal ('popularity', mu = mu_artist[spotify['artist_code']],  sigma = sigma_y, observed = spotify[\"popularity\"])\n",
    "    \n",
    "    # == PREDICTIONS\n",
    "    #prediction of popularity for the next song of  artist 25\n",
    "    prediction_artist25 = pm.Normal('prediction_artist25', mu = mu_artist[25],  sigma = sigma_y)\n",
    "    \n",
    "    #prediction of popularity for a song from a  novel artist. This requires two steps.\n",
    "    # step 1:  sample the popularity mu_j of the new artist, by sampling from the population of artists.\n",
    "    mu_new_artist = pm.Normal('new_mean', mu=mu_population, sigma=sigma_mu)\n",
    "    # step 2:  sample the popularity of a song using the sampled values mu_j for the unknown artist popularity\n",
    "    prediction_new_artist = pm.Normal('prediction_new_artist', mu = mu_new_artist,  sigma = sigma_y)\n",
    "    \n",
    "    trace_hier_with_preds   = pm.sample(return_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice the substamtial uncertainty for the prediction of the new artist\n",
    "az.summary(trace_hier_with_preds, var_names=['prediction_artist25', 'prediction_new_artist'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conceptual exercise \n",
    "\n",
    "* Four friends take a  speed-typing tests. Each person repeat 20 times the typing of a certain word.\n",
    "\n",
    "* Let $y_{ij}$ be the time it takes friend $j$ to complete test $i$.\n",
    "\n",
    "\n",
    "1) Formalize  a hierarchical model, discussing  the meaning of all model parameters: ($μ_j, μ, \\sigma_y, \\sigma_{\\mu}$)\n",
    "\n",
    "    \n",
    "2) Consider the following scenarios and discuss their effect on the posterior distribution of the parameters:\n",
    "\n",
    "    * The overall results of the 20 timed tests are remarkably similar among the four friends.\n",
    "    * Each person is quite consistent in their typing times, but there are big differences from person to person – some tend to type much faster than others.\n",
    "\n",
    "* For each scenario, indicate whether:\n",
    "* $ σ_y<σ_μ$. \n",
    "* $ σ_y≈σ_μ$. \n",
    "* $ σ_y>σ_μ$. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
