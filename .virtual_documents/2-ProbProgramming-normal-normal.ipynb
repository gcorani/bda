%matplotlib inline
import pymc as pm
import numpy as np
import scipy.stats as stats
import pandas as pd
import matplotlib.pyplot as plt
import arviz as az
from IPython.display import display, Markdown
az.style.use('arviz-darkgrid')
import numpy as np
import seaborn as sns
np.random.seed(44)



plt.rcParams['font.size'] = 15
plt.rcParams['legend.fontsize'] = 'medium'
plt.rcParams.update({
    'figure.figsize': [7,3],
    'figure.constrained_layout.use': True,
    'font.size': 14.0,
    'hist.bins': 'auto',
    'lines.linewidth': 3.0,
    'lines.markeredgewidth': 2.0,
    'lines.markerfacecolor': 'none',
    'lines.markersize': 8.0, 
})
sns.set(rc={'figure.figsize':(7,3)})














#Below  the  66 measurements.

data = np.array([248.28, 248.26, 248.33, 248.24, 248.34, 247.56, 248.27, 248.16,
                  248.4, 247.98, 248.29, 248.22, 248.24, 248.21, 248.25, 248.3,
                  248.23, 248.29, 248.31, 248.19, 248.24, 248.2, 248.36, 248.32,
                  248.36, 248.28, 248.25, 248.21, 248.28, 248.29, 248.37, 248.25,
                  248.28, 248.26, 248.3, 248.32, 248.36, 248.26, 248.3, 248.22,
                  248.36, 248.23, 248.27, 248.27, 248.28, 248.27, 248.31, 248.27,
                  248.26, 248.33, 248.26, 248.32, 248.32, 248.24, 248.39, 248.28,
                  248.24, 248.25, 248.32, 248.25, 248.29, 248.27, 248.28, 248.29,
                  248.16, 248.23])



# The data are almost  normally distributed, apart from some  low-values outliers  which are far from the mean.
#the density plot is called kde (kernel density estimation)
az.plot_kde(data, figsize=[12,5])
plt.yticks([])
plt.xlabel('milliseconds');


# Implementation with  informative priors 

with pm.Model() as normal_model:
    # Based on prior information, the mean travel time should be within  (120,  360).
    # This information can be represented by a normal distribution with mu = 240, sigma = 60.
    mu = pm.Normal('mu', 240, 60)
    
    # By using a  scale parameter of 3, the median  of the halfnormal is close to 2., 
    sigma = pm.HalfNormal('sigma', 3)

    
    #likelihood. The observation are stored in vector 'data'.
    #and assumed to be independent.
    y = pm.Normal('y', mu=mu, sigma=sigma, observed=data)
    
    trace_normal = pm.sample()


# The  density plots of  both parameters are smooth and the chains are well-mixing.

with  normal_model:
    az.plot_trace(trace_normal);





with pm.Model() as normal_model_2:

    #mu = pm.Normal('mu', 240, 60)
    #we assume to have  different prior information  about mu; larger mean and larger uncertainty.
    mu = pm.Normal('mu', 300, 100)
    
    #identical to the previous model
    sigma = pm.HalfNormal('sigma', 3)
    y = pm.Normal('y', mu=mu, sigma=sigma, observed=data)
    
    trace_normal_2 = pm.sample(return_inferencedata=True)


# The posterior is practically identical under the two different priors.
# This is  good: the data are strong enough to converge to the same posterior.
# The likelihood overwhelms the prior; the results are thus robust to the specification of the prior.


a = az.summary(trace_normal) 
b = az.summary(trace_normal_2) 
pd.concat([a,b])




















#extract samples from the trace, merging samples from the different chains
post_samples = az.extract(trace_normal.posterior)

#vector of samples of each parameter
mu_post = post_samples.mu.values
sigma_post = post_samples.sigma.values

#vector containing the predictions.
y_new = np.zeros(len(mu_post))

#your code: compute a probabilistic prediction from each pair of  value <mu_s sigma_s>
#compute the 95% HDI of your prediction
#plot the kde of the sample values of y_new using az.plot_kde(y_new)


# The posterior prediction can be computed conveniently also within pymc.
# However for didactic reason implementing the  loop on the posterior samples 
# helps understanding

with pm.Model() as normal_model_with_pred:
    #model as before
    mu = pm.Normal('mu', 240, 80)
    sigma = pm.HalfNormal('sigma', 3)
    y = pm.Normal('y', mu=mu, sigma=sigma, observed=data)
    
    #prediction
    #for each sample value mu_s and sigma_s, a prediction is generated as N(mu_s, sigma_s).
    y_new = pm.Normal('y_new', mu=mu, sigma=sigma)
        
    trace_normal_with_pred = pm.sample(return_inferencedata=True)


# using the 'var_names' argument we can select the variables shown in the summary
az.summary(trace_normal_with_pred, var_names='y_new', hdi_prob=0.95)








with normal_model:
    # select 500 samples <mu_s ,sigma_s> from the trace
    # for each samples, compute a draw N(mu_s ,sigma_s) 
    # The ppc variable is a dictionary, with the keys being the name of the observed variable in our model and the values an array of shape (samples, size). 
    # The dictionary allows dealing with models with more than one observed variable. 
    ppc = pm.sample_posterior_predictive(trace_normal)


# The density related to each mu_s and sigma_s is shown. The average density is also shown.
# The density reproduced by the model is too low around the center.
# That is becaude the  variance is thus too large, in order to assign some probability also to the outliers. 
az.plot_ppc(ppc);























with pm.Model() as model_t:
    #the priors are the same as those of  the first variant
    mu_t    = pm.Normal('mu_t', 240, 80)
    sigma_t = pm.HalfNormal('sigma_t', sigma=3)

    #robust likelihood
    y       = pm.StudentT('y', mu=mu_t, sigma=sigma_t, nu=4, observed=data)

    #prediction for a new measurement
    #prediction = pm.StudentT('prediction', mu=mu_t, sd=sigma_t, nu=4)
        
    trace_t = pm.sample(return_inferencedata=True) 


# The estimate of mu is  identical between the model with normal and t likelihood,
# but the second has much lower  sigma than the normal model. 
# The t- distribution gives less weight (being less surprised) to values away from the mean; it does not need inflating  the  variance.
#  This allows keeping higher the density around the bulk of the data, as it will be shown by the posterior predictive check.


with model_t:
    summary_t = az.summary(trace_t)
with normal_model:
    summary= az.summary(trace_normal)
pd.concat([summary, summary_t])[ ["mean","sd" ]]


with model_t:
    ppc_t = pm.sample_posterior_predictive(trace_t)
    
az.plot_ppc(ppc_t);











# predictive distribution from the normal-normal model case

#extract samples from different chains
post_samples = az.extract(trace_normal.posterior)
mu_post      = post_samples.mu.values
sigma_post   = post_samples.sigma.values

#your code: compute a probabilistic prediction for the next observation
y_new = np.zeros(len(mu_post))

#we should use together  the values of sigma e mu referrring to the same sample 
for i, current_mu in enumerate(mu_post):
        y_new[i] = np.random.normal (loc = mu_post[i], scale = sigma_post[i])

# apart from numerical differences, it is the same distribution returned 
# by pymc in the model "normal_model_with_pred"
pd.DataFrame(y_new).describe(percentiles=[0.025, 0.975])        


az.plot_kde(y_new);


# predictive distribution with the Student likelihood
from scipy.stats import t

#the  samples have been already extracted  in the previous cell
post_samples_t = az.extract(trace_t.posterior)
mu_post_t = post_samples_t.mu_t.values
sigma_post_t = post_samples_t.sigma_t.values

#your code: compute a probabilistic prediction for the next observation
y_new_t = np.zeros(len(mu_post_t))

#we should use together  the values of sigma e mu referrring to the same sample 
for i, current_mu in enumerate(mu_post):
        y_new_t[i] = t.rvs (loc = mu_post_t[i], scale = sigma_post_t[i], df=4)

pd.DataFrame(y_new_t).describe(percentiles=[0.025, 0.975])        


#here we visually compare the predictive distribution of the model with Gaussian and  Studend likelihood.
#the t distribution has higher density around the mean; at the same time it has  longer tails.

#to make the visual comparison,  we set up two plots with a shared x-axis
fig, (ax1, ax2) = plt.subplots(nrows=2, figsize=(6, 5.4), sharex=True)
plt.subplot(2,1,1)
az.plot_kde(y_new);
plt.subplot(2,1,2)
az.plot_kde(y_new_t);





#Below  the  66 measurements.

data = np.array([248.28, 248.26, 248.33, 248.24, 248.34, 247.56, 248.27, 248.16,
                  248.4, 247.98, 248.29, 248.22, 248.24, 248.21, 248.25, 248.3,
                  248.23, 248.29, 248.31, 248.19, 248.24, 248.2, 248.36, 248.32,
                  248.36, 248.28, 248.25, 248.21, 248.28, 248.29, 248.37, 248.25,
                  248.28, 248.26, 248.3, 248.32, 248.36, 248.26, 248.3, 248.22,
                  248.36, 248.23, 248.27, 248.27, 248.28, 248.27, 248.31, 248.27,
                  248.26, 248.33, 248.26, 248.32, 248.32, 248.24, 248.39, 248.28,
                  248.24, 248.25, 248.32, 248.25, 248.29, 248.27, 248.28, 248.29,
                  248.16, 248.23])

xbar= np.mean(data)
s = np.std(data)

#the criterion for detecting outliers is a heuristic one.
#here we consider as outliers observations which are more than 3.5 std far from the mean.

# we remove observations lower than xbar - 3.5 sigma
data2 = data [data > xbar - 3.5 * s]  

# we remove observations larger than xbar +3.5 sigma
data2 = data2 [data2 < xbar + 3.5 * s]  

#one observation has been removed
print(len(data))
print(len(data2))


with pm.Model() as normal_model_no_outliers:
    #prior for the location of mu
    # Based on prior information,  mu should lie within the interval (120,  360).
    # This corresponds to a normal distribution with mu = 240, sigma = 60
    mu = pm.Normal('mu', 240, 60)
    
    #Based on prior information, a plausible value for the standard deviation of the noise is 2.
    # This is roughly the median value of the halfnormal distribution with scale set to 3.
    sigma = pm.HalfNormal('sigma', sigma=3)

    
    #likelihood. Each observation is a sample from N(mu, sigma).
    y = pm.Normal('y', mu=mu, sigma=sigma, observed=data2)
    
    trace_normal_no_outliers = pm.sample( return_inferencedata= True )


# the sigma of the Gaussian model decreases much once outliers are removed.
b = az.summary(trace_normal)
c = az.summary(trace_normal_no_outliers)
pd.concat([b,c])



#the posterior predictive is improved compared to the Gaussian model fitted on the original data.
# Anyway, this requires the heuristic work of deciding which observation is an outlier.
# Moreover, this shows how sensitive can be the estimation of the normal model to outliers (we only removed one datum out of 66)
with normal_model_no_outliers:
    ppc_t = pm.sample_posterior_predictive(trace_normal_no_outliers)
    
az.plot_ppc(ppc_t);



