%matplotlib inline
import pymc as pm
import numpy as np
import scipy.stats as stats
import pandas as pd
import matplotlib.pyplot as plt
import arviz as az
from IPython.display import display, Markdown
az.style.use('arviz-darkgrid')
np.random.seed(44)
import seaborn as sns


plt.rcParams['font.size'] = 15
plt.rcParams['legend.fontsize'] = 'medium'
plt.rcParams.update({
    "figure.figsize": [12.0, 5.0],
    'figure.facecolor': '#fffff8',
    'axes.facecolor': '#fffff8',
    'figure.constrained_layout.use': True,
    'font.size': 14.0,
    'hist.bins': 'auto',
    'lines.linewidth': 3.0,
    'lines.markeredgewidth': 2.0,
    'lines.markerfacecolor': 'none',
    'lines.markersize': 8.0, 
})
sns.set(rc={'figure.figsize':(7,3)})














#Generating the data
np.random.seed(123)
n_draws = 100
theta_real = .35  

#we generate the data (rvs stands for random-variates)
data = stats.bernoulli.rvs(p=theta_real, size=n_draws)

# the data are  a vector of 0 and 1; with this seed they contain 30 successes.
print(data)
print(sum(data))








# Beta-binomial model in Pymc
beta_binomial_model = pm.Model()

# The lines within the "with" statement refer to the same model 
with beta_binomial_model:
    #The first argument is  a string specifying the name of the random variable, which should
    #match the name of the Python variable.

    #Uniform prior, i.e., Beta(1,1) 
    theta = pm.Beta('theta', alpha=1, beta=1)  

    
    #Likelihood, characterized by the keyword  `observed`.
    #y = pm.Bernoulli('y', p=theta, observed=data)  # likelihood (one Bernoulli trial at a time)
    y = pm.Binomial('y',n=n_draws, p=theta, observed=sum(data)) # likelihood (y successes within n_draws experiments)


with beta_binomial_model:
    trace = pm.sample()









# We get two plots for theta:
# - on the  left a density plot (KDE, Kernel Density Estimation). The different lines refer to different MCMC chains run in parallel.
# - on the right the trace (the values sampled at each step). Also here the trace of different chains is shown (light blue vs dark blue)

# We can also see  the chains mix well  and that the density plot are smooth and overlap.

#the plot_trace function allows to visually check the quality of the sampling: 
#do the different chains providing a similar histogram?
# does the chain behave almost as white noise?
#is the histogram smooth?


import arviz as az
az.plot_trace(trace);


# the plot_posterior function is more concerned with the estimated values, it reports point estimate and HDI
# the posterior mean, computed analytically, is (1 + 30)/ (1 + 30 + 70 +1) = 0.303, almost identical to the value estimated by pymc3.
#The true theta which generates the data  is 0.35, which is well within the HDI.
az.plot_posterior(trace);









































#the actual theta is 0.35
#recall our prior is Beta(1,1); we have 30 successes in 100 trials.
#the posterior mean  is analytically (1+30)/(2+100)=0.303
#this is indeed very close to the posterior mean of our samples

#This is relatively far from 0.35, but the actual value is included in the 95% HDI.
#That is why HDI is more meaningful than a point estimate.

# The posterior  is well sampled:  rhat is 1,  both ess are high.
az.summary(trace, hdi_prob=0.95)








az.plot_posterior (trace, hdi_prob = 0.95);














# We show both the 95% HDI and the rope. 
#We reject the hypothesis of the coin being practically unbiased.

az.plot_posterior (trace, rope = [0.49, 0.51], hdi_prob=0.95);








# the specific reference value (0.5) in this case rejected as it does not belong to the HDI.
az.plot_posterior(trace, ref_val=0.5);








#Solution
experiments = 50
n_correct = 48

# The lines within the "with" statement refer to the same model 
with pm.Model() as beta_binomial_classifier:
    #Uniform prior, i.e., Beta(1,1) 
    theta = pm.Beta('theta', alpha=1, beta=1)  
    y = pm.Binomial('y',n=experiments, p=theta, observed=n_correct) # likelihood (y successes within n_draws experiments)
    trace_classifier = pm.sample(return_inferencedata=True)
    


# according to the data, there is about 78% probability that your classifier constitutes an actual improvement
az.plot_posterior(trace_classifier, ref_val=0.92);


#actually we can use a more sensible prior. We can expect the performance of our classifier to be about 90%. 
#To tune the prior, we check the 2.5 and the 97.5 percentiles obtained under different choices of a and b.
from scipy.stats import beta

qtiles = [0.025, 0.975]
for (a,b) in [(9,1), (18,2)]:
    print(a,b)
    current_qtiles = beta.ppf(qtiles, a=a, b=b)
    print(current_qtiles)


# we now set the prior as beta(18,2)
with pm.Model() as beta_binomial_prior2:
    theta = pm.Beta('theta', alpha=18, beta=2)  
    y = pm.Binomial('y',n=experiments, p=theta, observed=n_correct) # likelihood (y successes within n_draws experiments)
    trace_classifier = pm.sample(return_inferencedata=True)
    az.plot_posterior(trace_classifier, ref_val=0.92);
    
# both  0.9 and 0.92 are within the HDI, so both values are plausible. With this more informative prior, the posterior probability of the classifier being larger than 92% 
#increases by a couple of points.
